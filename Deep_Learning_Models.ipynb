{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZBwHQqQ_L2R"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osaAJQu6GQ1l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgiT0mcA7psX",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load and rename columns\n",
        "df = pd.read_csv(\"merged_energy_weather_data.csv\", parse_dates=[\"Date\"])\n",
        "df.columns = [\n",
        "    \"date\", \"price\", \"temp\", \"precip\", \"wind\", \"humidity\",\n",
        "    \"cloud\", \"radiation\", \"week_day\", \"month\", \"day_month\"\n",
        "]\n",
        "print(df.shape)\n",
        "df.head()\n",
        "\n",
        "df = df.sort_values(\"date\").reset_index(drop=True)\n",
        "cloud_missing = df['cloud'].isna()\n",
        "df.loc[cloud_missing, 'cloud'] = (\n",
        "    df['cloud'].shift(1) + df['cloud'].shift(-1)\n",
        ") / 2\n",
        "print(\"Remaining NaNs:\\n\", df.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gRqHblAjnImY"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns=[\"day_month\"])\n",
        "for lag in range(1, 8):\n",
        "    df[f\"lag_{lag}\"] = df[\"price\"].shift(lag)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "a28c53oHo8Qf"
      },
      "outputs": [],
      "source": [
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq1ZZ17eZ1cs"
      },
      "source": [
        "**Feedforward NN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84u5CmGYxMRi"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=123):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6P_nlL0_Wrb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "forecast_horizon = 14\n",
        "\n",
        "# Feature matrix: all columns except target and date\n",
        "feature_cols = df.columns.difference(['date', 'price']).tolist()\n",
        "feature_cols.sort()\n",
        "\n",
        "X = df[feature_cols].values\n",
        "\n",
        "# Create rolling 14-day future targets\n",
        "y = []\n",
        "for i in range(len(df) - forecast_horizon):\n",
        "    y.append(df['price'].iloc[i+1 : i+1+forecast_horizon].values)\n",
        "\n",
        "X = X[:len(y)]\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN7pFnWuBLuV"
      },
      "outputs": [],
      "source": [
        "def expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14):\n",
        "    \"\"\"\n",
        "    Generator yielding expanding window splits with one 14-day forecast per fold.\n",
        "    \"\"\"\n",
        "    n_samples = len(X)\n",
        "    start = initial_train_size\n",
        "\n",
        "    while start + 1 <= n_samples:\n",
        "        X_train = X[:start]\n",
        "        y_train = y[:start]\n",
        "        X_val = X[start:start + 1]  # Single sample (14-day forecast)\n",
        "        y_val = y[start:start + 1]\n",
        "        yield X_train, y_train, X_val, y_val\n",
        "        start += step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9VlcgM1rBPs1"
      },
      "outputs": [],
      "source": [
        "for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "    expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14)\n",
        "):\n",
        "    print(f\"Fold {fold+1}\")\n",
        "    print(f\"  Train shape: {X_tr.shape}, {y_tr.shape}\")\n",
        "    print(f\"  Val shape:   {X_val.shape}, {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rwysMmGBd3l"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Use first fold\n",
        "X_train, y_train, X_val, y_val = next(expanding_window_cv(X, y))\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_val_scaled = scaler_X.transform(X_val)\n",
        "\n",
        "y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "print(\"Scaled shapes:\")\n",
        "print(\"  X_train_scaled:\", X_train_scaled.shape)\n",
        "print(\"  X_val_scaled:  \", X_val_scaled.shape)\n",
        "print(\"  y_train_scaled:\", y_train_scaled.shape)\n",
        "print(\"  y_val_scaled:  \", y_val_scaled.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCdq2Rz0BgKA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class PriceForecastDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojo1_hvsDLeF"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class FeedforwardNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[64, 32], dropout=0.2):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        dims = [input_dim] + hidden_dims\n",
        "        for i in range(len(dims) - 1):\n",
        "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        layers.append(nn.Linear(dims[-1], 14))  # 14-day forecast\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeCZYv36DLlH"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "def train_one_fold(X_train, y_train, X_val, y_val, epochs=10, lr=0.001, batch_size=32):\n",
        "    # Scale input/output\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_val_scaled = scaler_X.transform(X_val)\n",
        "\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "    y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "    # Datasets and DataLoaders\n",
        "    train_dataset = PriceForecastDataset(X_train_scaled, y_train_scaled)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Model setup\n",
        "    model = FeedforwardNN(input_dim=X_train.shape[1]).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch)\n",
        "            loss = criterion(preds, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "        preds_scaled = model(X_val_tensor).cpu().numpy()\n",
        "        preds = scaler_y.inverse_transform(preds_scaled)\n",
        "        actuals = scaler_y.inverse_transform(y_val_scaled)\n",
        "\n",
        "    # Metrics\n",
        "    mae = mean_absolute_error(actuals.flatten(), preds.flatten())\n",
        "    mse = mean_squared_error(actuals.flatten(), preds.flatten())\n",
        "    rmse = np.sqrt(mse)\n",
        "    mape = np.mean(np.abs((actuals - preds) / np.abs(actuals))) * 100\n",
        "\n",
        "    return mae, rmse, mape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gQpHCpCwDgoc"
      },
      "outputs": [],
      "source": [
        "mae_scores = []\n",
        "rmse_scores = []\n",
        "mape_scores = []\n",
        "\n",
        "for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "    expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14)\n",
        "):\n",
        "    print(f\"Fold {fold+1}\")\n",
        "    mae, rmse, mape = train_one_fold(X_tr, y_tr, X_val, y_val)\n",
        "    print(f\"  MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.4f}\")\n",
        "    mae_scores.append(mae)\n",
        "    rmse_scores.append(rmse)\n",
        "    mape_scores.append(mape)\n",
        "\n",
        "print(\"\\n Final CV Results:\")\n",
        "print(f\"Average MAE:  {np.mean(mae_scores):.4f}\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):.4f}\")\n",
        "print(f\"Average MAPE: {np.mean(mape_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSnw6agzDdUh"
      },
      "outputs": [],
      "source": [
        "# with early stopping\n",
        "def train_one_fold(X_train, y_train, X_val, y_val,\n",
        "                   epochs=50, lr=0.001, batch_size=32, patience=5):\n",
        "    torch.manual_seed(123)\n",
        "\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "    import numpy as np\n",
        "\n",
        "    # Scale inputs\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_val_scaled = scaler_X.transform(X_val)\n",
        "\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "    y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "    # Dataset & DataLoader\n",
        "    train_dataset = PriceForecastDataset(X_train_scaled, y_train_scaled)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Model setup\n",
        "    model = FeedforwardNN(input_dim=X_train.shape[1]).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Early stopping setup\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch)\n",
        "            loss = criterion(preds, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Validation loss (on 1 val sample)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "            y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32).to(device)\n",
        "            val_preds = model(X_val_tensor)\n",
        "            val_loss = criterion(val_preds, y_val_tensor).item()\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss - 1e-4:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Restore best model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    # Final prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "        preds_scaled = model(X_val_tensor).cpu().numpy()\n",
        "        preds = scaler_y.inverse_transform(preds_scaled)\n",
        "        actuals = scaler_y.inverse_transform(y_val_scaled)\n",
        "\n",
        "    mae = mean_absolute_error(actuals.flatten(), preds.flatten())\n",
        "    rmse = np.sqrt(mean_squared_error(actuals.flatten(), preds.flatten()))\n",
        "    mape = np.mean(np.abs((actuals - preds) / (actuals + 1e-8))) * 100\n",
        "\n",
        "    return mae, rmse, mape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlcEC-jVDdSR",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "mae_scores = []\n",
        "rmse_scores = []\n",
        "mape_scores = []\n",
        "\n",
        "for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "    expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14)\n",
        "):\n",
        "    print(f\"Fold {fold+1}\")\n",
        "    mae, rmse, mape = train_one_fold(X_tr, y_tr, X_val, y_val)\n",
        "    print(f\"  MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.4f}\")\n",
        "    mae_scores.append(mae)\n",
        "    rmse_scores.append(rmse)\n",
        "    mape_scores.append(mape)\n",
        "\n",
        "print(\"\\n Final CV Results:\")\n",
        "print(f\"Average MAE:  {np.mean(mae_scores):.4f}\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):.4f}\")\n",
        "print(f\"Average MAPE: {np.mean(mape_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m0sDBsIS30E"
      },
      "source": [
        "\n",
        "\n",
        "Hyperparameter tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rjaYNxuNaTa"
      },
      "outputs": [],
      "source": [
        "def train_one_fold(X_train, y_train, X_val, y_val,\n",
        "                   hidden_dims=[64, 32], dropout=0.2,\n",
        "                   epochs=50, lr=0.001, batch_size=32, patience=5):\n",
        "\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "    # Scale input/output\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_val_scaled = scaler_X.transform(X_val)\n",
        "\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "    y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "    # PyTorch dataset and DataLoader\n",
        "    train_dataset = PriceForecastDataset(X_train_scaled, y_train_scaled)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Model\n",
        "    model = FeedforwardNN(input_dim=X_train.shape[1], hidden_dims=hidden_dims, dropout=dropout).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch)\n",
        "            loss = criterion(preds, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "            y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32).to(device)\n",
        "            val_preds = model(X_val_tensor)\n",
        "            val_loss = criterion(val_preds, y_val_tensor).item()\n",
        "\n",
        "        if val_loss < best_val_loss - 1e-4:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "         #       print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    # Final prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "        preds_scaled = model(X_val_tensor).cpu().numpy()\n",
        "        preds = scaler_y.inverse_transform(preds_scaled)\n",
        "        actuals = scaler_y.inverse_transform(y_val_scaled)\n",
        "\n",
        "    # Metrics\n",
        "    mae = mean_absolute_error(actuals.flatten(), preds.flatten())\n",
        "    rmse = np.sqrt(mean_squared_error(actuals.flatten(), preds.flatten()))\n",
        "    mape = np.mean(np.abs((actuals - preds) / (actuals + 1e-8))) * 100\n",
        "\n",
        "    return mae, rmse, mape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGKwWYaCNaWA"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "# Define hyperparameter options\n",
        "hidden_layer_options = [[64, 32], [32, 16], [128, 64, 32], [64]]\n",
        "dropout_options = [0.0, 0.2, 0.3]\n",
        "lr_options = [0.001, 0.0005]\n",
        "\n",
        "# All combinations\n",
        "param_grid = list(itertools.product(hidden_layer_options, dropout_options, lr_options))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNSVEnjXNaZa",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "for i, (hidden_dims, dropout, lr) in enumerate(param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(param_grid)}: hidden={hidden_dims}, dropout={dropout}, lr={lr}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            hidden_dims=hidden_dims,\n",
        "            dropout=dropout,\n",
        "            lr=lr,\n",
        "            epochs=50,\n",
        "            patience=5,\n",
        "            batch_size=32\n",
        "        )\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    avg_mae = np.mean(mae_scores)\n",
        "    avg_rmse = np.mean(rmse_scores)\n",
        "    avg_mape = np.mean(mape_scores)\n",
        "\n",
        "    results.append({\n",
        "        \"hidden_dims\": hidden_dims,\n",
        "        \"dropout\": dropout,\n",
        "        \"lr\": lr,\n",
        "        \"MAE\": avg_mae,\n",
        "        \"RMSE\": avg_rmse,\n",
        "        \"MAPE\": avg_mape\n",
        "    })\n",
        "\n",
        "    print(f\"Avg MAE: {avg_mae:.4f}, RMSE: {avg_rmse:.4f}, MAPE: {avg_mape:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCX0zs14Nabw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results = df_results.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
        "df_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBeB2DCGZ7TD"
      },
      "source": [
        "**LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZNiUxGZ3Q0j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import trange\n",
        "\n",
        "# --- 1. LSTM Model ---\n",
        "class LSTMForecastNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=64, num_layers=1, dropout=0.2, output_size=14):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
        "                            batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)  # [batch, seq, hidden]\n",
        "        out = self.fc(lstm_out[:, -1, :])  # [batch, output_size]\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjdxCVg43Q2j"
      },
      "outputs": [],
      "source": [
        "# --- 2. Dataset Class  ---\n",
        "class LSTMForecastDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)  # [samples, seq_len, features]\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnzO0OrY3Q4w"
      },
      "outputs": [],
      "source": [
        "# --- 3. Sliding Window Input Creation ---\n",
        "def create_lstm_input(X_scaled, y_scaled, sequence_len=30):\n",
        "    X_lstm = []\n",
        "    y_lstm = []\n",
        "    for i in range(sequence_len, len(X_scaled)):\n",
        "        X_window = X_scaled[i-sequence_len:i]\n",
        "        y_target = y_scaled[i]  # already a 14-element array\n",
        "        X_lstm.append(X_window)\n",
        "        y_lstm.append(y_target)\n",
        "    return np.array(X_lstm), np.array(y_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3Iw2sV4VTtA"
      },
      "outputs": [],
      "source": [
        "# --- 4. Training One Fold ---\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_one_fold(X_train, y_train, X_val, y_val,\n",
        "                   sequence_len=30, num_epochs=50, patience=5,\n",
        "                   batch_size=32, lr=0.001, hidden_size=64, num_layers=1, dropout=0.2):\n",
        "\n",
        "    # --- 1. Scaling ---\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_val_scaled = scaler_X.transform(X_val)\n",
        "\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train)  # shape: (n_samples, 14)\n",
        "    y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "    # --- 2. Create LSTM sequences ---\n",
        "    X_train_seq, y_train_seq = create_lstm_input(X_train_scaled, y_train_scaled, sequence_len)\n",
        "    X_val_seq, y_val_seq = create_lstm_input(X_val_scaled, y_val_scaled, sequence_len)\n",
        "\n",
        "    if X_val_seq.shape[0] == 0:\n",
        "        print(\"Skipping fold: Not enough validation sequences.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # --- 3. Datasets and Loaders ---\n",
        "    train_loader = DataLoader(\n",
        "        LSTMForecastDataset(X_train_seq, y_train_seq), batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        LSTMForecastDataset(X_val_seq, y_val_seq), batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "\n",
        "    # --- 4. Model setup ---\n",
        "    model = LSTMForecastNet(\n",
        "        input_size=X_train_seq.shape[2],\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # --- 5. Training Loop with Early Stopping ---\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)  # shape: (batch_size, 14)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_preds = torch.cat([model(xb.to(device)) for xb, _ in val_loader])\n",
        "            val_targets = torch.cat([yb.to(device) for _, yb in val_loader])\n",
        "            val_loss = criterion(val_preds, val_targets)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss.item() < best_val_loss - 1e-4:\n",
        "            best_val_loss = val_loss.item()\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                break # Early stopping is triggered here\n",
        "\n",
        "    # --- 6. Load best model ---\n",
        "    if best_model_state:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    # --- 7. Final Evaluation ---\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32).to(device)\n",
        "        preds_scaled = model(X_val_tensor).cpu().numpy()\n",
        "        targets_scaled = y_val_seq  # already NumPy array\n",
        "\n",
        "    # Inverse scaling\n",
        "    preds = scaler_y.inverse_transform(preds_scaled)\n",
        "    targets = scaler_y.inverse_transform(targets_scaled)\n",
        "\n",
        "    # --- 8. Metrics ---\n",
        "    mae = mean_absolute_error(targets.flatten(), preds.flatten())\n",
        "    rmse = np.sqrt(mean_squared_error(targets.flatten(), preds.flatten()))\n",
        "    mape = mean_absolute_percentage_error(targets.flatten(), preds.flatten()) * 100\n",
        "\n",
        "    return mae, rmse, mape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbarPWuYVTvG"
      },
      "outputs": [],
      "source": [
        "# --- 5. Expanding Window Cross Validation ---\n",
        "def expanding_window_cv(X, y, initial_train_size, horizon, step, sequence_len):\n",
        "    # Ensure we have enough data for multiple sequences in validation\n",
        "    min_val_size = 60  # Should be > sequence_len + horizon to allow multiple samples\n",
        "    max_idx = len(X)\n",
        "\n",
        "    for start in range(initial_train_size, max_idx - min_val_size + 1, step):\n",
        "        end_val = start + min_val_size\n",
        "        if end_val > max_idx:\n",
        "            break\n",
        "        X_tr, y_tr = X[:start], y[:start]\n",
        "        X_val, y_val = X[start:end_val], y[start:end_val]\n",
        "        yield X_tr, y_tr, X_val, y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ztUOgQr1VloZ"
      },
      "outputs": [],
      "source": [
        "# --- 6. Main CV Execution ---\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"dropout option adds dropout after all but last recurrent layer\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"To copy construct from a tensor\")\n",
        "\n",
        "sequence_len = 30\n",
        "horizon = 14\n",
        "initial_train_size = 1095  # 3 years\n",
        "\n",
        "mae_scores = []\n",
        "rmse_scores = []\n",
        "mape_scores = []\n",
        "\n",
        "for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "    expanding_window_cv(X, y, initial_train_size=initial_train_size, horizon=horizon, step=horizon, sequence_len=sequence_len)\n",
        "):\n",
        "    print(f\"\\nFold {fold+1}\")\n",
        "    mae, rmse, mape = train_one_fold(\n",
        "        X_tr, y_tr, X_val, y_val, sequence_len=sequence_len\n",
        "    )\n",
        "    if mae is None:\n",
        "        continue  # Skip if fold was invalid\n",
        "    print(f\"  MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.4f}\")\n",
        "    mae_scores.append(mae)\n",
        "    rmse_scores.append(rmse)\n",
        "    mape_scores.append(mape)\n",
        "\n",
        "# --- 7. Final Results ---\n",
        "print(\"\\n Final CV Results:\")\n",
        "print(f\"Average MAE:  {np.mean(mae_scores):.4f}\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):.4f}\")\n",
        "print(f\"Average MAPE: {np.mean(mape_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWOLlbw59lU5"
      },
      "source": [
        "Hyperparamter tunning 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlCRNyGmVlqe"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "# Define hyperparameter grid\n",
        "hidden_sizes = [32, 64, 128]\n",
        "num_layers_list = [1, 2, 3]\n",
        "dropouts = [0.0, 0.1, 0.2, 0.3]\n",
        "learning_rates = [0.001] # subsistute with 0.0005\n",
        "batch_sizes = [32] # substitute with 64\n",
        "sequence_lens = [30]\n",
        "\n",
        "# Create grid of all combinations\n",
        "param_grid = list(itertools.product(\n",
        "    hidden_sizes,\n",
        "    num_layers_list,\n",
        "    dropouts,\n",
        "    learning_rates,\n",
        "    batch_sizes,\n",
        "    sequence_lens\n",
        "))\n",
        "\n",
        "print(f\"Total combinations: {len(param_grid)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1wXTpcNVls2",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "for i, (hidden_size, num_layers, dropout, lr, batch_size, sequence_len) in enumerate(param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(param_grid)}: hidden={hidden_size}, layers={num_layers}, dropout={dropout}, \"\n",
        "          f\"lr={lr}, batch_size={batch_size}, seq_len={sequence_len}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y,\n",
        "                            initial_train_size=1095,\n",
        "                            horizon=14,\n",
        "                            step=14,\n",
        "                            sequence_len=sequence_len)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            sequence_len=sequence_len,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue  # skip invalid fold\n",
        "\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    if mae_scores:  # only record results if at least one fold succeeded\n",
        "        results.append({\n",
        "            \"hidden_size\": hidden_size,\n",
        "            \"num_layers\": num_layers,\n",
        "            \"dropout\": dropout,\n",
        "            \"lr\": lr,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"sequence_len\": sequence_len,\n",
        "            \"MAE\": np.mean(mae_scores),\n",
        "            \"RMSE\": np.mean(rmse_scores),\n",
        "            \"MAPE\": np.mean(mape_scores)\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2328YGMVTxk"
      },
      "outputs": [],
      "source": [
        "df_results = pd.DataFrame(results)\n",
        "df_results = df_results.sort_values(\"RMSE\").reset_index(drop=True)\n",
        "df_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tunning 2"
      ],
      "metadata": {
        "id": "kH3FJfUe_HAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "# Define hyperparameter grid\n",
        "hidden_sizes = [32, 64, 128]\n",
        "num_layers_list = [1, 2, 3]\n",
        "dropouts = [0.0, 0.1, 0.2, 0.3]\n",
        "learning_rates = [0.0005]\n",
        "batch_sizes = [32] # substitute with 64\n",
        "sequence_lens = [30]\n",
        "\n",
        "# Create grid of all combinations\n",
        "param_grid = list(itertools.product(\n",
        "    hidden_sizes,\n",
        "    num_layers_list,\n",
        "    dropouts,\n",
        "    learning_rates,\n",
        "    batch_sizes,\n",
        "    sequence_lens\n",
        "))\n",
        "\n",
        "print(f\"Total combinations: {len(param_grid)}\")"
      ],
      "metadata": {
        "id": "Lop1gFmK_GLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for i, (hidden_size, num_layers, dropout, lr, batch_size, sequence_len) in enumerate(param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(param_grid)}: hidden={hidden_size}, layers={num_layers}, dropout={dropout}, \"\n",
        "          f\"lr={lr}, batch_size={batch_size}, seq_len={sequence_len}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y,\n",
        "                            initial_train_size=1095,\n",
        "                            horizon=14,\n",
        "                            step=14,\n",
        "                            sequence_len=sequence_len)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            sequence_len=sequence_len,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue  # skip invalid fold\n",
        "\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    if mae_scores:  # only record results if at least one fold succeeded\n",
        "        results.append({\n",
        "            \"hidden_size\": hidden_size,\n",
        "            \"num_layers\": num_layers,\n",
        "            \"dropout\": dropout,\n",
        "            \"lr\": lr,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"sequence_len\": sequence_len,\n",
        "            \"MAE\": np.mean(mae_scores),\n",
        "            \"RMSE\": np.mean(rmse_scores),\n",
        "            \"MAPE\": np.mean(mape_scores)\n",
        "        })"
      ],
      "metadata": {
        "id": "sM9E_dSZ_Gbs",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(results)\n",
        "df_results = df_results.sort_values(\"RMSE\").reset_index(drop=True)\n",
        "df_results"
      ],
      "metadata": {
        "id": "Gp0j-FSd_Gp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tunning 3"
      ],
      "metadata": {
        "id": "teHMQw8Z_X5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "# Define hyperparameter grid\n",
        "hidden_sizes = [32, 64, 128]\n",
        "num_layers_list = [1, 2, 3]\n",
        "dropouts = [0.0, 0.1, 0.2, 0.3]\n",
        "learning_rates = [0.001]\n",
        "batch_sizes = [64]\n",
        "sequence_lens = [30]\n",
        "\n",
        "# Create grid of all combinations\n",
        "param_grid = list(itertools.product(\n",
        "    hidden_sizes,\n",
        "    num_layers_list,\n",
        "    dropouts,\n",
        "    learning_rates,\n",
        "    batch_sizes,\n",
        "    sequence_lens\n",
        "))\n",
        "\n",
        "print(f\"Total combinations: {len(param_grid)}\")"
      ],
      "metadata": {
        "id": "hK_zRdLm_YTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for i, (hidden_size, num_layers, dropout, lr, batch_size, sequence_len) in enumerate(param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(param_grid)}: hidden={hidden_size}, layers={num_layers}, dropout={dropout}, \"\n",
        "          f\"lr={lr}, batch_size={batch_size}, seq_len={sequence_len}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y,\n",
        "                            initial_train_size=1095,\n",
        "                            horizon=14,\n",
        "                            step=14,\n",
        "                            sequence_len=sequence_len)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            sequence_len=sequence_len,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue  # skip invalid fold\n",
        "\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    if mae_scores:  # only record results if at least one fold succeeded\n",
        "        results.append({\n",
        "            \"hidden_size\": hidden_size,\n",
        "            \"num_layers\": num_layers,\n",
        "            \"dropout\": dropout,\n",
        "            \"lr\": lr,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"sequence_len\": sequence_len,\n",
        "            \"MAE\": np.mean(mae_scores),\n",
        "            \"RMSE\": np.mean(rmse_scores),\n",
        "            \"MAPE\": np.mean(mape_scores)\n",
        "        })"
      ],
      "metadata": {
        "id": "wug6QdiA_Yer",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(results)\n",
        "df_results = df_results.sort_values(\"RMSE\").reset_index(drop=True)\n",
        "df_results"
      ],
      "metadata": {
        "id": "mdHlMRF2_Yo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tunning 4"
      ],
      "metadata": {
        "id": "oXO6_j_q_Y7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "# Define hyperparameter grid\n",
        "hidden_sizes = [32, 64, 128]\n",
        "num_layers_list = [1, 2, 3]\n",
        "dropouts = [0.0, 0.1, 0.2, 0.3]\n",
        "learning_rates = [0.001]\n",
        "batch_sizes = [64]\n",
        "sequence_lens = [30]\n",
        "\n",
        "# Create grid of all combinations\n",
        "param_grid = list(itertools.product(\n",
        "    hidden_sizes,\n",
        "    num_layers_list,\n",
        "    dropouts,\n",
        "    learning_rates,\n",
        "    batch_sizes,\n",
        "    sequence_lens\n",
        "))\n",
        "\n",
        "print(f\"Total combinations: {len(param_grid)}\")"
      ],
      "metadata": {
        "id": "IUPbHUT6_ZTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for i, (hidden_size, num_layers, dropout, lr, batch_size, sequence_len) in enumerate(param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(param_grid)}: hidden={hidden_size}, layers={num_layers}, dropout={dropout}, \"\n",
        "          f\"lr={lr}, batch_size={batch_size}, seq_len={sequence_len}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y,\n",
        "                            initial_train_size=1095,\n",
        "                            horizon=14,\n",
        "                            step=14,\n",
        "                            sequence_len=sequence_len)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            sequence_len=sequence_len,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue  # skip invalid fold\n",
        "\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    if mae_scores:  # only record results if at least one fold succeeded\n",
        "        results.append({\n",
        "            \"hidden_size\": hidden_size,\n",
        "            \"num_layers\": num_layers,\n",
        "            \"dropout\": dropout,\n",
        "            \"lr\": lr,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"sequence_len\": sequence_len,\n",
        "            \"MAE\": np.mean(mae_scores),\n",
        "            \"RMSE\": np.mean(rmse_scores),\n",
        "            \"MAPE\": np.mean(mape_scores)\n",
        "        })"
      ],
      "metadata": {
        "id": "rEWlFOeA_Zb-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(results)\n",
        "df_results = df_results.sort_values(\"RMSE\").reset_index(drop=True)\n",
        "df_results"
      ],
      "metadata": {
        "id": "oZ-GWwQv_ZlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "311MqJSaCE-y"
      },
      "source": [
        "More restricted grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3aaPhgICETl",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Restricted Grid\n",
        "hidden_sizes = [64, 128]\n",
        "num_layers_list = [1, 2]\n",
        "lrs = [0.001, 0.0005]\n",
        "dropouts = [0.1, 0.3]\n",
        "sequence_lens = [30]\n",
        "\n",
        "# Fixed parameters\n",
        "batch_size = 64\n",
        "num_epochs = 50\n",
        "patience = 5\n",
        "initial_train_size = 1095\n",
        "horizon = 14\n",
        "step = 14\n",
        "\n",
        "# All combinations\n",
        "param_grid = list(itertools.product(hidden_sizes, num_layers_list, lrs, dropouts, sequence_lens))\n",
        "\n",
        "results = []\n",
        "\n",
        "# Grid search loop\n",
        "for i, (hidden_size, num_layers, lr, dropout, sequence_len) in enumerate(param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(param_grid)}  hidden: {hidden_size}, layers: {num_layers}, \"\n",
        "          f\"lr: {lr}, dropout: {dropout}, seq_len: {sequence_len}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y, initial_train_size=initial_train_size, horizon=horizon, step=step, sequence_len=sequence_len)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            sequence_len=sequence_len,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            lr=lr,\n",
        "            batch_size=batch_size,\n",
        "            num_epochs=num_epochs,\n",
        "            patience=patience\n",
        "        )\n",
        "        if mae is not None:\n",
        "            mae_scores.append(mae)\n",
        "            rmse_scores.append(rmse)\n",
        "            mape_scores.append(mape)\n",
        "\n",
        "    # Aggregate fold results\n",
        "    results.append({\n",
        "        \"hidden_size\": hidden_size,\n",
        "        \"num_layers\": num_layers,\n",
        "        \"lr\": lr,\n",
        "        \"dropout\": dropout,\n",
        "        \"sequence_len\": sequence_len,\n",
        "        \"MAE\": np.mean(mae_scores),\n",
        "        \"RMSE\": np.mean(rmse_scores),\n",
        "        \"MAPE\": np.mean(mape_scores)\n",
        "    })\n",
        "\n",
        "# Final DataFrame\n",
        "df_lstm_results = pd.DataFrame(results)\n",
        "df_lstm_results = df_lstm_results.sort_values(\"RMSE\").reset_index(drop=True)\n",
        "df_lstm_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgdknN1ALEP1"
      },
      "source": [
        "GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvWgcmyQCEWO"
      },
      "outputs": [],
      "source": [
        "class GRUForecastNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=64, num_layers=1, dropout=0.2, output_size=14):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers=num_layers,\n",
        "                          batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gru_out, _ = self.gru(x)  # [batch, seq, hidden]\n",
        "        out = self.fc(gru_out[:, -1, :])  # [batch, output_size]\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo2QXyLkCEZn"
      },
      "outputs": [],
      "source": [
        "def train_one_gru_fold(X_train, y_train, X_val, y_val,\n",
        "                       sequence_len=30, num_epochs=50, patience=5,\n",
        "                       batch_size=32, lr=0.001, hidden_size=64, num_layers=1, dropout=0.2):\n",
        "\n",
        "    # --- 1. Scaling ---\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_val_scaled = scaler_X.transform(X_val)\n",
        "\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train)  # shape: (n_samples, 14)\n",
        "    y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "    # --- 2. Create sequences ---\n",
        "    X_train_seq, y_train_seq = create_lstm_input(X_train_scaled, y_train_scaled, sequence_len)\n",
        "    X_val_seq, y_val_seq = create_lstm_input(X_val_scaled, y_val_scaled, sequence_len)\n",
        "\n",
        "    if X_val_seq.shape[0] == 0:\n",
        "        print(\"Skipping fold: Not enough validation sequences.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # --- 3. Data loaders ---\n",
        "    train_loader = DataLoader(\n",
        "        LSTMForecastDataset(X_train_seq, y_train_seq), batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        LSTMForecastDataset(X_val_seq, y_val_seq), batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "\n",
        "    # --- 4. Model setup ---\n",
        "    model = GRUForecastNet(\n",
        "        input_size=X_train_seq.shape[2],\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    # --- 5. Training loop ---\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_preds = torch.cat([model(xb.to(device)) for xb, _ in val_loader])\n",
        "            val_targets = torch.cat([yb.to(device) for _, yb in val_loader])\n",
        "            val_loss = criterion(val_preds, val_targets)\n",
        "\n",
        "        if val_loss.item() < best_val_loss - 1e-4:\n",
        "            best_val_loss = val_loss.item()\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                break\n",
        "\n",
        "    # --- 6. Load best model ---\n",
        "    if best_model_state:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    # --- 7. Final evaluation ---\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32).to(device)\n",
        "        preds_scaled = model(X_val_tensor).cpu().numpy()\n",
        "        targets_scaled = y_val_seq\n",
        "\n",
        "    preds = scaler_y.inverse_transform(preds_scaled)\n",
        "    targets = scaler_y.inverse_transform(targets_scaled)\n",
        "\n",
        "    # --- 8. Metrics ---\n",
        "    mae = mean_absolute_error(targets.flatten(), preds.flatten())\n",
        "    rmse = np.sqrt(mean_squared_error(targets.flatten(), preds.flatten()))\n",
        "    mape = mean_absolute_percentage_error(targets.flatten(), preds.flatten()) * 100\n",
        "\n",
        "    return mae, rmse, mape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG5rko9UOFZE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "sequence_len=30\n",
        "\n",
        "mae_scores = []\n",
        "rmse_scores = []\n",
        "mape_scores = []\n",
        "\n",
        "for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "    expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14, sequence_len=sequence_len)\n",
        "):\n",
        "    print(f\"\\nFold {fold+1}\")\n",
        "    mae, rmse, mape = train_one_gru_fold(\n",
        "        X_tr, y_tr, X_val, y_val, sequence_len=sequence_len\n",
        "    )\n",
        "    if mae is None:\n",
        "        continue  # Skip if fold was invalid\n",
        "    print(f\"  MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.2f}\")\n",
        "    mae_scores.append(mae)\n",
        "    rmse_scores.append(rmse)\n",
        "    mape_scores.append(mape)\n",
        "\n",
        "# --- Final CV Results ---\n",
        "print(\"\\n Final CV Results:\")\n",
        "print(f\"Average MAE:  {np.mean(mae_scores):.4f}\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):.4f}\")\n",
        "print(f\"Average MAPE: {np.mean(mape_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ve0l4fyPGE6"
      },
      "source": [
        "Hyperparamter tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dz73YRHZOFcK"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "# Hyperparameter grid for GRU\n",
        "hidden_sizes = [32, 64, 128]\n",
        "num_layers_list = [1, 2, 3]\n",
        "dropouts = [0.0, 0.1, 0.2, 0.3]\n",
        "learning_rates = [0.001, 0.0005]\n",
        "batch_sizes = [32, 64]\n",
        "\n",
        "# Cartesian product of all combinations\n",
        "gru_param_grid = list(itertools.product(hidden_sizes, num_layers_list, dropouts, learning_rates, batch_sizes))\n",
        "print(f\"Total GRU configurations: {len(gru_param_grid)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSxGjjW2OFd9",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "gru_results = []\n",
        "\n",
        "for i, (hidden_size, num_layers, dropout, lr, batch_size) in enumerate(gru_param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(gru_param_grid)}  hidden={hidden_size}, layers={num_layers}, dropout={dropout}, lr={lr}, batch_size={batch_size}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14, sequence_len=30)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_gru_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            lr=lr,\n",
        "            batch_size=batch_size,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            sequence_len=30\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    # Save results\n",
        "    avg_mae = np.mean(mae_scores)\n",
        "    avg_rmse = np.mean(rmse_scores)\n",
        "    avg_mape = np.mean(mape_scores)\n",
        "\n",
        "    gru_results.append({\n",
        "        \"hidden_size\": hidden_size,\n",
        "        \"num_layers\": num_layers,\n",
        "        \"dropout\": dropout,\n",
        "        \"lr\": lr,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"MAE\": avg_mae,\n",
        "        \"RMSE\": avg_rmse,\n",
        "        \"MAPE\": avg_mape\n",
        "    })\n",
        "\n",
        "    print(f\"Avg MAE: {avg_mae:.4f} | RMSE: {avg_rmse:.4f} | MAPE: {avg_mape:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D52kWBJrOFgN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_gru_results = pd.DataFrame(gru_results)\n",
        "df_gru_results = df_gru_results.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
        "df_gru_results.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More restrcited hyperparamter search (hidden size 32)"
      ],
      "metadata": {
        "id": "xzmjfKy4JK94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Hyperparameter grid for GRU\n",
        "hidden_sizes = [32]\n",
        "num_layers_list = [1, 2, 3]\n",
        "dropouts = [0.0, 0.1, 0.2, 0.3]\n",
        "learning_rates = [0.001, 0.0005]\n",
        "batch_sizes = [32]\n",
        "\n",
        "# Cartesian product of all combinations\n",
        "gru_param_grid = list(itertools.product(hidden_sizes, num_layers_list, dropouts, learning_rates, batch_sizes))\n",
        "print(f\"Total GRU configurations: {len(gru_param_grid)}\")"
      ],
      "metadata": {
        "id": "3_tzcMSCJJK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_results = []\n",
        "\n",
        "for i, (hidden_size, num_layers, dropout, lr, batch_size) in enumerate(gru_param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(gru_param_grid)}  hidden={hidden_size}, layers={num_layers}, dropout={dropout}, lr={lr}, batch_size={batch_size}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14, sequence_len=30)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_gru_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            lr=lr,\n",
        "            batch_size=batch_size,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            sequence_len=30\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    # Save results\n",
        "    avg_mae = np.mean(mae_scores)\n",
        "    avg_rmse = np.mean(rmse_scores)\n",
        "    avg_mape = np.mean(mape_scores)\n",
        "\n",
        "    gru_results.append({\n",
        "        \"hidden_size\": hidden_size,\n",
        "        \"num_layers\": num_layers,\n",
        "        \"dropout\": dropout,\n",
        "        \"lr\": lr,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"MAE\": avg_mae,\n",
        "        \"RMSE\": avg_rmse,\n",
        "        \"MAPE\": avg_mape\n",
        "    })\n",
        "\n",
        "    print(f\"Avg MAE: {avg_mae:.4f} | RMSE: {avg_rmse:.4f} | MAPE: {avg_mape:.4f}\")"
      ],
      "metadata": {
        "id": "JObgGT46JJYa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_gru_results = pd.DataFrame(gru_results)\n",
        "df_gru_results = df_gru_results.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
        "df_gru_results"
      ],
      "metadata": {
        "id": "aymVDgVHJJdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hidden size = 64"
      ],
      "metadata": {
        "id": "x5uBkuSbNjF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Hyperparameter grid for GRU\n",
        "hidden_sizes = [64]\n",
        "num_layers_list = [1, 2, 3]\n",
        "dropouts = [0.0, 0.1, 0.2, 0.3]\n",
        "learning_rates = [0.001, 0.0005]\n",
        "batch_sizes = [32]\n",
        "\n",
        "# Cartesian product of all combinations\n",
        "gru_param_grid = list(itertools.product(hidden_sizes, num_layers_list, dropouts, learning_rates, batch_sizes))\n",
        "print(f\"Total GRU configurations: {len(gru_param_grid)}\")"
      ],
      "metadata": {
        "id": "EYb5frKIJJi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_results = []\n",
        "\n",
        "for i, (hidden_size, num_layers, dropout, lr, batch_size) in enumerate(gru_param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(gru_param_grid)}  hidden={hidden_size}, layers={num_layers}, dropout={dropout}, lr={lr}, batch_size={batch_size}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14, sequence_len=30)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_gru_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            lr=lr,\n",
        "            batch_size=batch_size,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            sequence_len=30\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    # Save results\n",
        "    avg_mae = np.mean(mae_scores)\n",
        "    avg_rmse = np.mean(rmse_scores)\n",
        "    avg_mape = np.mean(mape_scores)\n",
        "\n",
        "    gru_results.append({\n",
        "        \"hidden_size\": hidden_size,\n",
        "        \"num_layers\": num_layers,\n",
        "        \"dropout\": dropout,\n",
        "        \"lr\": lr,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"MAE\": avg_mae,\n",
        "        \"RMSE\": avg_rmse,\n",
        "        \"MAPE\": avg_mape\n",
        "    })\n",
        "\n",
        "    print(f\"Avg MAE: {avg_mae:.4f} | RMSE: {avg_rmse:.4f} | MAPE: {avg_mape:.4f}\")"
      ],
      "metadata": {
        "id": "PbX0M46cJJoy",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_gru_results = pd.DataFrame(gru_results)\n",
        "df_gru_results = df_gru_results.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
        "df_gru_results"
      ],
      "metadata": {
        "id": "5nulbSSUJKXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "hidden size = 128"
      ],
      "metadata": {
        "id": "H8dBGrPYQyH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Hyperparameter grid for GRU\n",
        "hidden_sizes = [128]\n",
        "num_layers_list = [1, 2, 3]\n",
        "dropouts = [0.0, 0.1, 0.2, 0.3]\n",
        "learning_rates = [0.001, 0.0005]\n",
        "batch_sizes = [32]\n",
        "\n",
        "# Cartesian product of all combinations\n",
        "gru_param_grid = list(itertools.product(hidden_sizes, num_layers_list, dropouts, learning_rates, batch_sizes))\n",
        "print(f\"Total GRU configurations: {len(gru_param_grid)}\")"
      ],
      "metadata": {
        "id": "SeOAYd5FQxYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_results = []\n",
        "\n",
        "for i, (hidden_size, num_layers, dropout, lr, batch_size) in enumerate(gru_param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(gru_param_grid)}  hidden={hidden_size}, layers={num_layers}, dropout={dropout}, lr={lr}, batch_size={batch_size}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14, sequence_len=30)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_gru_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            lr=lr,\n",
        "            batch_size=batch_size,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            sequence_len=30\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    # Save results\n",
        "    avg_mae = np.mean(mae_scores)\n",
        "    avg_rmse = np.mean(rmse_scores)\n",
        "    avg_mape = np.mean(mape_scores)\n",
        "\n",
        "    gru_results.append({\n",
        "        \"hidden_size\": hidden_size,\n",
        "        \"num_layers\": num_layers,\n",
        "        \"dropout\": dropout,\n",
        "        \"lr\": lr,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"MAE\": avg_mae,\n",
        "        \"RMSE\": avg_rmse,\n",
        "        \"MAPE\": avg_mape\n",
        "    })\n",
        "\n",
        "    print(f\"Avg MAE: {avg_mae:.4f} | RMSE: {avg_rmse:.4f} | MAPE: {avg_mape:.4f}\")"
      ],
      "metadata": {
        "id": "BiDpr_ZhQxm2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_gru_results = pd.DataFrame(gru_results)\n",
        "df_gru_results = df_gru_results.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
        "df_gru_results"
      ],
      "metadata": {
        "id": "5hDfd_IfQxxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1D CNN"
      ],
      "metadata": {
        "id": "0nTSPOfM6vaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CNN1DForecastNet(nn.Module):\n",
        "    def __init__(self, input_channels, seq_len, num_outputs=14, num_filters=64, kernel_size=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=num_filters, kernel_size=kernel_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Compute output length after conv: L_out = L_in - kernel_size + 1\n",
        "        conv_out_len = seq_len - kernel_size + 1\n",
        "\n",
        "        self.flatten_dim = num_filters * conv_out_len\n",
        "        self.fc = nn.Linear(self.flatten_dim, num_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_len, input_channels]\n",
        "        x = x.permute(0, 2, 1)  # to shape [batch_size, input_channels, seq_len]\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.size(0), -1)  # flatten\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "H9JAneA833Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_cnn_fold(X_train, y_train, X_val, y_val,\n",
        "                       sequence_len=30,\n",
        "                       num_outputs=14,\n",
        "                       num_filters=64,\n",
        "                       kernel_size=3,\n",
        "                       dropout=0.2,\n",
        "                       num_epochs=50,\n",
        "                       patience=5,\n",
        "                       batch_size=32,\n",
        "                       lr=0.001):\n",
        "\n",
        "    # --- Scaling ---\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_val_scaled = scaler_X.transform(X_val)\n",
        "\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "    y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "    # --- Sequence creation ---\n",
        "    X_train_seq, y_train_seq = create_lstm_input(X_train_scaled, y_train_scaled, sequence_len)\n",
        "    X_val_seq, y_val_seq = create_lstm_input(X_val_scaled, y_val_scaled, sequence_len)\n",
        "\n",
        "    if X_val_seq.shape[0] == 0:\n",
        "        print(\"Skipping fold: Not enough validation sequences.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # --- DataLoader ---\n",
        "    train_loader = DataLoader(LSTMForecastDataset(X_train_seq, y_train_seq), batch_size=batch_size, shuffle=False)\n",
        "    val_loader = DataLoader(LSTMForecastDataset(X_val_seq, y_val_seq), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # --- Model ---\n",
        "    model = CNN1DForecastNet(\n",
        "        input_channels=X_train_seq.shape[2],\n",
        "        seq_len=sequence_len,\n",
        "        num_outputs=num_outputs,\n",
        "        num_filters=num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dropout=dropout\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    patience_counter = 0\n",
        "\n",
        "    # --- Training ---\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_preds = torch.cat([model(xb.to(device)) for xb, _ in val_loader])\n",
        "            val_targets = torch.cat([yb.to(device) for _, yb in val_loader])\n",
        "            val_loss = criterion(val_preds, val_targets)\n",
        "\n",
        "        if val_loss.item() < best_val_loss - 1e-4:\n",
        "            best_val_loss = val_loss.item()\n",
        "            best_model_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                break\n",
        "\n",
        "    # --- Load best model ---\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "    # --- Final Evaluation ---\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32).to(device)\n",
        "        preds_scaled = model(X_val_tensor).cpu().numpy()\n",
        "\n",
        "    preds = scaler_y.inverse_transform(preds_scaled)\n",
        "    targets = scaler_y.inverse_transform(y_val_seq)\n",
        "\n",
        "    mae = mean_absolute_error(targets.flatten(), preds.flatten())\n",
        "    rmse = np.sqrt(mean_squared_error(targets.flatten(), preds.flatten()))\n",
        "    mape = mean_absolute_percentage_error(targets.flatten(), preds.flatten()) * 100\n",
        "\n",
        "    return mae, rmse, mape"
      ],
      "metadata": {
        "id": "QwYvlrKr33MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_len = 30\n",
        "horizon = 14\n",
        "initial_train_size = 1095\n",
        "\n",
        "mae_scores = []\n",
        "rmse_scores = []\n",
        "mape_scores = []\n",
        "\n",
        "for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "    expanding_window_cv(X, y, initial_train_size=initial_train_size, horizon=horizon, step=horizon, sequence_len=sequence_len)\n",
        "):\n",
        "    print(f\"\\nFold {fold+1}\")\n",
        "    mae, rmse, mape = train_one_cnn_fold(\n",
        "        X_tr, y_tr, X_val, y_val,\n",
        "        sequence_len=sequence_len,\n",
        "        num_epochs=50,\n",
        "        patience=5,\n",
        "        batch_size=32,\n",
        "        lr=0.001,\n",
        "        num_filters=64,\n",
        "        kernel_size=3,\n",
        "        dropout=0.2\n",
        "    )\n",
        "\n",
        "    if mae is None:\n",
        "        continue  # skip invalid fold\n",
        "\n",
        "    print(f\"  MAE: {mae:.4f} | RMSE: {rmse:.4f} | MAPE: {mape:.4f}\")\n",
        "    mae_scores.append(mae)\n",
        "    rmse_scores.append(rmse)\n",
        "    mape_scores.append(mape)\n",
        "\n",
        "# --- 7. Final CV Results ---\n",
        "print(\"\\n Final CNN CV Results:\")\n",
        "print(f\"Average MAE:  {np.mean(mae_scores):.4f}\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):.4f}\")\n",
        "print(f\"Average MAPE: {np.mean(mape_scores):.4f}\")"
      ],
      "metadata": {
        "id": "awEUX2Ju600v",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial 1-layer hyperparameter tunning"
      ],
      "metadata": {
        "id": "Rq5sU1V5R-gM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "num_filters_list = [32, 64, 128]\n",
        "kernel_sizes = [2, 3, 5]\n",
        "dropouts = [0.0, 0.1, 0.2]\n",
        "learning_rates = [0.001]\n",
        "batch_sizes = [32]\n",
        "\n",
        "cnn_grid = list(itertools.product(num_filters_list, kernel_sizes, dropouts, learning_rates, batch_sizes))\n",
        "print(f\"Total CNN configurations: {len(cnn_grid)}\")"
      ],
      "metadata": {
        "id": "LVIjRNYf606f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_results = []\n",
        "\n",
        "for i, (num_filters, kernel_size, dropout, lr, batch_size) in enumerate(cnn_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(cnn_grid)}  filters={num_filters}, kernel={kernel_size}, dropout={dropout}, lr={lr}, batch={batch_size}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14, sequence_len=30)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_cnn_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            sequence_len=30,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            num_filters=num_filters,\n",
        "            kernel_size=kernel_size,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        if mae is not None:\n",
        "            mae_scores.append(mae)\n",
        "            rmse_scores.append(rmse)\n",
        "            mape_scores.append(mape)\n",
        "\n",
        "    if mae_scores:\n",
        "        cnn_results.append({\n",
        "            \"filters\": num_filters,\n",
        "            \"kernel\": kernel_size,\n",
        "            \"dropout\": dropout,\n",
        "            \"lr\": lr,\n",
        "            \"batch\": batch_size,\n",
        "            \"MAE\": np.mean(mae_scores),\n",
        "            \"RMSE\": np.mean(rmse_scores),\n",
        "            \"MAPE\": np.mean(mape_scores)\n",
        "        })\n",
        "\n",
        "        print(f\"Avg MAE: {np.mean(mae_scores):.2f} | RMSE: {np.mean(rmse_scores):.2f} | MAPE: {np.mean(mape_scores):.2f}\")\n",
        "    else:\n",
        "        print(\"Skipped config due to missing fold results\")"
      ],
      "metadata": {
        "id": "zoc9-VWS608d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cnn_results = pd.DataFrame(cnn_results)\n",
        "df_cnn_results = df_cnn_results.sort_values(\"RMSE\").reset_index(drop=True)\n",
        "df_cnn_results"
      ],
      "metadata": {
        "id": "xucyFBeF60-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-layer CNN"
      ],
      "metadata": {
        "id": "6zwWZjhtXV41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TwoLayerCNNForecastNet(nn.Module):\n",
        "    def __init__(self, input_channels, seq_len,\n",
        "                 num_filters1=64, kernel_size1=3,\n",
        "                 num_filters2=32, kernel_size2=2,\n",
        "                 dropout=0.0, output_size=14):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_channels,\n",
        "                               out_channels=num_filters1,\n",
        "                               kernel_size=kernel_size1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(in_channels=num_filters1,\n",
        "                               out_channels=num_filters2,\n",
        "                               kernel_size=kernel_size2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.AdaptiveAvgPool1d(1)  # Global pooling\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(num_filters2, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # [batch, channels, seq_len]\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))  # [batch, channels, 1]\n",
        "        x = x.squeeze(2)  # [batch, channels]\n",
        "        x = self.dropout(x)\n",
        "        out = self.fc(x)  # [batch, output_size]\n",
        "        return out"
      ],
      "metadata": {
        "id": "h1R0idXl61C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "def train_one_cnn_fold(X_train, y_train, X_val, y_val,\n",
        "                       sequence_len=30, num_epochs=50, patience=5,\n",
        "                       batch_size=32, lr=0.001,\n",
        "                       num_filters1=64, kernel_size1=3,\n",
        "                       num_filters2=32, kernel_size2=2,\n",
        "                       dropout=0.1):\n",
        "\n",
        "    # 1. Scaling\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_val_scaled = scaler_X.transform(X_val)\n",
        "\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "    y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "    # 2. Create CNN sequences\n",
        "    X_train_seq, y_train_seq = create_lstm_input(X_train_scaled, y_train_scaled, sequence_len)\n",
        "    X_val_seq, y_val_seq = create_lstm_input(X_val_scaled, y_val_scaled, sequence_len)\n",
        "\n",
        "    if X_val_seq.shape[0] == 0:\n",
        "        print(\"Skipping fold: Not enough validation sequences.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # 3. Dataset & Dataloader\n",
        "    train_loader = DataLoader(LSTMForecastDataset(X_train_seq, y_train_seq), batch_size=batch_size, shuffle=False)\n",
        "    val_loader = DataLoader(LSTMForecastDataset(X_val_seq, y_val_seq), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # 4. Model setup\n",
        "    model = TwoLayerCNNForecastNet(\n",
        "        input_channels=X_train_seq.shape[2],\n",
        "        seq_len=sequence_len,\n",
        "        num_filters1=num_filters1,\n",
        "        kernel_size1=kernel_size1,\n",
        "        num_filters2=num_filters2,\n",
        "        kernel_size2=kernel_size2,\n",
        "        dropout=dropout,\n",
        "        output_size=y_train_seq.shape[1]\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    # 5. Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_preds = torch.cat([model(xb.to(device)) for xb, _ in val_loader])\n",
        "            val_targets = torch.cat([yb.to(device) for _, yb in val_loader])\n",
        "            val_loss = criterion(val_preds, val_targets)\n",
        "\n",
        "        if val_loss.item() < best_val_loss - 1e-4:\n",
        "            best_val_loss = val_loss.item()\n",
        "            best_model_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                break\n",
        "\n",
        "    # 6. Load best model\n",
        "    if best_model_state:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    # 7. Final evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32).to(device)\n",
        "        preds_scaled = model(X_val_tensor).cpu().numpy()\n",
        "        targets_scaled = y_val_seq\n",
        "\n",
        "    preds = scaler_y.inverse_transform(preds_scaled)\n",
        "    targets = scaler_y.inverse_transform(targets_scaled)\n",
        "\n",
        "    # 8. Metrics\n",
        "    mae = mean_absolute_error(targets.flatten(), preds.flatten())\n",
        "    rmse = np.sqrt(mean_squared_error(targets.flatten(), preds.flatten()))\n",
        "    mape = mean_absolute_percentage_error(targets.flatten(), preds.flatten()) * 100\n",
        "\n",
        "    return mae, rmse, mape"
      ],
      "metadata": {
        "id": "CLFdhFkx61J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_len = 30\n",
        "horizon = 14\n",
        "initial_train_size = 1095\n",
        "\n",
        "mae_scores = []\n",
        "rmse_scores = []\n",
        "mape_scores = []\n",
        "\n",
        "for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "    expanding_window_cv(X, y, initial_train_size=initial_train_size, horizon=horizon, step=horizon, sequence_len=sequence_len)\n",
        "):\n",
        "    print(f\"\\n Fold {fold+1}\")\n",
        "    mae, rmse, mape = train_one_cnn_fold(\n",
        "        X_tr, y_tr, X_val, y_val,\n",
        "        sequence_len=sequence_len,\n",
        "        num_epochs=50,\n",
        "        patience=5,\n",
        "        batch_size=32,\n",
        "        lr=0.001,\n",
        "        num_filters1=64,\n",
        "        kernel_size1=3,\n",
        "        num_filters2=32,\n",
        "        kernel_size2=2,\n",
        "        dropout=0.2\n",
        "    )\n",
        "\n",
        "    if mae is None:\n",
        "        continue  # skip invalid fold\n",
        "\n",
        "    print(f\" MAE: {mae:.4f} | RMSE: {rmse:.4f} | MAPE: {mape:.4f}\")\n",
        "    mae_scores.append(mae)\n",
        "    rmse_scores.append(rmse)\n",
        "    mape_scores.append(mape)\n",
        "\n",
        "# --- 7. Final CV Results ---\n",
        "print(\"\\n Final CNN CV Results:\")\n",
        "print(f\"Average MAE:  {np.mean(mae_scores):.4f}\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):.4f}\")\n",
        "print(f\"Average MAPE: {np.mean(mape_scores):.4f}\")"
      ],
      "metadata": {
        "id": "ZEgn9MkM61L_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tunning"
      ],
      "metadata": {
        "id": "G9ogLaJxYxSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Define hyperparameter options\n",
        "num_filters1_list = [64, 128]\n",
        "num_filters2_list = [32, 64]\n",
        "kernel_size1_list = [2, 3]\n",
        "kernel_size2_list = [2, 3]\n",
        "dropouts = [0.0, 0.1, 0.2]\n",
        "learning_rates = [0.001]\n",
        "batch_sizes = [32]\n",
        "\n",
        "# Create full grid of combinations\n",
        "param_grid = list(itertools.product(\n",
        "    num_filters1_list,\n",
        "    num_filters2_list,\n",
        "    kernel_size1_list,\n",
        "    kernel_size2_list,\n",
        "    dropouts,\n",
        "    learning_rates,\n",
        "    batch_sizes\n",
        "))\n",
        "\n",
        "print(f\"Total combinations: {len(param_grid)}\")"
      ],
      "metadata": {
        "id": "juP-gz2tXZ7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "sequence_len = 30\n",
        "horizon = 14\n",
        "initial_train_size = 1095\n",
        "step = horizon\n",
        "\n",
        "for i, (nf1, nf2, ks1, ks2, dropout, lr, batch_size) in enumerate(param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(param_grid)}  \"\n",
        "          f\"nf1={nf1}, nf2={nf2}, ks1={ks1}, ks2={ks2}, dropout={dropout}, lr={lr}, batch={batch_size}\")\n",
        "\n",
        "    mae_scores, rmse_scores, mape_scores = [], [], []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y, initial_train_size=initial_train_size,\n",
        "                            horizon=horizon, step=step, sequence_len=sequence_len)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_cnn_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            sequence_len=sequence_len,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            num_filters1=nf1,\n",
        "            num_filters2=nf2,\n",
        "            kernel_size1=ks1,\n",
        "            kernel_size2=ks2,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue\n",
        "\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    # Average fold results\n",
        "    avg_mae = np.mean(mae_scores)\n",
        "    avg_rmse = np.mean(rmse_scores)\n",
        "    avg_mape = np.mean(mape_scores)\n",
        "\n",
        "    print(f\"Avg MAE: {avg_mae:.4f} | RMSE: {avg_rmse:.4f} | MAPE: {avg_mape:.4f}\")\n",
        "\n",
        "    results.append({\n",
        "        \"filters1\": nf1,\n",
        "        \"filters2\": nf2,\n",
        "        \"kernel1\": ks1,\n",
        "        \"kernel2\": ks2,\n",
        "        \"dropout\": dropout,\n",
        "        \"lr\": lr,\n",
        "        \"batch\": batch_size,\n",
        "        \"MAE\": avg_mae,\n",
        "        \"RMSE\": avg_rmse,\n",
        "        \"MAPE\": avg_mape\n",
        "    })"
      ],
      "metadata": {
        "id": "fDD5h0glXZ-1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results = df_results.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
        "\n",
        "# Display best configs\n",
        "df_results"
      ],
      "metadata": {
        "id": "Yj77BZ_TXaBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TCN"
      ],
      "metadata": {
        "id": "V_OOz67bdxIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Chomp1d(nn.Module):\n",
        "    \"\"\"Removes padding from the end to maintain causality.\"\"\"\n",
        "    def __init__(self, chomp_size):\n",
        "        super().__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :-self.chomp_size]  # Remove last chomp_size elements\n",
        "\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, dilation, padding, dropout):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                               stride=stride, padding=padding, dilation=dilation)\n",
        "        self.chomp1 = Chomp1d(padding)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,\n",
        "                               stride=stride, padding=padding, dilation=dilation)\n",
        "        self.chomp2 = Chomp1d(padding)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.chomp1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.dropout1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.chomp2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.dropout2(out)\n",
        "\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.relu(out + res)\n",
        "\n",
        "\n",
        "class TemporalConvNet(nn.Module):\n",
        "    def __init__(self, num_inputs, num_channels, kernel_size=3, dropout=0.2):\n",
        "        \"\"\"\n",
        "        num_inputs: number of input features\n",
        "        num_channels: list of output channels for each TCN layer (e.g. [64, 64])\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        num_levels = len(num_channels)\n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2 ** i\n",
        "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
        "            out_channels = num_channels[i]\n",
        "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1,\n",
        "                                     dilation=dilation_size, padding=(kernel_size-1)*dilation_size,\n",
        "                                     dropout=dropout)]\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch, seq_len, features]\n",
        "        x = x.permute(0, 2, 1)  # [batch, features, seq_len]\n",
        "        out = self.network(x)\n",
        "        out = out[:, :, -1]  # Use last timestep\n",
        "        return out\n",
        "\n",
        "\n",
        "class TCNForecastNet(nn.Module):\n",
        "    def __init__(self, input_size, num_channels=[64, 64], kernel_size=3, dropout=0.2, output_size=14):\n",
        "        super().__init__()\n",
        "        self.tcn = TemporalConvNet(num_inputs=input_size, num_channels=num_channels,\n",
        "                                   kernel_size=kernel_size, dropout=dropout)\n",
        "        self.fc = nn.Linear(num_channels[-1], output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.tcn(x)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "cPwQSex2Y14X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class TCNForecastDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "def create_tcn_input(X_scaled, y_scaled, sequence_len=30):\n",
        "    X_seq = []\n",
        "    y_seq = []\n",
        "    for i in range(sequence_len, len(X_scaled)):\n",
        "        X_window = X_scaled[i-sequence_len:i]\n",
        "        y_target = y_scaled[i]  # shape: (14,)\n",
        "        X_seq.append(X_window)\n",
        "        y_seq.append(y_target)\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "\n",
        "def train_one_tcn_fold(X_train, y_train, X_val, y_val,\n",
        "                       sequence_len=30, num_epochs=50, patience=5,\n",
        "                       batch_size=32, lr=0.001,\n",
        "                       num_channels=[64, 64], kernel_size=3, dropout=0.2):\n",
        "\n",
        "    # --- 1. Scaling ---\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_val_scaled = scaler_X.transform(X_val)\n",
        "\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "    y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "    # --- 2. Create TCN inputs ---\n",
        "    X_train_seq, y_train_seq = create_tcn_input(X_train_scaled, y_train_scaled, sequence_len)\n",
        "    X_val_seq, y_val_seq = create_tcn_input(X_val_scaled, y_val_scaled, sequence_len)\n",
        "\n",
        "    if len(X_val_seq) == 0:\n",
        "        print(\"Skipping fold: not enough validation sequences.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # --- 3. Dataloaders ---\n",
        "    train_loader = DataLoader(\n",
        "        TCNForecastDataset(X_train_seq, y_train_seq), batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        TCNForecastDataset(X_val_seq, y_val_seq), batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "\n",
        "    # --- 4. Model Setup ---\n",
        "    model = TCNForecastNet(\n",
        "        input_size=X_train_seq.shape[2],\n",
        "        num_channels=num_channels,\n",
        "        kernel_size=kernel_size,\n",
        "        dropout=dropout,\n",
        "        output_size=y_train.shape[1]  # usually 14\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    # --- 5. Early stopping setup ---\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_preds = torch.cat([model(xb.to(device)) for xb, _ in val_loader])\n",
        "            val_targets = torch.cat([yb.to(device) for _, yb in val_loader])\n",
        "            val_loss = criterion(val_preds, val_targets)\n",
        "\n",
        "        if val_loss.item() < best_val_loss - 1e-4:\n",
        "            best_val_loss = val_loss.item()\n",
        "            best_model_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                break\n",
        "\n",
        "    # --- 6. Load best model ---\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    # --- 7. Final evaluation ---\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32).to(device)\n",
        "        preds_scaled = model(X_val_tensor).cpu().numpy()\n",
        "\n",
        "    targets_scaled = y_val_seq\n",
        "    preds = scaler_y.inverse_transform(preds_scaled)\n",
        "    targets = scaler_y.inverse_transform(targets_scaled)\n",
        "\n",
        "    mae = mean_absolute_error(targets.flatten(), preds.flatten())\n",
        "    rmse = np.sqrt(mean_squared_error(targets.flatten(), preds.flatten()))\n",
        "    mape = mean_absolute_percentage_error(targets.flatten(), preds.flatten()) * 100\n",
        "\n",
        "    return mae, rmse, mape"
      ],
      "metadata": {
        "id": "mZnbS2z7Y16U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_len = 30\n",
        "horizon = 14\n",
        "initial_train_size = 1095\n",
        "\n",
        "mae_scores = []\n",
        "rmse_scores = []\n",
        "mape_scores = []\n",
        "\n",
        "for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "    expanding_window_cv(X, y, initial_train_size=initial_train_size, horizon=horizon, step=horizon, sequence_len=sequence_len)\n",
        "):\n",
        "    print(f\"\\nFold {fold+1}\")\n",
        "    mae, rmse, mape = train_one_tcn_fold(\n",
        "        X_tr, y_tr, X_val, y_val,\n",
        "        sequence_len=sequence_len,\n",
        "        num_epochs=50,\n",
        "        patience=5,\n",
        "        batch_size=32,\n",
        "        lr=0.001,\n",
        "        num_channels=[64, 64],\n",
        "        kernel_size=3,\n",
        "        dropout=0.2\n",
        "    )\n",
        "\n",
        "    if mae is None:\n",
        "        continue\n",
        "\n",
        "    print(f\"  MAE: {mae:.4f} | RMSE: {rmse:.4f} | MAPE: {mape:.4f}\")\n",
        "    mae_scores.append(mae)\n",
        "    rmse_scores.append(rmse)\n",
        "    mape_scores.append(mape)\n",
        "\n",
        "# --- Final Results ---\n",
        "print(\"\\n Final TCN CV Results:\")\n",
        "print(f\"Average MAE:  {np.mean(mae_scores):.4f}\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):.4f}\")\n",
        "print(f\"Average MAPE: {np.mean(mape_scores):.4f}\")"
      ],
      "metadata": {
        "id": "kBVS9-iCY18N",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tunning (lr = 0.001)"
      ],
      "metadata": {
        "id": "5Wqo1q1eeO8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "# Grid options\n",
        "num_channels_list = [\n",
        "    [64, 64],\n",
        "    [128, 64],\n",
        "    [128, 128]\n",
        "]\n",
        "\n",
        "kernel_sizes = [2, 3, 5]\n",
        "dropouts = [0.0, 0.1, 0.2]\n",
        "learning_rates = [0.001]\n",
        "batch_sizes = [32]\n",
        "\n",
        "# Generate combinations\n",
        "tcn_param_grid = list(product(num_channels_list, kernel_sizes, dropouts, learning_rates, batch_sizes))\n",
        "print(f\"Total combinations: {len(tcn_param_grid)}\")"
      ],
      "metadata": {
        "id": "TrXTo2t9eKq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tcn_results = []\n",
        "\n",
        "for i, (channels, kernel_size, dropout, lr, batch_size) in enumerate(tcn_param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(tcn_param_grid)}  channels={channels}, kernel={kernel_size}, dropout={dropout}, lr={lr}, batch={batch_size}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14, sequence_len=30)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_tcn_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            sequence_len=30,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            num_channels=channels,\n",
        "            kernel_size=kernel_size,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue\n",
        "\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    # Save results\n",
        "    tcn_results.append({\n",
        "        \"channels\": channels,\n",
        "        \"kernel_size\": kernel_size,\n",
        "        \"dropout\": dropout,\n",
        "        \"lr\": lr,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"MAE\": np.mean(mae_scores),\n",
        "        \"RMSE\": np.mean(rmse_scores),\n",
        "        \"MAPE\": np.mean(mape_scores)\n",
        "    })\n",
        "\n",
        "    print(f\"Avg MAE: {np.mean(mae_scores):.4f} | RMSE: {np.mean(rmse_scores):.4f} | MAPE: {np.mean(mape_scores):.4f}\")"
      ],
      "metadata": {
        "id": "H7yS11DweKtP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_tcn = pd.DataFrame(tcn_results)\n",
        "df_tcn = df_tcn.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
        "df_tcn"
      ],
      "metadata": {
        "id": "LRJD2ZeleKvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tunning (lr = 0.0005)"
      ],
      "metadata": {
        "id": "6Bzp87aB0Enu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "# Grid options\n",
        "num_channels_list = [\n",
        "    [64, 64],\n",
        "    [128, 64],\n",
        "    [128, 128]\n",
        "]\n",
        "\n",
        "kernel_sizes = [2, 3, 5]\n",
        "dropouts = [0.0, 0.1, 0.2]\n",
        "learning_rates = [0.0005]\n",
        "batch_sizes = [32]\n",
        "\n",
        "# Generate combinations\n",
        "tcn_param_grid = list(product(num_channels_list, kernel_sizes, dropouts, learning_rates, batch_sizes))\n",
        "print(f\"Total combinations: {len(tcn_param_grid)}\")"
      ],
      "metadata": {
        "id": "oALYS-Lm0Dnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tcn_results = []\n",
        "\n",
        "for i, (channels, kernel_size, dropout, lr, batch_size) in enumerate(tcn_param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(tcn_param_grid)}  channels={channels}, kernel={kernel_size}, dropout={dropout}, lr={lr}, batch={batch_size}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14, sequence_len=30)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_tcn_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            sequence_len=30,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            num_channels=channels,\n",
        "            kernel_size=kernel_size,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue\n",
        "\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    # Save results\n",
        "    tcn_results.append({\n",
        "        \"channels\": channels,\n",
        "        \"kernel_size\": kernel_size,\n",
        "        \"dropout\": dropout,\n",
        "        \"lr\": lr,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"MAE\": np.mean(mae_scores),\n",
        "        \"RMSE\": np.mean(rmse_scores),\n",
        "        \"MAPE\": np.mean(mape_scores)\n",
        "    })\n",
        "\n",
        "    print(f\" Avg MAE: {np.mean(mae_scores):.4f} | RMSE: {np.mean(rmse_scores):.4f} | MAPE: {np.mean(mape_scores):.4f}\")"
      ],
      "metadata": {
        "id": "S8myg9M40Dwe",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_tcn = pd.DataFrame(tcn_results)\n",
        "df_tcn = df_tcn.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
        "df_tcn"
      ],
      "metadata": {
        "id": "Ao3b56-B0D6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kw3HJdUhwaOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pTb4SmE0wbS0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}