{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "AZBwHQqQ_L2R",
        "outputId": "1e6a26fd-49ec-4661-8003-237759a70ca7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dc16c105-1f17-4e00-acf4-e4531496c64d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-dc16c105-1f17-4e00-acf4-e4531496c64d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving merged_energy_weather_data.csv to merged_energy_weather_data.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osaAJQu6GQ1l",
        "outputId": "61e4ce9a-2b1d-4df4-a2ef-14dfae2461eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgiT0mcA7psX",
        "outputId": "249bd5b5-3ff2-4113-ebcd-2bcc8853f854",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3865, 11)\n",
            "Remaining NaNs:\n",
            " date         0\n",
            "price        0\n",
            "temp         0\n",
            "precip       0\n",
            "wind         0\n",
            "humidity     0\n",
            "cloud        0\n",
            "radiation    0\n",
            "week_day     0\n",
            "month        0\n",
            "day_month    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load and rename columns\n",
        "df = pd.read_csv(\"merged_energy_weather_data.csv\", parse_dates=[\"Date\"])\n",
        "df.columns = [\n",
        "    \"date\", \"price\", \"temp\", \"precip\", \"wind\", \"humidity\",\n",
        "    \"cloud\", \"radiation\", \"week_day\", \"month\", \"day_month\"\n",
        "]\n",
        "print(df.shape)\n",
        "df.head()\n",
        "\n",
        "df = df.sort_values(\"date\").reset_index(drop=True)\n",
        "cloud_missing = df['cloud'].isna()\n",
        "df.loc[cloud_missing, 'cloud'] = (\n",
        "    df['cloud'].shift(1) + df['cloud'].shift(-1)\n",
        ") / 2\n",
        "print(\"Remaining NaNs:\\n\", df.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "collapsed": true,
        "id": "gRqHblAjnImY",
        "outputId": "b281725c-0ab0-4369-f22a-4dee3d9c7108"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        date      price      temp    precip       wind   humidity      cloud  \\\n",
              "0 2015-01-01  14.706087  6.440000  3.060714   8.392733  93.472826  95.130383   \n",
              "1 2015-01-02   7.075217  6.490322  5.457143  12.129323  84.318486  70.348317   \n",
              "2 2015-01-03  15.683333  4.429734  4.339286   8.461127  83.168743  35.944177   \n",
              "3 2015-01-04  23.827727  3.844092  1.325000   6.188518  79.221513  30.214925   \n",
              "4 2015-01-05  38.073750  4.910970  0.978571   3.414181  92.058873  77.380103   \n",
              "5 2015-01-06  29.161739  4.021349  0.810714   5.655600  92.616659  85.929154   \n",
              "6 2015-01-07  29.011667  4.691019  1.192857   5.781492  91.003623  54.440213   \n",
              "7 2015-01-08  24.343478  6.013413  3.214286   7.359274  92.091256  71.431795   \n",
              "8 2015-01-09  23.687917  5.783474  9.492857  10.275274  81.372204  72.396564   \n",
              "9 2015-01-10  14.021250  6.319171  7.303571  11.792501  83.443880  73.162782   \n",
              "\n",
              "    radiation  week_day  month      lag_1      lag_2      lag_3      lag_4  \\\n",
              "0   83.391304         3      1        NaN        NaN        NaN        NaN   \n",
              "1  456.913043         4      1  14.706087        NaN        NaN        NaN   \n",
              "2  503.043478         5      1   7.075217  14.706087        NaN        NaN   \n",
              "3  705.652174         6      1  15.683333   7.075217  14.706087        NaN   \n",
              "4  239.478261         0      1  23.827727  15.683333   7.075217  14.706087   \n",
              "5  110.086957         1      1  38.073750  23.827727  15.683333   7.075217   \n",
              "6  622.695652         2      1  29.161739  38.073750  23.827727  15.683333   \n",
              "7  288.086957         3      1  29.011667  29.161739  38.073750  23.827727   \n",
              "8  270.913043         4      1  24.343478  29.011667  29.161739  38.073750   \n",
              "9  236.913043         5      1  23.687917  24.343478  29.011667  29.161739   \n",
              "\n",
              "       lag_5      lag_6      lag_7  \n",
              "0        NaN        NaN        NaN  \n",
              "1        NaN        NaN        NaN  \n",
              "2        NaN        NaN        NaN  \n",
              "3        NaN        NaN        NaN  \n",
              "4        NaN        NaN        NaN  \n",
              "5  14.706087        NaN        NaN  \n",
              "6   7.075217  14.706087        NaN  \n",
              "7  15.683333   7.075217  14.706087  \n",
              "8  23.827727  15.683333   7.075217  \n",
              "9  38.073750  23.827727  15.683333  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1c4bb33a-4e4f-42ab-b195-8db684c7c8b1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>price</th>\n",
              "      <th>temp</th>\n",
              "      <th>precip</th>\n",
              "      <th>wind</th>\n",
              "      <th>humidity</th>\n",
              "      <th>cloud</th>\n",
              "      <th>radiation</th>\n",
              "      <th>week_day</th>\n",
              "      <th>month</th>\n",
              "      <th>lag_1</th>\n",
              "      <th>lag_2</th>\n",
              "      <th>lag_3</th>\n",
              "      <th>lag_4</th>\n",
              "      <th>lag_5</th>\n",
              "      <th>lag_6</th>\n",
              "      <th>lag_7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2015-01-01</td>\n",
              "      <td>14.706087</td>\n",
              "      <td>6.440000</td>\n",
              "      <td>3.060714</td>\n",
              "      <td>8.392733</td>\n",
              "      <td>93.472826</td>\n",
              "      <td>95.130383</td>\n",
              "      <td>83.391304</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2015-01-02</td>\n",
              "      <td>7.075217</td>\n",
              "      <td>6.490322</td>\n",
              "      <td>5.457143</td>\n",
              "      <td>12.129323</td>\n",
              "      <td>84.318486</td>\n",
              "      <td>70.348317</td>\n",
              "      <td>456.913043</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>14.706087</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2015-01-03</td>\n",
              "      <td>15.683333</td>\n",
              "      <td>4.429734</td>\n",
              "      <td>4.339286</td>\n",
              "      <td>8.461127</td>\n",
              "      <td>83.168743</td>\n",
              "      <td>35.944177</td>\n",
              "      <td>503.043478</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>7.075217</td>\n",
              "      <td>14.706087</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2015-01-04</td>\n",
              "      <td>23.827727</td>\n",
              "      <td>3.844092</td>\n",
              "      <td>1.325000</td>\n",
              "      <td>6.188518</td>\n",
              "      <td>79.221513</td>\n",
              "      <td>30.214925</td>\n",
              "      <td>705.652174</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>15.683333</td>\n",
              "      <td>7.075217</td>\n",
              "      <td>14.706087</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2015-01-05</td>\n",
              "      <td>38.073750</td>\n",
              "      <td>4.910970</td>\n",
              "      <td>0.978571</td>\n",
              "      <td>3.414181</td>\n",
              "      <td>92.058873</td>\n",
              "      <td>77.380103</td>\n",
              "      <td>239.478261</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>23.827727</td>\n",
              "      <td>15.683333</td>\n",
              "      <td>7.075217</td>\n",
              "      <td>14.706087</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2015-01-06</td>\n",
              "      <td>29.161739</td>\n",
              "      <td>4.021349</td>\n",
              "      <td>0.810714</td>\n",
              "      <td>5.655600</td>\n",
              "      <td>92.616659</td>\n",
              "      <td>85.929154</td>\n",
              "      <td>110.086957</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>38.073750</td>\n",
              "      <td>23.827727</td>\n",
              "      <td>15.683333</td>\n",
              "      <td>7.075217</td>\n",
              "      <td>14.706087</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2015-01-07</td>\n",
              "      <td>29.011667</td>\n",
              "      <td>4.691019</td>\n",
              "      <td>1.192857</td>\n",
              "      <td>5.781492</td>\n",
              "      <td>91.003623</td>\n",
              "      <td>54.440213</td>\n",
              "      <td>622.695652</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>29.161739</td>\n",
              "      <td>38.073750</td>\n",
              "      <td>23.827727</td>\n",
              "      <td>15.683333</td>\n",
              "      <td>7.075217</td>\n",
              "      <td>14.706087</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2015-01-08</td>\n",
              "      <td>24.343478</td>\n",
              "      <td>6.013413</td>\n",
              "      <td>3.214286</td>\n",
              "      <td>7.359274</td>\n",
              "      <td>92.091256</td>\n",
              "      <td>71.431795</td>\n",
              "      <td>288.086957</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>29.011667</td>\n",
              "      <td>29.161739</td>\n",
              "      <td>38.073750</td>\n",
              "      <td>23.827727</td>\n",
              "      <td>15.683333</td>\n",
              "      <td>7.075217</td>\n",
              "      <td>14.706087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2015-01-09</td>\n",
              "      <td>23.687917</td>\n",
              "      <td>5.783474</td>\n",
              "      <td>9.492857</td>\n",
              "      <td>10.275274</td>\n",
              "      <td>81.372204</td>\n",
              "      <td>72.396564</td>\n",
              "      <td>270.913043</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>24.343478</td>\n",
              "      <td>29.011667</td>\n",
              "      <td>29.161739</td>\n",
              "      <td>38.073750</td>\n",
              "      <td>23.827727</td>\n",
              "      <td>15.683333</td>\n",
              "      <td>7.075217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2015-01-10</td>\n",
              "      <td>14.021250</td>\n",
              "      <td>6.319171</td>\n",
              "      <td>7.303571</td>\n",
              "      <td>11.792501</td>\n",
              "      <td>83.443880</td>\n",
              "      <td>73.162782</td>\n",
              "      <td>236.913043</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>23.687917</td>\n",
              "      <td>24.343478</td>\n",
              "      <td>29.011667</td>\n",
              "      <td>29.161739</td>\n",
              "      <td>38.073750</td>\n",
              "      <td>23.827727</td>\n",
              "      <td>15.683333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c4bb33a-4e4f-42ab-b195-8db684c7c8b1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1c4bb33a-4e4f-42ab-b195-8db684c7c8b1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1c4bb33a-4e4f-42ab-b195-8db684c7c8b1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-be8078d7-a783-4694-b296-8a7c36d3b952\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-be8078d7-a783-4694-b296-8a7c36d3b952')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-be8078d7-a783-4694-b296-8a7c36d3b952 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3865,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2015-01-01 00:00:00\",\n        \"max\": \"2025-07-31 00:00:00\",\n        \"num_unique_values\": 3865,\n        \"samples\": [\n          \"2021-05-12 00:00:00\",\n          \"2018-11-20 00:00:00\",\n          \"2021-10-13 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73.33427316576301,\n        \"min\": -63.12478261,\n        \"max\": 705.3633333,\n        \"num_unique_values\": 3843,\n        \"samples\": [\n          37.1972,\n          105.4275,\n          20.19565217\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"temp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.937809413217449,\n        \"min\": -7.333386698123807,\n        \"max\": 24.46425806982872,\n        \"num_unique_values\": 3864,\n        \"samples\": [\n          5.150555555555556,\n          8.345977319781666,\n          8.135628177196804\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precip\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.238733746930321,\n        \"min\": 0.0,\n        \"max\": 34.434375,\n        \"num_unique_values\": 2188,\n        \"samples\": [\n          0.2468749999999999,\n          2.1266666666666665,\n          2.589655172413793\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wind\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.8900325234784205,\n        \"min\": 1.2830890425999122,\n        \"max\": 12.472602031436711,\n        \"num_unique_values\": 3864,\n        \"samples\": [\n          3.1234126984126984,\n          10.524565340629003,\n          3.8226008700581686\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"humidity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.859572766290563,\n        \"min\": 51.32619550245538,\n        \"max\": 99.40178571428572,\n        \"num_unique_values\": 3864,\n        \"samples\": [\n          91.15925925925929,\n          90.24304117782378,\n          76.87202148890745\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cloud\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 24.252857481173425,\n        \"min\": 0.1432967347942493,\n        \"max\": 99.31423611111111,\n        \"num_unique_values\": 3865,\n        \"samples\": [\n          89.72001062171321,\n          75.50961254811193,\n          55.34363973327439\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"radiation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2273.0243174859784,\n        \"min\": 51.0,\n        \"max\": 8536.181818181818,\n        \"num_unique_values\": 3838,\n        \"samples\": [\n          336.0,\n          5216.590909090909,\n          2280.1739130434785\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"week_day\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          3,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 12,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          11,\n          10,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73.34303519851791,\n        \"min\": -63.12478261,\n        \"max\": 705.3633333,\n        \"num_unique_values\": 3842,\n        \"samples\": [\n          28.7375,\n          142.7204348,\n          44.25521739\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73.35246375182177,\n        \"min\": -63.12478261,\n        \"max\": 705.3633333,\n        \"num_unique_values\": 3841,\n        \"samples\": [\n          28.76042553,\n          78.7925,\n          86.37375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73.36185346108535,\n        \"min\": -63.12478261,\n        \"max\": 705.3633333,\n        \"num_unique_values\": 3840,\n        \"samples\": [\n          28.76042553,\n          88.48833333,\n          86.37375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag_4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73.3713409993035,\n        \"min\": -63.12478261,\n        \"max\": 705.3633333,\n        \"num_unique_values\": 3839,\n        \"samples\": [\n          28.76042553,\n          95.43916667,\n          86.37375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag_5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73.38056189786951,\n        \"min\": -63.12478261,\n        \"max\": 705.3633333,\n        \"num_unique_values\": 3838,\n        \"samples\": [\n          40.86318182,\n          55.46956522,\n          27.7884\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag_6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73.38918497153138,\n        \"min\": -63.12478261,\n        \"max\": 705.3633333,\n        \"num_unique_values\": 3837,\n        \"samples\": [\n          40.86318182,\n          55.46956522,\n          27.7884\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag_7\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73.39717535566113,\n        \"min\": -63.12478261,\n        \"max\": 705.3633333,\n        \"num_unique_values\": 3836,\n        \"samples\": [\n          40.86318182,\n          55.46956522,\n          27.7884\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df = df.drop(columns=[\"day_month\"])\n",
        "for lag in range(1, 8):\n",
        "    df[f\"lag_{lag}\"] = df[\"price\"].shift(lag)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "collapsed": true,
        "id": "a28c53oHo8Qf",
        "outputId": "bb24ccd3-eecf-475b-b4d6-8bfd6045cdeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3858, 17)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        date      price      temp    precip       wind   humidity      cloud  \\\n",
              "0 2015-01-08  24.343478  6.013413  3.214286   7.359274  92.091256  71.431795   \n",
              "1 2015-01-09  23.687917  5.783474  9.492857  10.275274  81.372204  72.396564   \n",
              "2 2015-01-10  14.021250  6.319171  7.303571  11.792501  83.443880  73.162782   \n",
              "3 2015-01-11   4.296087  3.989937  2.457143  10.834715  79.724570  42.435521   \n",
              "4 2015-01-12  10.938696  6.529484  2.867857   9.851411  87.450542  69.666470   \n",
              "\n",
              "    radiation  week_day  month      lag_1      lag_2      lag_3      lag_4  \\\n",
              "0  288.086957         3      1  29.011667  29.161739  38.073750  23.827727   \n",
              "1  270.913043         4      1  24.343478  29.011667  29.161739  38.073750   \n",
              "2  236.913043         5      1  23.687917  24.343478  29.011667  29.161739   \n",
              "3  406.434783         6      1  14.021250  23.687917  24.343478  29.011667   \n",
              "4  123.043478         0      1   4.296087  14.021250  23.687917  24.343478   \n",
              "\n",
              "       lag_5      lag_6      lag_7  \n",
              "0  15.683333   7.075217  14.706087  \n",
              "1  23.827727  15.683333   7.075217  \n",
              "2  38.073750  23.827727  15.683333  \n",
              "3  29.161739  38.073750  23.827727  \n",
              "4  29.011667  29.161739  38.073750  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9eeb6e87-27b8-492e-aff9-fc1269fc195d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>price</th>\n",
              "      <th>temp</th>\n",
              "      <th>precip</th>\n",
              "      <th>wind</th>\n",
              "      <th>humidity</th>\n",
              "      <th>cloud</th>\n",
              "      <th>radiation</th>\n",
              "      <th>week_day</th>\n",
              "      <th>month</th>\n",
              "      <th>lag_1</th>\n",
              "      <th>lag_2</th>\n",
              "      <th>lag_3</th>\n",
              "      <th>lag_4</th>\n",
              "      <th>lag_5</th>\n",
              "      <th>lag_6</th>\n",
              "      <th>lag_7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2015-01-08</td>\n",
              "      <td>24.343478</td>\n",
              "      <td>6.013413</td>\n",
              "      <td>3.214286</td>\n",
              "      <td>7.359274</td>\n",
              "      <td>92.091256</td>\n",
              "      <td>71.431795</td>\n",
              "      <td>288.086957</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>29.011667</td>\n",
              "      <td>29.161739</td>\n",
              "      <td>38.073750</td>\n",
              "      <td>23.827727</td>\n",
              "      <td>15.683333</td>\n",
              "      <td>7.075217</td>\n",
              "      <td>14.706087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2015-01-09</td>\n",
              "      <td>23.687917</td>\n",
              "      <td>5.783474</td>\n",
              "      <td>9.492857</td>\n",
              "      <td>10.275274</td>\n",
              "      <td>81.372204</td>\n",
              "      <td>72.396564</td>\n",
              "      <td>270.913043</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>24.343478</td>\n",
              "      <td>29.011667</td>\n",
              "      <td>29.161739</td>\n",
              "      <td>38.073750</td>\n",
              "      <td>23.827727</td>\n",
              "      <td>15.683333</td>\n",
              "      <td>7.075217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2015-01-10</td>\n",
              "      <td>14.021250</td>\n",
              "      <td>6.319171</td>\n",
              "      <td>7.303571</td>\n",
              "      <td>11.792501</td>\n",
              "      <td>83.443880</td>\n",
              "      <td>73.162782</td>\n",
              "      <td>236.913043</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>23.687917</td>\n",
              "      <td>24.343478</td>\n",
              "      <td>29.011667</td>\n",
              "      <td>29.161739</td>\n",
              "      <td>38.073750</td>\n",
              "      <td>23.827727</td>\n",
              "      <td>15.683333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2015-01-11</td>\n",
              "      <td>4.296087</td>\n",
              "      <td>3.989937</td>\n",
              "      <td>2.457143</td>\n",
              "      <td>10.834715</td>\n",
              "      <td>79.724570</td>\n",
              "      <td>42.435521</td>\n",
              "      <td>406.434783</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>14.021250</td>\n",
              "      <td>23.687917</td>\n",
              "      <td>24.343478</td>\n",
              "      <td>29.011667</td>\n",
              "      <td>29.161739</td>\n",
              "      <td>38.073750</td>\n",
              "      <td>23.827727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2015-01-12</td>\n",
              "      <td>10.938696</td>\n",
              "      <td>6.529484</td>\n",
              "      <td>2.867857</td>\n",
              "      <td>9.851411</td>\n",
              "      <td>87.450542</td>\n",
              "      <td>69.666470</td>\n",
              "      <td>123.043478</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4.296087</td>\n",
              "      <td>14.021250</td>\n",
              "      <td>23.687917</td>\n",
              "      <td>24.343478</td>\n",
              "      <td>29.011667</td>\n",
              "      <td>29.161739</td>\n",
              "      <td>38.073750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9eeb6e87-27b8-492e-aff9-fc1269fc195d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9eeb6e87-27b8-492e-aff9-fc1269fc195d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9eeb6e87-27b8-492e-aff9-fc1269fc195d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e3c9d552-f116-40b6-85cf-8d78546aff51\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e3c9d552-f116-40b6-85cf-8d78546aff51')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e3c9d552-f116-40b6-85cf-8d78546aff51 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3858,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2015-01-08 00:00:00\",\n        \"max\": \"2025-07-31 00:00:00\",\n        \"num_unique_values\": 3858,\n        \"samples\": [\n          \"2020-01-29 00:00:00\",\n          \"2018-06-16 00:00:00\",\n          \"2022-04-22 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73.37605893031063,\n        \"min\": -63.12478261,\n        \"max\": 705.3633333,\n        \"num_unique_values\": 3836,\n        \"samples\": [\n          21.08916667,\n          55.26208333,\n          30.46\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"temp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.940009318248437,\n        \"min\": -7.333386698123807,\n        \"max\": 24.46425806982872,\n        \"num_unique_values\": 3857,\n        \"samples\": [\n          4.8925232705371045,\n          14.949580039525692,\n          10.839488636363637\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precip\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.240788513168945,\n        \"min\": 0.0,\n        \"max\": 34.434375,\n        \"num_unique_values\": 2181,\n        \"samples\": [\n          3.3000000000000003,\n          0.221875,\n          5.233333333333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wind\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.8857198024496977,\n        \"min\": 1.2830890425999122,\n        \"max\": 12.472602031436711,\n        \"num_unique_values\": 3857,\n        \"samples\": [\n          6.024835533686466,\n          2.7282953761214634,\n          4.989227642276422\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"humidity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.862447037284435,\n        \"min\": 51.32619550245538,\n        \"max\": 99.40178571428572,\n        \"num_unique_values\": 3857,\n        \"samples\": [\n          86.68173923500011,\n          83.22234299516909,\n          65.36458333333331\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cloud\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 24.24918429737419,\n        \"min\": 0.1432967347942493,\n        \"max\": 99.31423611111111,\n        \"num_unique_values\": 3858,\n        \"samples\": [\n          44.74383226955765,\n          61.19171545494966,\n          4.807739002450541\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"radiation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2272.632127018401,\n        \"min\": 51.0,\n        \"max\": 8536.181818181818,\n        \"num_unique_values\": 3831,\n        \"samples\": [\n          7572.0,\n          4217.772727272727,\n          6691.272727272727\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"week_day\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          3,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 12,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          11,\n          10,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73.3777730981653,\n        \"min\": -63.12478261,\n        \"max\": 705.3633333,\n        \"num_unique_values\": 3836,\n        \"samples\": [\n          11.24041667,\n          56.45541667,\n          27.55041667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73.38012557041687,\n        \"min\": -63.12478261,\n        \"max\": 705.3633333,\n        \"num_unique_values\": 3836,\n        \"samples\": [\n          28.13958333,\n          57.49326087,\n          27.10875\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73.38140718321822,\n        \"min\": -63.12478261,\n        \"max\": 705.3633333,\n        \"num_unique_values\": 3836,\n        \"samples\": [\n          34.27083333,\n          47.9484,\n          31.91291667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag_4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73.38455518632067,\n        \"min\": -63.12478261,\n        \"max\": 705.3633333,\n        \"num_unique_values\": 3836,\n        \"samples\": [\n          26.20166667,\n          45.09583333,\n          31.69083333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag_5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73.38876573083495,\n        \"min\": -63.12478261,\n        \"max\": 705.3633333,\n        \"num_unique_values\": 3836,\n        \"samples\": [\n          23.43375,\n          44.44125,\n          30.15333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag_6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73.39403533074137,\n        \"min\": -63.12478261,\n        \"max\": 705.3633333,\n        \"num_unique_values\": 3836,\n        \"samples\": [\n          33.06041667,\n          53.475,\n          25.7706383\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lag_7\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73.39717535566113,\n        \"min\": -63.12478261,\n        \"max\": 705.3633333,\n        \"num_unique_values\": 3836,\n        \"samples\": [\n          40.86318182,\n          55.46956522,\n          27.7884\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq1ZZ17eZ1cs"
      },
      "source": [
        "**Feedforward NN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84u5CmGYxMRi"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=123):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6P_nlL0_Wrb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "forecast_horizon = 14\n",
        "\n",
        "# Feature matrix: all columns except target and date\n",
        "feature_cols = df.columns.difference(['date', 'price']).tolist()\n",
        "feature_cols.sort()\n",
        "\n",
        "X = df[feature_cols].values\n",
        "\n",
        "# Create rolling 14-day future targets\n",
        "y = []\n",
        "for i in range(len(df) - forecast_horizon):\n",
        "    y.append(df['price'].iloc[i+1 : i+1+forecast_horizon].values)\n",
        "\n",
        "X = X[:len(y)]\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN7pFnWuBLuV"
      },
      "outputs": [],
      "source": [
        "def expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14):\n",
        "    \"\"\"\n",
        "    Generator yielding expanding window splits with one 14-day forecast per fold.\n",
        "    \"\"\"\n",
        "    n_samples = len(X)\n",
        "    start = initial_train_size\n",
        "\n",
        "    while start + 1 <= n_samples:\n",
        "        X_train = X[:start]\n",
        "        y_train = y[:start]\n",
        "        X_val = X[start:start + 1]  # Single sample (14-day forecast)\n",
        "        y_val = y[start:start + 1]\n",
        "        yield X_train, y_train, X_val, y_val\n",
        "        start += step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9VlcgM1rBPs1",
        "outputId": "eeff8210-9e5b-4619-e288-5a16418befb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1\n",
            "  Train shape: (1095, 15), (1095, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 2\n",
            "  Train shape: (1109, 15), (1109, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 3\n",
            "  Train shape: (1123, 15), (1123, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 4\n",
            "  Train shape: (1137, 15), (1137, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 5\n",
            "  Train shape: (1151, 15), (1151, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 6\n",
            "  Train shape: (1165, 15), (1165, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 7\n",
            "  Train shape: (1179, 15), (1179, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 8\n",
            "  Train shape: (1193, 15), (1193, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 9\n",
            "  Train shape: (1207, 15), (1207, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 10\n",
            "  Train shape: (1221, 15), (1221, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 11\n",
            "  Train shape: (1235, 15), (1235, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 12\n",
            "  Train shape: (1249, 15), (1249, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 13\n",
            "  Train shape: (1263, 15), (1263, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 14\n",
            "  Train shape: (1277, 15), (1277, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 15\n",
            "  Train shape: (1291, 15), (1291, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 16\n",
            "  Train shape: (1305, 15), (1305, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 17\n",
            "  Train shape: (1319, 15), (1319, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 18\n",
            "  Train shape: (1333, 15), (1333, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 19\n",
            "  Train shape: (1347, 15), (1347, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 20\n",
            "  Train shape: (1361, 15), (1361, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 21\n",
            "  Train shape: (1375, 15), (1375, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 22\n",
            "  Train shape: (1389, 15), (1389, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 23\n",
            "  Train shape: (1403, 15), (1403, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 24\n",
            "  Train shape: (1417, 15), (1417, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 25\n",
            "  Train shape: (1431, 15), (1431, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 26\n",
            "  Train shape: (1445, 15), (1445, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 27\n",
            "  Train shape: (1459, 15), (1459, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 28\n",
            "  Train shape: (1473, 15), (1473, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 29\n",
            "  Train shape: (1487, 15), (1487, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 30\n",
            "  Train shape: (1501, 15), (1501, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 31\n",
            "  Train shape: (1515, 15), (1515, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 32\n",
            "  Train shape: (1529, 15), (1529, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 33\n",
            "  Train shape: (1543, 15), (1543, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 34\n",
            "  Train shape: (1557, 15), (1557, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 35\n",
            "  Train shape: (1571, 15), (1571, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 36\n",
            "  Train shape: (1585, 15), (1585, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 37\n",
            "  Train shape: (1599, 15), (1599, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 38\n",
            "  Train shape: (1613, 15), (1613, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 39\n",
            "  Train shape: (1627, 15), (1627, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 40\n",
            "  Train shape: (1641, 15), (1641, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 41\n",
            "  Train shape: (1655, 15), (1655, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 42\n",
            "  Train shape: (1669, 15), (1669, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 43\n",
            "  Train shape: (1683, 15), (1683, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 44\n",
            "  Train shape: (1697, 15), (1697, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 45\n",
            "  Train shape: (1711, 15), (1711, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 46\n",
            "  Train shape: (1725, 15), (1725, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 47\n",
            "  Train shape: (1739, 15), (1739, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 48\n",
            "  Train shape: (1753, 15), (1753, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 49\n",
            "  Train shape: (1767, 15), (1767, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 50\n",
            "  Train shape: (1781, 15), (1781, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 51\n",
            "  Train shape: (1795, 15), (1795, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 52\n",
            "  Train shape: (1809, 15), (1809, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 53\n",
            "  Train shape: (1823, 15), (1823, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 54\n",
            "  Train shape: (1837, 15), (1837, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 55\n",
            "  Train shape: (1851, 15), (1851, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 56\n",
            "  Train shape: (1865, 15), (1865, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 57\n",
            "  Train shape: (1879, 15), (1879, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 58\n",
            "  Train shape: (1893, 15), (1893, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 59\n",
            "  Train shape: (1907, 15), (1907, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 60\n",
            "  Train shape: (1921, 15), (1921, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 61\n",
            "  Train shape: (1935, 15), (1935, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 62\n",
            "  Train shape: (1949, 15), (1949, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 63\n",
            "  Train shape: (1963, 15), (1963, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 64\n",
            "  Train shape: (1977, 15), (1977, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 65\n",
            "  Train shape: (1991, 15), (1991, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 66\n",
            "  Train shape: (2005, 15), (2005, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 67\n",
            "  Train shape: (2019, 15), (2019, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 68\n",
            "  Train shape: (2033, 15), (2033, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 69\n",
            "  Train shape: (2047, 15), (2047, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 70\n",
            "  Train shape: (2061, 15), (2061, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 71\n",
            "  Train shape: (2075, 15), (2075, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 72\n",
            "  Train shape: (2089, 15), (2089, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 73\n",
            "  Train shape: (2103, 15), (2103, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 74\n",
            "  Train shape: (2117, 15), (2117, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 75\n",
            "  Train shape: (2131, 15), (2131, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 76\n",
            "  Train shape: (2145, 15), (2145, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 77\n",
            "  Train shape: (2159, 15), (2159, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 78\n",
            "  Train shape: (2173, 15), (2173, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 79\n",
            "  Train shape: (2187, 15), (2187, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 80\n",
            "  Train shape: (2201, 15), (2201, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 81\n",
            "  Train shape: (2215, 15), (2215, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 82\n",
            "  Train shape: (2229, 15), (2229, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 83\n",
            "  Train shape: (2243, 15), (2243, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 84\n",
            "  Train shape: (2257, 15), (2257, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 85\n",
            "  Train shape: (2271, 15), (2271, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 86\n",
            "  Train shape: (2285, 15), (2285, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 87\n",
            "  Train shape: (2299, 15), (2299, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 88\n",
            "  Train shape: (2313, 15), (2313, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 89\n",
            "  Train shape: (2327, 15), (2327, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 90\n",
            "  Train shape: (2341, 15), (2341, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 91\n",
            "  Train shape: (2355, 15), (2355, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 92\n",
            "  Train shape: (2369, 15), (2369, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 93\n",
            "  Train shape: (2383, 15), (2383, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 94\n",
            "  Train shape: (2397, 15), (2397, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 95\n",
            "  Train shape: (2411, 15), (2411, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 96\n",
            "  Train shape: (2425, 15), (2425, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 97\n",
            "  Train shape: (2439, 15), (2439, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 98\n",
            "  Train shape: (2453, 15), (2453, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 99\n",
            "  Train shape: (2467, 15), (2467, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 100\n",
            "  Train shape: (2481, 15), (2481, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 101\n",
            "  Train shape: (2495, 15), (2495, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 102\n",
            "  Train shape: (2509, 15), (2509, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 103\n",
            "  Train shape: (2523, 15), (2523, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 104\n",
            "  Train shape: (2537, 15), (2537, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 105\n",
            "  Train shape: (2551, 15), (2551, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 106\n",
            "  Train shape: (2565, 15), (2565, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 107\n",
            "  Train shape: (2579, 15), (2579, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 108\n",
            "  Train shape: (2593, 15), (2593, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 109\n",
            "  Train shape: (2607, 15), (2607, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 110\n",
            "  Train shape: (2621, 15), (2621, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 111\n",
            "  Train shape: (2635, 15), (2635, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 112\n",
            "  Train shape: (2649, 15), (2649, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 113\n",
            "  Train shape: (2663, 15), (2663, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 114\n",
            "  Train shape: (2677, 15), (2677, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 115\n",
            "  Train shape: (2691, 15), (2691, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 116\n",
            "  Train shape: (2705, 15), (2705, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 117\n",
            "  Train shape: (2719, 15), (2719, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 118\n",
            "  Train shape: (2733, 15), (2733, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 119\n",
            "  Train shape: (2747, 15), (2747, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 120\n",
            "  Train shape: (2761, 15), (2761, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 121\n",
            "  Train shape: (2775, 15), (2775, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 122\n",
            "  Train shape: (2789, 15), (2789, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 123\n",
            "  Train shape: (2803, 15), (2803, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 124\n",
            "  Train shape: (2817, 15), (2817, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 125\n",
            "  Train shape: (2831, 15), (2831, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 126\n",
            "  Train shape: (2845, 15), (2845, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 127\n",
            "  Train shape: (2859, 15), (2859, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 128\n",
            "  Train shape: (2873, 15), (2873, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 129\n",
            "  Train shape: (2887, 15), (2887, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 130\n",
            "  Train shape: (2901, 15), (2901, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 131\n",
            "  Train shape: (2915, 15), (2915, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 132\n",
            "  Train shape: (2929, 15), (2929, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 133\n",
            "  Train shape: (2943, 15), (2943, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 134\n",
            "  Train shape: (2957, 15), (2957, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 135\n",
            "  Train shape: (2971, 15), (2971, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 136\n",
            "  Train shape: (2985, 15), (2985, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 137\n",
            "  Train shape: (2999, 15), (2999, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 138\n",
            "  Train shape: (3013, 15), (3013, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 139\n",
            "  Train shape: (3027, 15), (3027, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 140\n",
            "  Train shape: (3041, 15), (3041, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 141\n",
            "  Train shape: (3055, 15), (3055, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 142\n",
            "  Train shape: (3069, 15), (3069, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 143\n",
            "  Train shape: (3083, 15), (3083, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 144\n",
            "  Train shape: (3097, 15), (3097, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 145\n",
            "  Train shape: (3111, 15), (3111, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 146\n",
            "  Train shape: (3125, 15), (3125, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 147\n",
            "  Train shape: (3139, 15), (3139, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 148\n",
            "  Train shape: (3153, 15), (3153, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 149\n",
            "  Train shape: (3167, 15), (3167, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 150\n",
            "  Train shape: (3181, 15), (3181, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 151\n",
            "  Train shape: (3195, 15), (3195, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 152\n",
            "  Train shape: (3209, 15), (3209, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 153\n",
            "  Train shape: (3223, 15), (3223, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 154\n",
            "  Train shape: (3237, 15), (3237, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 155\n",
            "  Train shape: (3251, 15), (3251, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 156\n",
            "  Train shape: (3265, 15), (3265, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 157\n",
            "  Train shape: (3279, 15), (3279, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 158\n",
            "  Train shape: (3293, 15), (3293, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 159\n",
            "  Train shape: (3307, 15), (3307, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 160\n",
            "  Train shape: (3321, 15), (3321, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 161\n",
            "  Train shape: (3335, 15), (3335, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 162\n",
            "  Train shape: (3349, 15), (3349, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 163\n",
            "  Train shape: (3363, 15), (3363, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 164\n",
            "  Train shape: (3377, 15), (3377, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 165\n",
            "  Train shape: (3391, 15), (3391, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 166\n",
            "  Train shape: (3405, 15), (3405, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 167\n",
            "  Train shape: (3419, 15), (3419, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 168\n",
            "  Train shape: (3433, 15), (3433, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 169\n",
            "  Train shape: (3447, 15), (3447, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 170\n",
            "  Train shape: (3461, 15), (3461, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 171\n",
            "  Train shape: (3475, 15), (3475, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 172\n",
            "  Train shape: (3489, 15), (3489, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 173\n",
            "  Train shape: (3503, 15), (3503, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 174\n",
            "  Train shape: (3517, 15), (3517, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 175\n",
            "  Train shape: (3531, 15), (3531, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 176\n",
            "  Train shape: (3545, 15), (3545, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 177\n",
            "  Train shape: (3559, 15), (3559, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 178\n",
            "  Train shape: (3573, 15), (3573, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 179\n",
            "  Train shape: (3587, 15), (3587, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 180\n",
            "  Train shape: (3601, 15), (3601, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 181\n",
            "  Train shape: (3615, 15), (3615, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 182\n",
            "  Train shape: (3629, 15), (3629, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 183\n",
            "  Train shape: (3643, 15), (3643, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 184\n",
            "  Train shape: (3657, 15), (3657, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 185\n",
            "  Train shape: (3671, 15), (3671, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 186\n",
            "  Train shape: (3685, 15), (3685, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 187\n",
            "  Train shape: (3699, 15), (3699, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 188\n",
            "  Train shape: (3713, 15), (3713, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 189\n",
            "  Train shape: (3727, 15), (3727, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 190\n",
            "  Train shape: (3741, 15), (3741, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 191\n",
            "  Train shape: (3755, 15), (3755, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 192\n",
            "  Train shape: (3769, 15), (3769, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 193\n",
            "  Train shape: (3783, 15), (3783, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 194\n",
            "  Train shape: (3797, 15), (3797, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 195\n",
            "  Train shape: (3811, 15), (3811, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 196\n",
            "  Train shape: (3825, 15), (3825, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n",
            "Fold 197\n",
            "  Train shape: (3839, 15), (3839, 14)\n",
            "  Val shape:   (1, 15), (1, 14)\n"
          ]
        }
      ],
      "source": [
        "for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "    expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14)\n",
        "):\n",
        "    print(f\"Fold {fold+1}\")\n",
        "    print(f\"  Train shape: {X_tr.shape}, {y_tr.shape}\")\n",
        "    print(f\"  Val shape:   {X_val.shape}, {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rwysMmGBd3l",
        "outputId": "c0f589b9-cb90-4c9b-8036-68f80bc69ec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled shapes:\n",
            "  X_train_scaled: (1095, 15)\n",
            "  X_val_scaled:   (1, 15)\n",
            "  y_train_scaled: (1095, 14)\n",
            "  y_val_scaled:   (1, 14)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Use first fold\n",
        "X_train, y_train, X_val, y_val = next(expanding_window_cv(X, y))\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_val_scaled = scaler_X.transform(X_val)\n",
        "\n",
        "y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "print(\"Scaled shapes:\")\n",
        "print(\"  X_train_scaled:\", X_train_scaled.shape)\n",
        "print(\"  X_val_scaled:  \", X_val_scaled.shape)\n",
        "print(\"  y_train_scaled:\", y_train_scaled.shape)\n",
        "print(\"  y_val_scaled:  \", y_val_scaled.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCdq2Rz0BgKA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class PriceForecastDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojo1_hvsDLeF"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class FeedforwardNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[64, 32], dropout=0.2):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        dims = [input_dim] + hidden_dims\n",
        "        for i in range(len(dims) - 1):\n",
        "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        layers.append(nn.Linear(dims[-1], 14))  # 14-day forecast\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeCZYv36DLlH"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "def train_one_fold(X_train, y_train, X_val, y_val, epochs=10, lr=0.001, batch_size=32):\n",
        "    # Scale input/output\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_val_scaled = scaler_X.transform(X_val)\n",
        "\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "    y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "    # Datasets and DataLoaders\n",
        "    train_dataset = PriceForecastDataset(X_train_scaled, y_train_scaled)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Model setup\n",
        "    model = FeedforwardNN(input_dim=X_train.shape[1]).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch)\n",
        "            loss = criterion(preds, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "        preds_scaled = model(X_val_tensor).cpu().numpy()\n",
        "        preds = scaler_y.inverse_transform(preds_scaled)\n",
        "        actuals = scaler_y.inverse_transform(y_val_scaled)\n",
        "\n",
        "    # Metrics\n",
        "    mae = mean_absolute_error(actuals.flatten(), preds.flatten())\n",
        "    mse = mean_squared_error(actuals.flatten(), preds.flatten())\n",
        "    rmse = np.sqrt(mse)\n",
        "    mape = np.mean(np.abs((actuals - preds) / np.abs(actuals))) * 100\n",
        "\n",
        "    return mae, rmse, mape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gQpHCpCwDgoc",
        "outputId": "866374c5-36f9-42d6-8517-1b0b6180b9a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1\n",
            "  MAE: 6.2473, RMSE: 7.1531, MAPE: 17.5927\n",
            "Fold 2\n",
            "  MAE: 5.3045, RMSE: 6.6728, MAPE: 23.4663\n",
            "Fold 3\n",
            "  MAE: 8.5751, RMSE: 10.5162, MAPE: 27.8835\n",
            "Fold 4\n",
            "  MAE: 11.5512, RMSE: 12.7947, MAPE: 26.8518\n",
            "Fold 5\n",
            "  MAE: 10.6407, RMSE: 12.8441, MAPE: 110.9050\n",
            "Fold 6\n",
            "  MAE: 10.5127, RMSE: 11.1842, MAPE: 24.9258\n",
            "Fold 7\n",
            "  MAE: 4.7153, RMSE: 5.7063, MAPE: 14.6408\n",
            "Fold 8\n",
            "  MAE: 5.2797, RMSE: 6.4927, MAPE: 13.4753\n",
            "Fold 9\n",
            "  MAE: 7.1043, RMSE: 8.4423, MAPE: 31.3333\n",
            "Fold 10\n",
            "  MAE: 8.0454, RMSE: 8.9329, MAPE: 20.8324\n",
            "Fold 11\n",
            "  MAE: 12.4983, RMSE: 12.8175, MAPE: 26.7261\n",
            "Fold 12\n",
            "  MAE: 7.6326, RMSE: 8.0471, MAPE: 19.5679\n",
            "Fold 13\n",
            "  MAE: 16.2515, RMSE: 17.1699, MAPE: 32.0438\n",
            "Fold 14\n",
            "  MAE: 5.9484, RMSE: 6.3339, MAPE: 11.3156\n",
            "Fold 15\n",
            "  MAE: 4.9652, RMSE: 5.9237, MAPE: 8.7144\n",
            "Fold 16\n",
            "  MAE: 5.1129, RMSE: 6.4292, MAPE: 9.3011\n",
            "Fold 17\n",
            "  MAE: 7.9587, RMSE: 9.4586, MAPE: 12.9831\n",
            "Fold 18\n",
            "  MAE: 8.0411, RMSE: 9.2881, MAPE: 13.2988\n",
            "Fold 19\n",
            "  MAE: 10.0517, RMSE: 12.6109, MAPE: 33.5137\n",
            "Fold 20\n",
            "  MAE: 14.5324, RMSE: 16.5797, MAPE: 36.2377\n",
            "Fold 21\n",
            "  MAE: 11.5293, RMSE: 15.2677, MAPE: 20.1767\n",
            "Fold 22\n",
            "  MAE: 12.4979, RMSE: 14.3506, MAPE: 24.0511\n",
            "Fold 23\n",
            "  MAE: 9.9361, RMSE: 13.5629, MAPE: 15.9029\n",
            "Fold 24\n",
            "  MAE: 12.5476, RMSE: 16.4266, MAPE: 44.7108\n",
            "Fold 25\n",
            "  MAE: 19.3487, RMSE: 20.5233, MAPE: 35.0182\n",
            "Fold 26\n",
            "  MAE: 10.5487, RMSE: 16.7912, MAPE: 77.1654\n",
            "Fold 27\n",
            "  MAE: 13.2560, RMSE: 15.2772, MAPE: 25.6171\n",
            "Fold 28\n",
            "  MAE: 11.2971, RMSE: 13.7279, MAPE: 17.8070\n",
            "Fold 29\n",
            "  MAE: 6.6960, RMSE: 11.7816, MAPE: 48.8468\n",
            "Fold 30\n",
            "  MAE: 6.2608, RMSE: 8.0715, MAPE: 22.0703\n",
            "Fold 31\n",
            "  MAE: 10.0244, RMSE: 15.2858, MAPE: 150.5617\n",
            "Fold 32\n",
            "  MAE: 3.0876, RMSE: 4.2809, MAPE: 8.0992\n",
            "Fold 33\n",
            "  MAE: 2.0893, RMSE: 2.8222, MAPE: 5.1127\n",
            "Fold 34\n",
            "  MAE: 7.9009, RMSE: 12.2151, MAPE: 38.2480\n",
            "Fold 35\n",
            "  MAE: 7.8339, RMSE: 8.8397, MAPE: 22.7752\n",
            "Fold 36\n",
            "  MAE: 3.5349, RMSE: 4.9721, MAPE: 10.2715\n",
            "Fold 37\n",
            "  MAE: 8.1379, RMSE: 12.2588, MAPE: 58.6613\n",
            "Fold 38\n",
            "  MAE: 6.6762, RMSE: 7.7614, MAPE: 17.5185\n",
            "Fold 39\n",
            "  MAE: 10.1480, RMSE: 11.1086, MAPE: 38.2183\n",
            "Fold 40\n",
            "  MAE: 11.3005, RMSE: 11.7592, MAPE: 27.4222\n",
            "Fold 41\n",
            "  MAE: 3.8127, RMSE: 4.6109, MAPE: 8.5797\n",
            "Fold 42\n",
            "  MAE: 8.1179, RMSE: 12.2111, MAPE: 41.9645\n",
            "Fold 43\n",
            "  MAE: 7.9237, RMSE: 9.3507, MAPE: 18.8742\n",
            "Fold 44\n",
            "  MAE: 6.7482, RMSE: 8.6964, MAPE: 24.7614\n",
            "Fold 45\n",
            "  MAE: 5.3403, RMSE: 6.1688, MAPE: 15.6179\n",
            "Fold 46\n",
            "  MAE: 5.5419, RMSE: 6.9992, MAPE: 18.2062\n",
            "Fold 47\n",
            "  MAE: 9.7248, RMSE: 11.2386, MAPE: 23.3254\n",
            "Fold 48\n",
            "  MAE: 6.3327, RMSE: 7.2582, MAPE: 14.5552\n",
            "Fold 49\n",
            "  MAE: 4.5927, RMSE: 5.7931, MAPE: 10.8765\n",
            "Fold 50\n",
            "  MAE: 8.6291, RMSE: 13.0475, MAPE: 116.1614\n",
            "Fold 51\n",
            "  MAE: 4.7475, RMSE: 5.7340, MAPE: 15.4640\n",
            "Fold 52\n",
            "  MAE: 3.7098, RMSE: 4.6168, MAPE: 13.6265\n",
            "Fold 53\n",
            "  MAE: 4.5066, RMSE: 4.9655, MAPE: 17.7712\n",
            "Fold 54\n",
            "  MAE: 8.5681, RMSE: 9.7312, MAPE: 60.0482\n",
            "Fold 55\n",
            "  MAE: 9.5854, RMSE: 11.1193, MAPE: 150.5553\n",
            "Fold 56\n",
            "  MAE: 9.4165, RMSE: 10.4346, MAPE: 173.8891\n",
            "Fold 57\n",
            "  MAE: 11.5777, RMSE: 13.0684, MAPE: 98.4882\n",
            "Fold 58\n",
            "  MAE: 9.3139, RMSE: 10.6450, MAPE: 99.6207\n",
            "Fold 59\n",
            "  MAE: 8.5868, RMSE: 10.6729, MAPE: 118.6544\n",
            "Fold 60\n",
            "  MAE: 9.3680, RMSE: 11.4948, MAPE: 110.1547\n",
            "Fold 61\n",
            "  MAE: 5.1311, RMSE: 6.3817, MAPE: 40.6149\n",
            "Fold 62\n",
            "  MAE: 9.7492, RMSE: 11.5134, MAPE: 141.3034\n",
            "Fold 63\n",
            "  MAE: 5.8813, RMSE: 7.5810, MAPE: 49.6693\n",
            "Fold 64\n",
            "  MAE: 7.6627, RMSE: 9.4996, MAPE: 25.7386\n",
            "Fold 65\n",
            "  MAE: 9.0693, RMSE: 14.3597, MAPE: 62.2876\n",
            "Fold 66\n",
            "  MAE: 11.1183, RMSE: 12.2377, MAPE: 158.3620\n",
            "Fold 67\n",
            "  MAE: 8.6619, RMSE: 12.6587, MAPE: 167.4153\n",
            "Fold 68\n",
            "  MAE: 9.6144, RMSE: 10.2661, MAPE: 27.2564\n",
            "Fold 69\n",
            "  MAE: 7.6089, RMSE: 10.2723, MAPE: 31.4370\n",
            "Fold 70\n",
            "  MAE: 8.1159, RMSE: 10.2728, MAPE: 27.8689\n",
            "Fold 71\n",
            "  MAE: 15.4271, RMSE: 19.1980, MAPE: 87.6739\n",
            "Fold 72\n",
            "  MAE: 9.8126, RMSE: 13.3971, MAPE: 112.4352\n",
            "Fold 73\n",
            "  MAE: 9.1036, RMSE: 10.9447, MAPE: 37.6180\n",
            "Fold 74\n",
            "  MAE: 12.5298, RMSE: 14.7502, MAPE: 203.0387\n",
            "Fold 75\n",
            "  MAE: 16.8579, RMSE: 18.1306, MAPE: 723.6747\n",
            "Fold 76\n",
            "  MAE: 21.7323, RMSE: 26.5161, MAPE: 57.8074\n",
            "Fold 77\n",
            "  MAE: 11.6965, RMSE: 14.2623, MAPE: 39.2358\n",
            "Fold 78\n",
            "  MAE: 13.7922, RMSE: 16.6057, MAPE: 61.2662\n",
            "Fold 79\n",
            "  MAE: 15.8798, RMSE: 20.1412, MAPE: 25.8994\n",
            "Fold 80\n",
            "  MAE: 9.0611, RMSE: 11.6258, MAPE: 22.6658\n",
            "Fold 81\n",
            "  MAE: 11.6687, RMSE: 15.3096, MAPE: 25.2774\n",
            "Fold 82\n",
            "  MAE: 8.2407, RMSE: 10.8054, MAPE: 23.4749\n",
            "Fold 83\n",
            "  MAE: 14.1265, RMSE: 15.2560, MAPE: 41.0485\n",
            "Fold 84\n",
            "  MAE: 15.2536, RMSE: 16.9611, MAPE: 30.4566\n",
            "Fold 85\n",
            "  MAE: 11.6085, RMSE: 16.1903, MAPE: 63.2588\n",
            "Fold 86\n",
            "  MAE: 21.1135, RMSE: 24.1873, MAPE: 35.8531\n",
            "Fold 87\n",
            "  MAE: 11.5904, RMSE: 14.8958, MAPE: 45.0731\n",
            "Fold 88\n",
            "  MAE: 21.0765, RMSE: 22.9392, MAPE: 55.5054\n",
            "Fold 89\n",
            "  MAE: 16.0791, RMSE: 17.7545, MAPE: 24.8338\n",
            "Fold 90\n",
            "  MAE: 21.2393, RMSE: 21.8913, MAPE: 34.9546\n",
            "Fold 91\n",
            "  MAE: 28.1176, RMSE: 28.5920, MAPE: 32.2349\n",
            "Fold 92\n",
            "  MAE: 11.9622, RMSE: 15.2282, MAPE: 17.3431\n",
            "Fold 93\n",
            "  MAE: 14.4010, RMSE: 18.0105, MAPE: 24.9486\n",
            "Fold 94\n",
            "  MAE: 26.1693, RMSE: 30.3759, MAPE: 39.3758\n",
            "Fold 95\n",
            "  MAE: 14.9871, RMSE: 17.0776, MAPE: 16.8035\n",
            "Fold 96\n",
            "  MAE: 32.7231, RMSE: 34.4291, MAPE: 27.2322\n",
            "Fold 97\n",
            "  MAE: 22.6933, RMSE: 29.8432, MAPE: 21.4346\n",
            "Fold 98\n",
            "  MAE: 45.9806, RMSE: 65.2335, MAPE: 35.6710\n",
            "Fold 99\n",
            "  MAE: 43.4993, RMSE: 53.4529, MAPE: 56.9606\n",
            "Fold 100\n",
            "  MAE: 43.0245, RMSE: 52.1739, MAPE: 58.3575\n",
            "Fold 101\n",
            "  MAE: 47.5327, RMSE: 57.4330, MAPE: 35.8088\n",
            "Fold 102\n",
            "  MAE: 47.2875, RMSE: 58.8920, MAPE: 26.4990\n",
            "Fold 103\n",
            "  MAE: 50.6650, RMSE: 57.5716, MAPE: 27.2542\n",
            "Fold 104\n",
            "  MAE: 96.1574, RMSE: 122.0857, MAPE: 77.7750\n",
            "Fold 105\n",
            "  MAE: 49.7911, RMSE: 63.4634, MAPE: 57.7027\n",
            "Fold 106\n",
            "  MAE: 38.9289, RMSE: 45.7986, MAPE: 59.0748\n",
            "Fold 107\n",
            "  MAE: 35.2690, RMSE: 43.5682, MAPE: 34.4616\n",
            "Fold 108\n",
            "  MAE: 28.4481, RMSE: 43.6068, MAPE: 30.6698\n",
            "Fold 109\n",
            "  MAE: 162.5810, RMSE: 189.5488, MAPE: 53.3493\n",
            "Fold 110\n",
            "  MAE: 39.8662, RMSE: 61.0224, MAPE: 41.0115\n",
            "Fold 111\n",
            "  MAE: 48.9563, RMSE: 56.6516, MAPE: 49.4282\n",
            "Fold 112\n",
            "  MAE: 70.3452, RMSE: 80.8280, MAPE: 36.7718\n",
            "Fold 113\n",
            "  MAE: 77.0757, RMSE: 78.4906, MAPE: 35.4644\n",
            "Fold 114\n",
            "  MAE: 30.4307, RMSE: 38.0566, MAPE: 16.1205\n",
            "Fold 115\n",
            "  MAE: 48.5258, RMSE: 59.8340, MAPE: 61.3994\n",
            "Fold 116\n",
            "  MAE: 39.3929, RMSE: 44.8697, MAPE: 23.2336\n",
            "Fold 117\n",
            "  MAE: 116.4009, RMSE: 124.2662, MAPE: 40.4119\n",
            "Fold 118\n",
            "  MAE: 60.2052, RMSE: 78.9969, MAPE: 31.7492\n",
            "Fold 119\n",
            "  MAE: 126.3828, RMSE: 143.0494, MAPE: 36.1210\n",
            "Fold 120\n",
            "  MAE: 62.2906, RMSE: 70.0370, MAPE: 18.1764\n",
            "Fold 121\n",
            "  MAE: 174.1069, RMSE: 195.6233, MAPE: 30.4518\n",
            "Fold 122\n",
            "  MAE: 130.3126, RMSE: 162.3309, MAPE: 38.5087\n",
            "Fold 123\n",
            "  MAE: 80.8777, RMSE: 117.5256, MAPE: 67.8987\n",
            "Fold 124\n",
            "  MAE: 118.1154, RMSE: 137.7330, MAPE: 217.5111\n",
            "Fold 125\n",
            "  MAE: 76.5250, RMSE: 95.0277, MAPE: 37.6922\n",
            "Fold 126\n",
            "  MAE: 41.8182, RMSE: 48.4898, MAPE: 60.5958\n",
            "Fold 127\n",
            "  MAE: 54.2566, RMSE: 61.4436, MAPE: 471.9402\n",
            "Fold 128\n",
            "  MAE: 146.7002, RMSE: 168.2849, MAPE: 51.4055\n",
            "Fold 129\n",
            "  MAE: 102.1257, RMSE: 112.9319, MAPE: 27.5851\n",
            "Fold 130\n",
            "  MAE: 176.2323, RMSE: 187.1712, MAPE: 554.9059\n",
            "Fold 131\n",
            "  MAE: 43.8134, RMSE: 51.8378, MAPE: 61.3647\n",
            "Fold 132\n",
            "  MAE: 38.1755, RMSE: 45.4207, MAPE: 24.1631\n",
            "Fold 133\n",
            "  MAE: 34.6034, RMSE: 40.1550, MAPE: 35.5256\n",
            "Fold 134\n",
            "  MAE: 24.1222, RMSE: 29.2587, MAPE: 27.1822\n",
            "Fold 135\n",
            "  MAE: 18.1964, RMSE: 21.8946, MAPE: 14.0305\n",
            "Fold 136\n",
            "  MAE: 32.6046, RMSE: 43.4360, MAPE: 82.1537\n",
            "Fold 137\n",
            "  MAE: 24.4850, RMSE: 27.4999, MAPE: 25.2273\n",
            "Fold 138\n",
            "  MAE: 27.6503, RMSE: 34.7163, MAPE: 43.7108\n",
            "Fold 139\n",
            "  MAE: 19.3141, RMSE: 24.0629, MAPE: 25.8222\n",
            "Fold 140\n",
            "  MAE: 24.9052, RMSE: 31.4172, MAPE: 47.9248\n",
            "Fold 141\n",
            "  MAE: 23.7771, RMSE: 28.6013, MAPE: 60.9147\n",
            "Fold 142\n",
            "  MAE: 38.1012, RMSE: 41.5243, MAPE: 40.4134\n",
            "Fold 143\n",
            "  MAE: 30.6624, RMSE: 49.7095, MAPE: 41.9442\n",
            "Fold 144\n",
            "  MAE: 32.9670, RMSE: 42.3129, MAPE: 246.9474\n",
            "Fold 145\n",
            "  MAE: 35.1827, RMSE: 40.8591, MAPE: 83.8171\n",
            "Fold 146\n",
            "  MAE: 23.0733, RMSE: 32.4099, MAPE: 247.0420\n",
            "Fold 147\n",
            "  MAE: 52.1685, RMSE: 56.5001, MAPE: 46.1042\n",
            "Fold 148\n",
            "  MAE: 12.1328, RMSE: 13.3886, MAPE: 12.2210\n",
            "Fold 149\n",
            "  MAE: 39.0154, RMSE: 52.7879, MAPE: 1572.6571\n",
            "Fold 150\n",
            "  MAE: 33.5306, RMSE: 40.8378, MAPE: 398.2360\n",
            "Fold 151\n",
            "  MAE: 33.2847, RMSE: 38.6649, MAPE: 2384.7678\n",
            "Fold 152\n",
            "  MAE: 28.1778, RMSE: 32.8082, MAPE: 51.9024\n",
            "Fold 153\n",
            "  MAE: 32.7149, RMSE: 36.0501, MAPE: 34.1611\n",
            "Fold 154\n",
            "  MAE: 28.5472, RMSE: 34.8477, MAPE: 34.0449\n",
            "Fold 155\n",
            "  MAE: 26.7099, RMSE: 34.7240, MAPE: 41.9519\n",
            "Fold 156\n",
            "  MAE: 36.9945, RMSE: 41.5772, MAPE: 180.3596\n",
            "Fold 157\n",
            "  MAE: 44.0755, RMSE: 48.7759, MAPE: 48.5213\n",
            "Fold 158\n",
            "  MAE: 30.4884, RMSE: 36.2200, MAPE: 59.0248\n",
            "Fold 159\n",
            "  MAE: 15.4110, RMSE: 18.4008, MAPE: 39.9243\n",
            "Fold 160\n",
            "  MAE: 12.5845, RMSE: 14.9547, MAPE: 23.9633\n",
            "Fold 161\n",
            "  MAE: 11.7300, RMSE: 13.9231, MAPE: 19.6111\n",
            "Fold 162\n",
            "  MAE: 21.4080, RMSE: 25.3011, MAPE: 44.4171\n",
            "Fold 163\n",
            "  MAE: 14.8996, RMSE: 18.8605, MAPE: 41.3100\n",
            "Fold 164\n",
            "  MAE: 17.7487, RMSE: 23.1910, MAPE: 59.1277\n",
            "Fold 165\n",
            "  MAE: 23.5119, RMSE: 28.6430, MAPE: 65.1670\n",
            "Fold 166\n",
            "  MAE: 24.4701, RMSE: 28.3110, MAPE: 79.9917\n",
            "Fold 167\n",
            "  MAE: 29.9983, RMSE: 34.6141, MAPE: 42.1932\n",
            "Fold 168\n",
            "  MAE: 23.2401, RMSE: 29.4658, MAPE: 113.5465\n",
            "Fold 169\n",
            "  MAE: 18.4424, RMSE: 22.8720, MAPE: 22.3349\n",
            "Fold 170\n",
            "  MAE: 27.5284, RMSE: 33.1055, MAPE: 123.6151\n",
            "Fold 171\n",
            "  MAE: 11.7871, RMSE: 15.2284, MAPE: 20.1607\n",
            "Fold 172\n",
            "  MAE: 21.4563, RMSE: 25.6972, MAPE: 72.4817\n",
            "Fold 173\n",
            "  MAE: 33.0122, RMSE: 35.2188, MAPE: 79.0851\n",
            "Fold 174\n",
            "  MAE: 30.0981, RMSE: 36.8415, MAPE: 33.0469\n",
            "Fold 175\n",
            "  MAE: 14.6437, RMSE: 22.0073, MAPE: 35.3476\n",
            "Fold 176\n",
            "  MAE: 25.7904, RMSE: 30.9123, MAPE: 78.3503\n",
            "Fold 177\n",
            "  MAE: 25.1994, RMSE: 29.2779, MAPE: 65.6415\n",
            "Fold 178\n",
            "  MAE: 23.7425, RMSE: 33.4632, MAPE: 88.3482\n",
            "Fold 179\n",
            "  MAE: 34.0569, RMSE: 42.6674, MAPE: 60.1040\n",
            "Fold 180\n",
            "  MAE: 28.6045, RMSE: 38.3845, MAPE: 194.3799\n",
            "Fold 181\n",
            "  MAE: 66.9598, RMSE: 104.5923, MAPE: 49.8895\n",
            "Fold 182\n",
            "  MAE: 100.9769, RMSE: 109.8823, MAPE: 308.1357\n",
            "Fold 183\n",
            "  MAE: 41.4438, RMSE: 52.2918, MAPE: 213.0232\n",
            "Fold 184\n",
            "  MAE: 44.8925, RMSE: 61.3850, MAPE: 40.9365\n",
            "Fold 185\n",
            "  MAE: 24.0635, RMSE: 30.3875, MAPE: 29.7835\n",
            "Fold 186\n",
            "  MAE: 32.7823, RMSE: 36.5023, MAPE: 32.5307\n",
            "Fold 187\n",
            "  MAE: 32.1881, RMSE: 36.7243, MAPE: 59.2213\n",
            "Fold 188\n",
            "  MAE: 32.0453, RMSE: 35.5904, MAPE: 61.9316\n",
            "Fold 189\n",
            "  MAE: 21.6883, RMSE: 28.3730, MAPE: 44.8061\n",
            "Fold 190\n",
            "  MAE: 18.4083, RMSE: 20.9733, MAPE: 31.1992\n",
            "Fold 191\n",
            "  MAE: 18.3932, RMSE: 21.1764, MAPE: 29.5262\n",
            "Fold 192\n",
            "  MAE: 11.4343, RMSE: 15.5661, MAPE: 13.5349\n",
            "Fold 193\n",
            "  MAE: 22.4831, RMSE: 25.5032, MAPE: 65.3575\n",
            "Fold 194\n",
            "  MAE: 11.8984, RMSE: 14.4650, MAPE: 24.5168\n",
            "Fold 195\n",
            "  MAE: 18.4460, RMSE: 20.8963, MAPE: 45.7315\n",
            "Fold 196\n",
            "  MAE: 31.7850, RMSE: 36.4874, MAPE: 38.2078\n",
            "Fold 197\n",
            "  MAE: 15.1256, RMSE: 18.0336, MAPE: 16.1210\n",
            "\n",
            " Final CV Results:\n",
            "Average MAE:  27.6004\n",
            "Average RMSE: 33.1404\n",
            "Average MAPE: 81.3104\n"
          ]
        }
      ],
      "source": [
        "mae_scores = []\n",
        "rmse_scores = []\n",
        "mape_scores = []\n",
        "\n",
        "for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "    expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14)\n",
        "):\n",
        "    print(f\"Fold {fold+1}\")\n",
        "    mae, rmse, mape = train_one_fold(X_tr, y_tr, X_val, y_val)\n",
        "    print(f\"  MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.4f}\")\n",
        "    mae_scores.append(mae)\n",
        "    rmse_scores.append(rmse)\n",
        "    mape_scores.append(mape)\n",
        "\n",
        "print(\"\\n Final CV Results:\")\n",
        "print(f\"Average MAE:  {np.mean(mae_scores):.4f}\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):.4f}\")\n",
        "print(f\"Average MAPE: {np.mean(mape_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSnw6agzDdUh"
      },
      "outputs": [],
      "source": [
        "# with early stopping\n",
        "def train_one_fold(X_train, y_train, X_val, y_val,\n",
        "                   epochs=50, lr=0.001, batch_size=32, patience=5):\n",
        "    torch.manual_seed(123)\n",
        "\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "    import numpy as np\n",
        "\n",
        "    # Scale inputs\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_val_scaled = scaler_X.transform(X_val)\n",
        "\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "    y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "    # Dataset & DataLoader\n",
        "    train_dataset = PriceForecastDataset(X_train_scaled, y_train_scaled)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Model setup\n",
        "    model = FeedforwardNN(input_dim=X_train.shape[1]).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Early stopping setup\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch)\n",
        "            loss = criterion(preds, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Validation loss (on 1 val sample)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "            y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32).to(device)\n",
        "            val_preds = model(X_val_tensor)\n",
        "            val_loss = criterion(val_preds, y_val_tensor).item()\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss - 1e-4:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Restore best model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    # Final prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "        preds_scaled = model(X_val_tensor).cpu().numpy()\n",
        "        preds = scaler_y.inverse_transform(preds_scaled)\n",
        "        actuals = scaler_y.inverse_transform(y_val_scaled)\n",
        "\n",
        "    mae = mean_absolute_error(actuals.flatten(), preds.flatten())\n",
        "    rmse = np.sqrt(mean_squared_error(actuals.flatten(), preds.flatten()))\n",
        "    mape = np.mean(np.abs((actuals - preds) / (actuals + 1e-8))) * 100\n",
        "\n",
        "    return mae, rmse, mape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlcEC-jVDdSR",
        "outputId": "55ae4b7e-13e6-4c21-c7cb-9b179e11886d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1\n",
            " Early stopping at epoch 33\n",
            "  MAE: 5.4730, RMSE: 6.3111, MAPE: 15.4720\n",
            "Fold 2\n",
            " Early stopping at epoch 27\n",
            "  MAE: 5.2818, RMSE: 6.5054, MAPE: 23.4355\n",
            "Fold 3\n",
            " Early stopping at epoch 46\n",
            "  MAE: 7.3530, RMSE: 8.9523, MAPE: 25.3389\n",
            "Fold 4\n",
            " Early stopping at epoch 14\n",
            "  MAE: 11.2930, RMSE: 12.4842, MAPE: 26.2442\n",
            "Fold 5\n",
            " Early stopping at epoch 14\n",
            "  MAE: 10.8048, RMSE: 13.2135, MAPE: 115.7303\n",
            "Fold 6\n",
            " Early stopping at epoch 23\n",
            "  MAE: 9.8491, RMSE: 10.6902, MAPE: 23.3322\n",
            "Fold 7\n",
            " Early stopping at epoch 12\n",
            "  MAE: 5.2988, RMSE: 6.1739, MAPE: 15.9145\n",
            "Fold 8\n",
            " Early stopping at epoch 11\n",
            "  MAE: 5.9077, RMSE: 6.8902, MAPE: 15.2768\n",
            "Fold 9\n",
            " Early stopping at epoch 8\n",
            "  MAE: 6.9886, RMSE: 8.2625, MAPE: 30.5973\n",
            "Fold 10\n",
            " Early stopping at epoch 39\n",
            "  MAE: 7.7743, RMSE: 8.6949, MAPE: 19.9236\n",
            "Fold 11\n",
            " Early stopping at epoch 23\n",
            "  MAE: 12.6153, RMSE: 12.8803, MAPE: 27.0893\n",
            "Fold 12\n",
            " Early stopping at epoch 11\n",
            "  MAE: 7.5123, RMSE: 8.0771, MAPE: 19.9018\n",
            "Fold 13\n",
            " Early stopping at epoch 31\n",
            "  MAE: 15.5407, RMSE: 16.3207, MAPE: 30.7919\n",
            "Fold 14\n",
            " Early stopping at epoch 27\n",
            "  MAE: 6.1825, RMSE: 6.5453, MAPE: 11.7784\n",
            "Fold 15\n",
            " Early stopping at epoch 27\n",
            "  MAE: 7.1188, RMSE: 8.0859, MAPE: 12.5090\n",
            "Fold 16\n",
            " Early stopping at epoch 14\n",
            "  MAE: 5.8043, RMSE: 7.3709, MAPE: 11.1764\n",
            "Fold 17\n",
            " Early stopping at epoch 21\n",
            "  MAE: 6.7897, RMSE: 8.2128, MAPE: 11.0794\n",
            "Fold 18\n",
            " Early stopping at epoch 25\n",
            "  MAE: 8.4364, RMSE: 10.3668, MAPE: 13.4870\n",
            "Fold 19\n",
            " Early stopping at epoch 8\n",
            "  MAE: 11.0535, RMSE: 13.5346, MAPE: 36.3571\n",
            "Fold 20\n",
            " Early stopping at epoch 12\n",
            "  MAE: 15.9622, RMSE: 18.2184, MAPE: 36.0959\n",
            "Fold 21\n",
            " Early stopping at epoch 12\n",
            "  MAE: 11.7788, RMSE: 15.2748, MAPE: 21.0485\n",
            "Fold 22\n",
            " Early stopping at epoch 10\n",
            "  MAE: 12.6972, RMSE: 14.5196, MAPE: 24.4931\n",
            "Fold 23\n",
            " Early stopping at epoch 22\n",
            "  MAE: 10.6395, RMSE: 13.6592, MAPE: 17.4300\n",
            "Fold 24\n",
            " Early stopping at epoch 15\n",
            "  MAE: 12.7350, RMSE: 16.5984, MAPE: 45.9885\n",
            "Fold 25\n",
            " Early stopping at epoch 11\n",
            "  MAE: 17.0762, RMSE: 18.3759, MAPE: 30.7076\n",
            "Fold 26\n",
            " Early stopping at epoch 21\n",
            "  MAE: 10.5701, RMSE: 16.5443, MAPE: 75.6076\n",
            "Fold 27\n",
            " Early stopping at epoch 10\n",
            "  MAE: 14.5379, RMSE: 16.9712, MAPE: 27.6974\n",
            "Fold 28\n",
            " Early stopping at epoch 27\n",
            "  MAE: 10.5455, RMSE: 13.0512, MAPE: 16.6552\n",
            "Fold 29\n",
            " Early stopping at epoch 26\n",
            "  MAE: 6.7760, RMSE: 11.7947, MAPE: 48.8741\n",
            "Fold 30\n",
            " Early stopping at epoch 33\n",
            "  MAE: 6.1176, RMSE: 7.5536, MAPE: 21.0233\n",
            "Fold 31\n",
            " Early stopping at epoch 6\n",
            "  MAE: 10.2005, RMSE: 15.0671, MAPE: 149.3636\n",
            "Fold 32\n",
            " Early stopping at epoch 15\n",
            "  MAE: 3.9353, RMSE: 5.0285, MAPE: 10.5154\n",
            "Fold 33\n",
            " Early stopping at epoch 30\n",
            "  MAE: 2.9440, RMSE: 3.5673, MAPE: 7.1167\n",
            "Fold 34\n",
            " Early stopping at epoch 8\n",
            "  MAE: 7.2469, RMSE: 11.6012, MAPE: 35.6828\n",
            "Fold 35\n",
            " Early stopping at epoch 14\n",
            "  MAE: 7.9548, RMSE: 8.8906, MAPE: 22.8867\n",
            "Fold 36\n",
            " Early stopping at epoch 13\n",
            "  MAE: 4.0137, RMSE: 5.1081, MAPE: 11.3886\n",
            "Fold 37\n",
            " Early stopping at epoch 6\n",
            "  MAE: 7.4213, RMSE: 11.5001, MAPE: 54.9199\n",
            "Fold 38\n",
            " Early stopping at epoch 9\n",
            "  MAE: 6.2442, RMSE: 7.2768, MAPE: 16.5048\n",
            "Fold 39\n",
            " Early stopping at epoch 6\n",
            "  MAE: 8.4219, RMSE: 9.5169, MAPE: 32.2417\n",
            "Fold 40\n",
            " Early stopping at epoch 6\n",
            "  MAE: 11.6488, RMSE: 12.3912, MAPE: 28.0182\n",
            "Fold 41\n",
            " Early stopping at epoch 21\n",
            "  MAE: 3.9775, RMSE: 4.7624, MAPE: 8.9260\n",
            "Fold 42\n",
            " Early stopping at epoch 6\n",
            "  MAE: 7.8247, RMSE: 12.0149, MAPE: 41.0534\n",
            "Fold 43\n",
            " Early stopping at epoch 8\n",
            "  MAE: 8.0573, RMSE: 9.7765, MAPE: 18.9128\n",
            "Fold 44\n",
            " Early stopping at epoch 7\n",
            "  MAE: 6.7720, RMSE: 8.6113, MAPE: 24.6675\n",
            "Fold 45\n",
            " Early stopping at epoch 18\n",
            "  MAE: 5.1295, RMSE: 5.9814, MAPE: 15.4184\n",
            "Fold 46\n",
            " Early stopping at epoch 14\n",
            "  MAE: 5.7399, RMSE: 6.8528, MAPE: 18.0459\n",
            "Fold 47\n",
            " Early stopping at epoch 8\n",
            "  MAE: 9.8801, RMSE: 10.9200, MAPE: 24.0435\n",
            "Fold 48\n",
            " Early stopping at epoch 47\n",
            "  MAE: 6.1405, RMSE: 6.9946, MAPE: 14.2866\n",
            "Fold 49\n",
            " Early stopping at epoch 50\n",
            "  MAE: 4.1524, RMSE: 5.2006, MAPE: 9.6443\n",
            "Fold 50\n",
            " Early stopping at epoch 34\n",
            "  MAE: 8.9337, RMSE: 12.7336, MAPE: 113.0169\n",
            "Fold 51\n",
            " Early stopping at epoch 19\n",
            "  MAE: 4.7554, RMSE: 5.6608, MAPE: 14.8469\n",
            "Fold 52\n",
            " Early stopping at epoch 49\n",
            "  MAE: 3.0566, RMSE: 3.9772, MAPE: 10.7569\n",
            "Fold 53\n",
            " Early stopping at epoch 18\n",
            "  MAE: 4.6397, RMSE: 5.1548, MAPE: 18.3808\n",
            "Fold 54\n",
            " Early stopping at epoch 23\n",
            "  MAE: 8.7276, RMSE: 10.0290, MAPE: 61.6368\n",
            "Fold 55\n",
            "  MAE: 9.2254, RMSE: 10.1594, MAPE: 132.6173\n",
            "Fold 56\n",
            " Early stopping at epoch 30\n",
            "  MAE: 9.0008, RMSE: 10.3222, MAPE: 176.2694\n",
            "Fold 57\n",
            " Early stopping at epoch 9\n",
            "  MAE: 11.7231, RMSE: 13.4431, MAPE: 104.4235\n",
            "Fold 58\n",
            " Early stopping at epoch 21\n",
            "  MAE: 9.9308, RMSE: 10.9217, MAPE: 101.7849\n",
            "Fold 59\n",
            " Early stopping at epoch 11\n",
            "  MAE: 8.7138, RMSE: 10.8239, MAPE: 121.6683\n",
            "Fold 60\n",
            " Early stopping at epoch 34\n",
            "  MAE: 9.2383, RMSE: 11.5741, MAPE: 109.9658\n",
            "Fold 61\n",
            " Early stopping at epoch 11\n",
            "  MAE: 5.6118, RMSE: 6.9891, MAPE: 44.7657\n",
            "Fold 62\n",
            " Early stopping at epoch 11\n",
            "  MAE: 10.3106, RMSE: 12.2241, MAPE: 150.5407\n",
            "Fold 63\n",
            " Early stopping at epoch 43\n",
            "  MAE: 5.4332, RMSE: 6.6466, MAPE: 38.7576\n",
            "Fold 64\n",
            " Early stopping at epoch 7\n",
            "  MAE: 7.7638, RMSE: 9.3347, MAPE: 27.6314\n",
            "Fold 65\n",
            " Early stopping at epoch 28\n",
            "  MAE: 8.5811, RMSE: 14.1460, MAPE: 61.5562\n",
            "Fold 66\n",
            " Early stopping at epoch 22\n",
            "  MAE: 10.3430, RMSE: 11.2697, MAPE: 137.9375\n",
            "Fold 67\n",
            " Early stopping at epoch 6\n",
            "  MAE: 10.3133, RMSE: 13.5807, MAPE: 174.0832\n",
            "Fold 68\n",
            " Early stopping at epoch 6\n",
            "  MAE: 9.8306, RMSE: 10.4528, MAPE: 27.9002\n",
            "Fold 69\n",
            " Early stopping at epoch 13\n",
            "  MAE: 7.5075, RMSE: 10.1843, MAPE: 31.3098\n",
            "Fold 70\n",
            " Early stopping at epoch 33\n",
            "  MAE: 8.1301, RMSE: 9.7569, MAPE: 25.5944\n",
            "Fold 71\n",
            " Early stopping at epoch 9\n",
            "  MAE: 14.8092, RMSE: 18.2323, MAPE: 84.2110\n",
            "Fold 72\n",
            " Early stopping at epoch 29\n",
            "  MAE: 9.2625, RMSE: 12.4633, MAPE: 97.6924\n",
            "Fold 73\n",
            " Early stopping at epoch 14\n",
            "  MAE: 8.8979, RMSE: 10.7794, MAPE: 36.9149\n",
            "Fold 74\n",
            " Early stopping at epoch 16\n",
            "  MAE: 12.4174, RMSE: 14.5465, MAPE: 198.4983\n",
            "Fold 75\n",
            " Early stopping at epoch 12\n",
            "  MAE: 17.0198, RMSE: 18.1855, MAPE: 717.4478\n",
            "Fold 76\n",
            " Early stopping at epoch 6\n",
            "  MAE: 22.6996, RMSE: 27.2048, MAPE: 62.0435\n",
            "Fold 77\n",
            " Early stopping at epoch 28\n",
            "  MAE: 11.8639, RMSE: 14.6443, MAPE: 37.4010\n",
            "Fold 78\n",
            " Early stopping at epoch 23\n",
            "  MAE: 13.0587, RMSE: 15.5535, MAPE: 57.5494\n",
            "Fold 79\n",
            " Early stopping at epoch 17\n",
            "  MAE: 16.3027, RMSE: 19.7203, MAPE: 27.2553\n",
            "Fold 80\n",
            " Early stopping at epoch 9\n",
            "  MAE: 9.0751, RMSE: 11.3652, MAPE: 22.4083\n",
            "Fold 81\n",
            " Early stopping at epoch 34\n",
            "  MAE: 10.9748, RMSE: 13.9784, MAPE: 22.7879\n",
            "Fold 82\n",
            " Early stopping at epoch 7\n",
            "  MAE: 9.0486, RMSE: 11.6605, MAPE: 25.7103\n",
            "Fold 83\n",
            " Early stopping at epoch 49\n",
            "  MAE: 11.5704, RMSE: 13.4001, MAPE: 37.3697\n",
            "Fold 84\n",
            "  MAE: 10.5449, RMSE: 11.9828, MAPE: 22.8624\n",
            "Fold 85\n",
            " Early stopping at epoch 13\n",
            "  MAE: 11.4981, RMSE: 16.4204, MAPE: 63.6051\n",
            "Fold 86\n",
            " Early stopping at epoch 34\n",
            "  MAE: 17.5786, RMSE: 20.6695, MAPE: 30.0187\n",
            "Fold 87\n",
            " Early stopping at epoch 29\n",
            "  MAE: 11.5640, RMSE: 15.2042, MAPE: 44.4347\n",
            "Fold 88\n",
            " Early stopping at epoch 15\n",
            "  MAE: 19.7256, RMSE: 21.8242, MAPE: 54.6426\n",
            "Fold 89\n",
            " Early stopping at epoch 32\n",
            "  MAE: 12.4824, RMSE: 14.5212, MAPE: 19.4150\n",
            "Fold 90\n",
            " Early stopping at epoch 32\n",
            "  MAE: 19.7120, RMSE: 20.3401, MAPE: 32.6935\n",
            "Fold 91\n",
            " Early stopping at epoch 38\n",
            "  MAE: 19.8637, RMSE: 20.3635, MAPE: 22.7421\n",
            "Fold 92\n",
            " Early stopping at epoch 31\n",
            "  MAE: 11.0776, RMSE: 13.7385, MAPE: 16.6274\n",
            "Fold 93\n",
            " Early stopping at epoch 14\n",
            "  MAE: 13.8738, RMSE: 17.4996, MAPE: 23.8877\n",
            "Fold 94\n",
            "  MAE: 17.9370, RMSE: 21.5590, MAPE: 29.7677\n",
            "Fold 95\n",
            " Early stopping at epoch 21\n",
            "  MAE: 13.3768, RMSE: 15.2757, MAPE: 15.2509\n",
            "Fold 96\n",
            " Early stopping at epoch 34\n",
            "  MAE: 29.7125, RMSE: 31.0325, MAPE: 24.8558\n",
            "Fold 97\n",
            " Early stopping at epoch 15\n",
            "  MAE: 23.2142, RMSE: 29.7425, MAPE: 22.0053\n",
            "Fold 98\n",
            " Early stopping at epoch 24\n",
            "  MAE: 48.5462, RMSE: 66.2205, MAPE: 36.8963\n",
            "Fold 99\n",
            " Early stopping at epoch 10\n",
            "  MAE: 48.3836, RMSE: 61.6258, MAPE: 65.4871\n",
            "Fold 100\n",
            " Early stopping at epoch 13\n",
            "  MAE: 39.6317, RMSE: 53.4704, MAPE: 45.0500\n",
            "Fold 101\n",
            " Early stopping at epoch 13\n",
            "  MAE: 51.1598, RMSE: 61.1951, MAPE: 37.2191\n",
            "Fold 102\n",
            " Early stopping at epoch 24\n",
            "  MAE: 59.0025, RMSE: 70.9314, MAPE: 32.9115\n",
            "Fold 103\n",
            " Early stopping at epoch 24\n",
            "  MAE: 58.9767, RMSE: 67.7735, MAPE: 29.6276\n",
            "Fold 104\n",
            " Early stopping at epoch 12\n",
            "  MAE: 96.5488, RMSE: 129.2738, MAPE: 67.6300\n",
            "Fold 105\n",
            " Early stopping at epoch 27\n",
            "  MAE: 47.1432, RMSE: 60.4820, MAPE: 55.1496\n",
            "Fold 106\n",
            " Early stopping at epoch 9\n",
            "  MAE: 38.2025, RMSE: 42.8727, MAPE: 50.4429\n",
            "Fold 107\n",
            " Early stopping at epoch 15\n",
            "  MAE: 39.5645, RMSE: 48.9898, MAPE: 34.0779\n",
            "Fold 108\n",
            " Early stopping at epoch 9\n",
            "  MAE: 27.7921, RMSE: 42.8000, MAPE: 29.0815\n",
            "Fold 109\n",
            "  MAE: 131.3163, RMSE: 148.6907, MAPE: 52.9434\n",
            "Fold 110\n",
            " Early stopping at epoch 10\n",
            "  MAE: 39.1373, RMSE: 61.2389, MAPE: 41.1459\n",
            "Fold 111\n",
            " Early stopping at epoch 12\n",
            "  MAE: 49.7442, RMSE: 62.2480, MAPE: 57.1423\n",
            "Fold 112\n",
            " Early stopping at epoch 31\n",
            "  MAE: 76.6570, RMSE: 87.3845, MAPE: 40.3885\n",
            "Fold 113\n",
            " Early stopping at epoch 13\n",
            "  MAE: 82.5356, RMSE: 83.9968, MAPE: 37.9749\n",
            "Fold 114\n",
            " Early stopping at epoch 23\n",
            "  MAE: 29.4497, RMSE: 35.6813, MAPE: 15.9887\n",
            "Fold 115\n",
            " Early stopping at epoch 9\n",
            "  MAE: 45.8219, RMSE: 57.5819, MAPE: 59.9026\n",
            "Fold 116\n",
            " Early stopping at epoch 15\n",
            "  MAE: 39.6350, RMSE: 45.7319, MAPE: 22.9378\n",
            "Fold 117\n",
            " Early stopping at epoch 17\n",
            "  MAE: 118.4915, RMSE: 125.7255, MAPE: 41.2892\n",
            "Fold 118\n",
            " Early stopping at epoch 19\n",
            "  MAE: 59.3821, RMSE: 78.6279, MAPE: 29.1716\n",
            "Fold 119\n",
            " Early stopping at epoch 13\n",
            "  MAE: 126.3915, RMSE: 142.6173, MAPE: 36.1308\n",
            "Fold 120\n",
            " Early stopping at epoch 14\n",
            "  MAE: 63.2550, RMSE: 70.1203, MAPE: 18.7974\n",
            "Fold 121\n",
            " Early stopping at epoch 26\n",
            "  MAE: 181.3456, RMSE: 199.6554, MAPE: 32.0461\n",
            "Fold 122\n",
            " Early stopping at epoch 11\n",
            "  MAE: 101.4926, RMSE: 128.0102, MAPE: 28.3668\n",
            "Fold 123\n",
            " Early stopping at epoch 10\n",
            "  MAE: 78.7503, RMSE: 119.7163, MAPE: 69.1147\n",
            "Fold 124\n",
            " Early stopping at epoch 12\n",
            "  MAE: 130.5561, RMSE: 153.1664, MAPE: 244.6269\n",
            "Fold 125\n",
            " Early stopping at epoch 30\n",
            "  MAE: 66.3397, RMSE: 83.8874, MAPE: 32.8932\n",
            "Fold 126\n",
            " Early stopping at epoch 7\n",
            "  MAE: 38.8865, RMSE: 45.8690, MAPE: 57.4521\n",
            "Fold 127\n",
            " Early stopping at epoch 9\n",
            "  MAE: 53.7775, RMSE: 61.3319, MAPE: 413.7135\n",
            "Fold 128\n",
            " Early stopping at epoch 29\n",
            "  MAE: 128.2557, RMSE: 150.5200, MAPE: 44.0526\n",
            "Fold 129\n",
            " Early stopping at epoch 12\n",
            "  MAE: 96.3242, RMSE: 105.8328, MAPE: 26.4573\n",
            "Fold 130\n",
            " Early stopping at epoch 6\n",
            "  MAE: 206.1467, RMSE: 216.2385, MAPE: 631.4935\n",
            "Fold 131\n",
            " Early stopping at epoch 6\n",
            "  MAE: 43.2421, RMSE: 50.8314, MAPE: 63.8943\n",
            "Fold 132\n",
            " Early stopping at epoch 32\n",
            "  MAE: 25.6857, RMSE: 33.8803, MAPE: 18.1071\n",
            "Fold 133\n",
            " Early stopping at epoch 19\n",
            "  MAE: 36.7276, RMSE: 41.4131, MAPE: 37.3361\n",
            "Fold 134\n",
            " Early stopping at epoch 13\n",
            "  MAE: 26.9381, RMSE: 32.6076, MAPE: 30.9991\n",
            "Fold 135\n",
            " Early stopping at epoch 25\n",
            "  MAE: 19.0294, RMSE: 23.1260, MAPE: 15.0160\n",
            "Fold 136\n",
            " Early stopping at epoch 7\n",
            "  MAE: 35.6857, RMSE: 45.2695, MAPE: 85.0877\n",
            "Fold 137\n",
            " Early stopping at epoch 36\n",
            "  MAE: 24.3457, RMSE: 28.0819, MAPE: 22.3903\n",
            "Fold 138\n",
            " Early stopping at epoch 7\n",
            "  MAE: 30.1729, RMSE: 37.7210, MAPE: 47.7818\n",
            "Fold 139\n",
            " Early stopping at epoch 9\n",
            "  MAE: 14.2034, RMSE: 19.0851, MAPE: 19.5252\n",
            "Fold 140\n",
            " Early stopping at epoch 11\n",
            "  MAE: 21.1747, RMSE: 28.4735, MAPE: 42.2249\n",
            "Fold 141\n",
            " Early stopping at epoch 6\n",
            "  MAE: 23.5140, RMSE: 27.6991, MAPE: 59.9709\n",
            "Fold 142\n",
            " Early stopping at epoch 10\n",
            "  MAE: 38.2559, RMSE: 42.1621, MAPE: 39.8168\n",
            "Fold 143\n",
            " Early stopping at epoch 7\n",
            "  MAE: 37.3866, RMSE: 59.7543, MAPE: 53.4252\n",
            "Fold 144\n",
            " Early stopping at epoch 6\n",
            "  MAE: 26.8174, RMSE: 38.4424, MAPE: 243.9098\n",
            "Fold 145\n",
            " Early stopping at epoch 6\n",
            "  MAE: 27.1011, RMSE: 34.4701, MAPE: 69.5717\n",
            "Fold 146\n",
            " Early stopping at epoch 6\n",
            "  MAE: 26.8108, RMSE: 36.1298, MAPE: 267.1621\n",
            "Fold 147\n",
            " Early stopping at epoch 6\n",
            "  MAE: 56.4456, RMSE: 60.9321, MAPE: 49.9953\n",
            "Fold 148\n",
            " Early stopping at epoch 9\n",
            "  MAE: 14.5950, RMSE: 18.2480, MAPE: 15.8399\n",
            "Fold 149\n",
            " Early stopping at epoch 7\n",
            "  MAE: 41.7440, RMSE: 56.5552, MAPE: 1625.6989\n",
            "Fold 150\n",
            " Early stopping at epoch 6\n",
            "  MAE: 34.6862, RMSE: 42.2097, MAPE: 334.4726\n",
            "Fold 151\n",
            " Early stopping at epoch 10\n",
            "  MAE: 32.6598, RMSE: 37.2919, MAPE: 2209.7416\n",
            "Fold 152\n",
            " Early stopping at epoch 6\n",
            "  MAE: 26.1211, RMSE: 30.9676, MAPE: 47.4664\n",
            "Fold 153\n",
            " Early stopping at epoch 6\n",
            "  MAE: 34.8089, RMSE: 38.3399, MAPE: 36.3994\n",
            "Fold 154\n",
            " Early stopping at epoch 15\n",
            "  MAE: 27.8457, RMSE: 39.3325, MAPE: 42.4057\n",
            "Fold 155\n",
            " Early stopping at epoch 15\n",
            "  MAE: 31.5982, RMSE: 38.6654, MAPE: 48.4727\n",
            "Fold 156\n",
            " Early stopping at epoch 6\n",
            "  MAE: 44.0482, RMSE: 48.5744, MAPE: 212.3384\n",
            "Fold 157\n",
            " Early stopping at epoch 6\n",
            "  MAE: 38.8510, RMSE: 44.0082, MAPE: 42.4040\n",
            "Fold 158\n",
            " Early stopping at epoch 6\n",
            "  MAE: 32.4183, RMSE: 37.1149, MAPE: 61.4519\n",
            "Fold 159\n",
            " Early stopping at epoch 34\n",
            "  MAE: 17.6041, RMSE: 21.1055, MAPE: 49.5127\n",
            "Fold 160\n",
            " Early stopping at epoch 6\n",
            "  MAE: 19.3721, RMSE: 22.3294, MAPE: 36.4140\n",
            "Fold 161\n",
            " Early stopping at epoch 8\n",
            "  MAE: 11.9796, RMSE: 14.2613, MAPE: 20.6434\n",
            "Fold 162\n",
            " Early stopping at epoch 6\n",
            "  MAE: 26.1966, RMSE: 30.0239, MAPE: 53.6965\n",
            "Fold 163\n",
            " Early stopping at epoch 6\n",
            "  MAE: 20.8121, RMSE: 24.3936, MAPE: 57.3105\n",
            "Fold 164\n",
            " Early stopping at epoch 21\n",
            "  MAE: 19.7409, RMSE: 23.9476, MAPE: 68.2281\n",
            "Fold 165\n",
            " Early stopping at epoch 6\n",
            "  MAE: 23.7810, RMSE: 30.6227, MAPE: 70.8608\n",
            "Fold 166\n",
            " Early stopping at epoch 8\n",
            "  MAE: 24.7354, RMSE: 28.6294, MAPE: 80.8926\n",
            "Fold 167\n",
            " Early stopping at epoch 6\n",
            "  MAE: 31.4702, RMSE: 35.7368, MAPE: 43.9904\n",
            "Fold 168\n",
            " Early stopping at epoch 6\n",
            "  MAE: 25.6909, RMSE: 34.9542, MAPE: 134.6740\n",
            "Fold 169\n",
            " Early stopping at epoch 6\n",
            "  MAE: 16.8155, RMSE: 20.6199, MAPE: 20.8776\n",
            "Fold 170\n",
            " Early stopping at epoch 6\n",
            "  MAE: 28.8057, RMSE: 34.9900, MAPE: 130.4706\n",
            "Fold 171\n",
            " Early stopping at epoch 33\n",
            "  MAE: 11.0403, RMSE: 13.6032, MAPE: 19.4431\n",
            "Fold 172\n",
            " Early stopping at epoch 15\n",
            "  MAE: 19.4820, RMSE: 26.8956, MAPE: 80.0616\n",
            "Fold 173\n",
            " Early stopping at epoch 7\n",
            "  MAE: 29.7187, RMSE: 33.6286, MAPE: 77.0683\n",
            "Fold 174\n",
            " Early stopping at epoch 6\n",
            "  MAE: 31.6472, RMSE: 38.6288, MAPE: 34.0462\n",
            "Fold 175\n",
            " Early stopping at epoch 17\n",
            "  MAE: 13.0827, RMSE: 19.3210, MAPE: 29.8250\n",
            "Fold 176\n",
            " Early stopping at epoch 6\n",
            "  MAE: 30.3331, RMSE: 36.4182, MAPE: 93.1244\n",
            "Fold 177\n",
            " Early stopping at epoch 6\n",
            "  MAE: 25.9603, RMSE: 30.3310, MAPE: 71.4498\n",
            "Fold 178\n",
            " Early stopping at epoch 10\n",
            "  MAE: 28.6898, RMSE: 33.4648, MAPE: 76.7522\n",
            "Fold 179\n",
            " Early stopping at epoch 25\n",
            "  MAE: 33.4796, RMSE: 42.6035, MAPE: 63.1392\n",
            "Fold 180\n",
            " Early stopping at epoch 16\n",
            "  MAE: 30.4728, RMSE: 41.7430, MAPE: 218.7470\n",
            "Fold 181\n",
            " Early stopping at epoch 21\n",
            "  MAE: 65.8562, RMSE: 103.4944, MAPE: 46.9399\n",
            "Fold 182\n",
            " Early stopping at epoch 6\n",
            "  MAE: 110.2680, RMSE: 118.2208, MAPE: 329.9048\n",
            "Fold 183\n",
            " Early stopping at epoch 6\n",
            "  MAE: 40.4163, RMSE: 49.2692, MAPE: 192.8744\n",
            "Fold 184\n",
            " Early stopping at epoch 23\n",
            "  MAE: 42.5627, RMSE: 58.6096, MAPE: 40.2178\n",
            "Fold 185\n",
            " Early stopping at epoch 10\n",
            "  MAE: 22.3846, RMSE: 28.1023, MAPE: 26.0558\n",
            "Fold 186\n",
            " Early stopping at epoch 9\n",
            "  MAE: 34.4064, RMSE: 37.8996, MAPE: 32.2879\n",
            "Fold 187\n",
            " Early stopping at epoch 7\n",
            "  MAE: 30.8113, RMSE: 35.3587, MAPE: 52.3938\n",
            "Fold 188\n",
            " Early stopping at epoch 15\n",
            "  MAE: 32.0146, RMSE: 36.0978, MAPE: 61.7680\n",
            "Fold 189\n",
            " Early stopping at epoch 35\n",
            "  MAE: 20.3316, RMSE: 26.5663, MAPE: 40.7261\n",
            "Fold 190\n",
            " Early stopping at epoch 21\n",
            "  MAE: 19.0472, RMSE: 21.7367, MAPE: 30.2089\n",
            "Fold 191\n",
            " Early stopping at epoch 6\n",
            "  MAE: 20.1247, RMSE: 24.7991, MAPE: 34.6098\n",
            "Fold 192\n",
            " Early stopping at epoch 8\n",
            "  MAE: 12.1741, RMSE: 15.2675, MAPE: 14.9423\n",
            "Fold 193\n",
            " Early stopping at epoch 6\n",
            "  MAE: 24.4810, RMSE: 27.9809, MAPE: 70.4920\n",
            "Fold 194\n",
            " Early stopping at epoch 17\n",
            "  MAE: 12.3037, RMSE: 15.1719, MAPE: 26.8380\n",
            "Fold 195\n",
            " Early stopping at epoch 8\n",
            "  MAE: 20.3957, RMSE: 22.1345, MAPE: 48.8179\n",
            "Fold 196\n",
            " Early stopping at epoch 6\n",
            "  MAE: 32.3947, RMSE: 37.4824, MAPE: 39.1890\n",
            "Fold 197\n",
            " Early stopping at epoch 13\n",
            "  MAE: 12.8375, RMSE: 16.3177, MAPE: 13.4813\n",
            "\n",
            " Final CV Results:\n",
            "Average MAE:  27.5656\n",
            "Average RMSE: 33.1885\n",
            "Average MAPE: 80.8311\n"
          ]
        }
      ],
      "source": [
        "mae_scores = []\n",
        "rmse_scores = []\n",
        "mape_scores = []\n",
        "\n",
        "for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "    expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14)\n",
        "):\n",
        "    print(f\"Fold {fold+1}\")\n",
        "    mae, rmse, mape = train_one_fold(X_tr, y_tr, X_val, y_val)\n",
        "    print(f\"  MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.4f}\")\n",
        "    mae_scores.append(mae)\n",
        "    rmse_scores.append(rmse)\n",
        "    mape_scores.append(mape)\n",
        "\n",
        "print(\"\\n Final CV Results:\")\n",
        "print(f\"Average MAE:  {np.mean(mae_scores):.4f}\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):.4f}\")\n",
        "print(f\"Average MAPE: {np.mean(mape_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m0sDBsIS30E"
      },
      "source": [
        "\n",
        "\n",
        "Hyperparameter tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rjaYNxuNaTa"
      },
      "outputs": [],
      "source": [
        "def train_one_fold(X_train, y_train, X_val, y_val,\n",
        "                   hidden_dims=[64, 32], dropout=0.2,\n",
        "                   epochs=50, lr=0.001, batch_size=32, patience=5):\n",
        "\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "    # Scale input/output\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_val_scaled = scaler_X.transform(X_val)\n",
        "\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "    y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "    # PyTorch dataset and DataLoader\n",
        "    train_dataset = PriceForecastDataset(X_train_scaled, y_train_scaled)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Model\n",
        "    model = FeedforwardNN(input_dim=X_train.shape[1], hidden_dims=hidden_dims, dropout=dropout).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch)\n",
        "            loss = criterion(preds, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "            y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32).to(device)\n",
        "            val_preds = model(X_val_tensor)\n",
        "            val_loss = criterion(val_preds, y_val_tensor).item()\n",
        "\n",
        "        if val_loss < best_val_loss - 1e-4:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "         #       print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    # Final prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "        preds_scaled = model(X_val_tensor).cpu().numpy()\n",
        "        preds = scaler_y.inverse_transform(preds_scaled)\n",
        "        actuals = scaler_y.inverse_transform(y_val_scaled)\n",
        "\n",
        "    # Metrics\n",
        "    mae = mean_absolute_error(actuals.flatten(), preds.flatten())\n",
        "    rmse = np.sqrt(mean_squared_error(actuals.flatten(), preds.flatten()))\n",
        "    mape = np.mean(np.abs((actuals - preds) / (actuals + 1e-8))) * 100\n",
        "\n",
        "    return mae, rmse, mape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGKwWYaCNaWA"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "# Define hyperparameter options\n",
        "hidden_layer_options = [[64, 32], [32, 16], [128, 64, 32], [64]]\n",
        "dropout_options = [0.0, 0.2, 0.3]\n",
        "lr_options = [0.001, 0.0005]\n",
        "\n",
        "# All combinations\n",
        "param_grid = list(itertools.product(hidden_layer_options, dropout_options, lr_options))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNSVEnjXNaZa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f827f7b-32cf-41e0-dbeb-40eb2173ad9b",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Config 1/24: hidden=[64, 32], dropout=0.0, lr=0.001\n",
            "Avg MAE: 27.3507, RMSE: 32.9551, MAPE: 80.8614\n",
            "\n",
            " Config 2/24: hidden=[64, 32], dropout=0.0, lr=0.0005\n",
            "Avg MAE: 26.9973, RMSE: 32.5369, MAPE: 81.0100\n",
            "\n",
            " Config 3/24: hidden=[64, 32], dropout=0.2, lr=0.001\n",
            "Avg MAE: 27.6666, RMSE: 33.2643, MAPE: 81.7499\n",
            "\n",
            " Config 4/24: hidden=[64, 32], dropout=0.2, lr=0.0005\n",
            "Avg MAE: 27.6302, RMSE: 33.1489, MAPE: 79.0938\n",
            "\n",
            " Config 5/24: hidden=[64, 32], dropout=0.3, lr=0.001\n",
            "Avg MAE: 27.7590, RMSE: 33.1863, MAPE: 80.5522\n",
            "\n",
            " Config 6/24: hidden=[64, 32], dropout=0.3, lr=0.0005\n",
            "Avg MAE: 27.8262, RMSE: 33.4035, MAPE: 84.2523\n",
            "\n",
            " Config 7/24: hidden=[32, 16], dropout=0.0, lr=0.001\n",
            "Avg MAE: 27.1601, RMSE: 32.7037, MAPE: 82.5618\n",
            "\n",
            " Config 8/24: hidden=[32, 16], dropout=0.0, lr=0.0005\n",
            "Avg MAE: 27.3554, RMSE: 33.0033, MAPE: 79.9574\n",
            "\n",
            " Config 9/24: hidden=[32, 16], dropout=0.2, lr=0.001\n",
            "Avg MAE: 28.2018, RMSE: 33.7014, MAPE: 80.8394\n",
            "\n",
            " Config 10/24: hidden=[32, 16], dropout=0.2, lr=0.0005\n",
            "Avg MAE: 27.9062, RMSE: 33.5329, MAPE: 80.2425\n",
            "\n",
            " Config 11/24: hidden=[32, 16], dropout=0.3, lr=0.001\n",
            "Avg MAE: 28.5320, RMSE: 33.8667, MAPE: 82.4961\n",
            "\n",
            " Config 12/24: hidden=[32, 16], dropout=0.3, lr=0.0005\n",
            "Avg MAE: 28.0357, RMSE: 33.6601, MAPE: 81.1850\n",
            "\n",
            " Config 13/24: hidden=[128, 64, 32], dropout=0.0, lr=0.001\n",
            "Avg MAE: 28.5170, RMSE: 34.1725, MAPE: 82.9902\n",
            "\n",
            " Config 14/24: hidden=[128, 64, 32], dropout=0.0, lr=0.0005\n",
            "Avg MAE: 27.4459, RMSE: 33.0059, MAPE: 81.0338\n",
            "\n",
            " Config 15/24: hidden=[128, 64, 32], dropout=0.2, lr=0.001\n",
            "Avg MAE: 29.1341, RMSE: 34.7546, MAPE: 82.5931\n",
            "\n",
            " Config 16/24: hidden=[128, 64, 32], dropout=0.2, lr=0.0005\n",
            "Avg MAE: 27.7090, RMSE: 33.1712, MAPE: 82.2883\n",
            "\n",
            " Config 17/24: hidden=[128, 64, 32], dropout=0.3, lr=0.001\n",
            "Avg MAE: 29.5968, RMSE: 35.0086, MAPE: 84.2767\n",
            "\n",
            " Config 18/24: hidden=[128, 64, 32], dropout=0.3, lr=0.0005\n",
            "Avg MAE: 27.8634, RMSE: 33.3473, MAPE: 81.2011\n",
            "\n",
            " Config 19/24: hidden=[64], dropout=0.0, lr=0.001\n",
            "Avg MAE: 26.6821, RMSE: 32.2378, MAPE: 80.9410\n",
            "\n",
            " Config 20/24: hidden=[64], dropout=0.0, lr=0.0005\n",
            "Avg MAE: 26.9633, RMSE: 32.5970, MAPE: 82.9609\n",
            "\n",
            " Config 21/24: hidden=[64], dropout=0.2, lr=0.001\n",
            "Avg MAE: 27.3252, RMSE: 32.8641, MAPE: 80.7023\n",
            "\n",
            " Config 22/24: hidden=[64], dropout=0.2, lr=0.0005\n",
            "Avg MAE: 27.4604, RMSE: 32.9247, MAPE: 80.5378\n",
            "\n",
            " Config 23/24: hidden=[64], dropout=0.3, lr=0.001\n",
            "Avg MAE: 27.4136, RMSE: 32.8441, MAPE: 80.2014\n",
            "\n",
            " Config 24/24: hidden=[64], dropout=0.3, lr=0.0005\n",
            "Avg MAE: 27.6153, RMSE: 33.0985, MAPE: 80.3134\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "for i, (hidden_dims, dropout, lr) in enumerate(param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(param_grid)}: hidden={hidden_dims}, dropout={dropout}, lr={lr}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            hidden_dims=hidden_dims,\n",
        "            dropout=dropout,\n",
        "            lr=lr,\n",
        "            epochs=50,\n",
        "            patience=5,\n",
        "            batch_size=32\n",
        "        )\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    avg_mae = np.mean(mae_scores)\n",
        "    avg_rmse = np.mean(rmse_scores)\n",
        "    avg_mape = np.mean(mape_scores)\n",
        "\n",
        "    results.append({\n",
        "        \"hidden_dims\": hidden_dims,\n",
        "        \"dropout\": dropout,\n",
        "        \"lr\": lr,\n",
        "        \"MAE\": avg_mae,\n",
        "        \"RMSE\": avg_rmse,\n",
        "        \"MAPE\": avg_mape\n",
        "    })\n",
        "\n",
        "    print(f\"Avg MAE: {avg_mae:.4f}, RMSE: {avg_rmse:.4f}, MAPE: {avg_mape:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCX0zs14Nabw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "outputId": "a9d25076-ca60-4a14-d47c-eff3bc7b86b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      hidden_dims  dropout      lr        MAE       RMSE       MAPE\n",
              "0            [64]      0.0  0.0010  26.682077  32.237810  80.940988\n",
              "1        [64, 32]      0.0  0.0005  26.997264  32.536932  81.009969\n",
              "2            [64]      0.0  0.0005  26.963289  32.597032  82.960948\n",
              "3        [32, 16]      0.0  0.0010  27.160058  32.703665  82.561809\n",
              "4            [64]      0.3  0.0010  27.413612  32.844114  80.201353\n",
              "5            [64]      0.2  0.0010  27.325231  32.864061  80.702280\n",
              "6            [64]      0.2  0.0005  27.460403  32.924670  80.537835\n",
              "7        [64, 32]      0.0  0.0010  27.350739  32.955068  80.861363\n",
              "8        [32, 16]      0.0  0.0005  27.355444  33.003290  79.957396\n",
              "9   [128, 64, 32]      0.0  0.0005  27.445917  33.005884  81.033842\n",
              "10           [64]      0.3  0.0005  27.615270  33.098488  80.313369\n",
              "11       [64, 32]      0.2  0.0005  27.630181  33.148901  79.093778\n",
              "12  [128, 64, 32]      0.2  0.0005  27.709045  33.171215  82.288333\n",
              "13       [64, 32]      0.3  0.0010  27.759028  33.186299  80.552178\n",
              "14       [64, 32]      0.2  0.0010  27.666569  33.264267  81.749855\n",
              "15  [128, 64, 32]      0.3  0.0005  27.863370  33.347293  81.201078\n",
              "16       [64, 32]      0.3  0.0005  27.826153  33.403539  84.252328\n",
              "17       [32, 16]      0.2  0.0005  27.906219  33.532925  80.242462\n",
              "18       [32, 16]      0.3  0.0005  28.035731  33.660103  81.185027\n",
              "19       [32, 16]      0.2  0.0010  28.201763  33.701359  80.839382\n",
              "20       [32, 16]      0.3  0.0010  28.532032  33.866737  82.496070\n",
              "21  [128, 64, 32]      0.0  0.0010  28.517014  34.172494  82.990180\n",
              "22  [128, 64, 32]      0.2  0.0010  29.134121  34.754592  82.593054\n",
              "23  [128, 64, 32]      0.3  0.0010  29.596836  35.008593  84.276700"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d562e819-bf7b-4e63-a29b-06a9e4f9cfe9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hidden_dims</th>\n",
              "      <th>dropout</th>\n",
              "      <th>lr</th>\n",
              "      <th>MAE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[64]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>26.682077</td>\n",
              "      <td>32.237810</td>\n",
              "      <td>80.940988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[64, 32]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>26.997264</td>\n",
              "      <td>32.536932</td>\n",
              "      <td>81.009969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[64]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>26.963289</td>\n",
              "      <td>32.597032</td>\n",
              "      <td>82.960948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[32, 16]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>27.160058</td>\n",
              "      <td>32.703665</td>\n",
              "      <td>82.561809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[64]</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>27.413612</td>\n",
              "      <td>32.844114</td>\n",
              "      <td>80.201353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[64]</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>27.325231</td>\n",
              "      <td>32.864061</td>\n",
              "      <td>80.702280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[64]</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>27.460403</td>\n",
              "      <td>32.924670</td>\n",
              "      <td>80.537835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[64, 32]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>27.350739</td>\n",
              "      <td>32.955068</td>\n",
              "      <td>80.861363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[32, 16]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>27.355444</td>\n",
              "      <td>33.003290</td>\n",
              "      <td>79.957396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[128, 64, 32]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>27.445917</td>\n",
              "      <td>33.005884</td>\n",
              "      <td>81.033842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[64]</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>27.615270</td>\n",
              "      <td>33.098488</td>\n",
              "      <td>80.313369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[64, 32]</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>27.630181</td>\n",
              "      <td>33.148901</td>\n",
              "      <td>79.093778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[128, 64, 32]</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>27.709045</td>\n",
              "      <td>33.171215</td>\n",
              "      <td>82.288333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[64, 32]</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>27.759028</td>\n",
              "      <td>33.186299</td>\n",
              "      <td>80.552178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[64, 32]</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>27.666569</td>\n",
              "      <td>33.264267</td>\n",
              "      <td>81.749855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>[128, 64, 32]</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>27.863370</td>\n",
              "      <td>33.347293</td>\n",
              "      <td>81.201078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>[64, 32]</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>27.826153</td>\n",
              "      <td>33.403539</td>\n",
              "      <td>84.252328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>[32, 16]</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>27.906219</td>\n",
              "      <td>33.532925</td>\n",
              "      <td>80.242462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>[32, 16]</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>28.035731</td>\n",
              "      <td>33.660103</td>\n",
              "      <td>81.185027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>[32, 16]</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>28.201763</td>\n",
              "      <td>33.701359</td>\n",
              "      <td>80.839382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>[32, 16]</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>28.532032</td>\n",
              "      <td>33.866737</td>\n",
              "      <td>82.496070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>[128, 64, 32]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>28.517014</td>\n",
              "      <td>34.172494</td>\n",
              "      <td>82.990180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>[128, 64, 32]</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>29.134121</td>\n",
              "      <td>34.754592</td>\n",
              "      <td>82.593054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>[128, 64, 32]</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>29.596836</td>\n",
              "      <td>35.008593</td>\n",
              "      <td>84.276700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d562e819-bf7b-4e63-a29b-06a9e4f9cfe9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d562e819-bf7b-4e63-a29b-06a9e4f9cfe9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d562e819-bf7b-4e63-a29b-06a9e4f9cfe9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-06e1267c-0022-4386-9bb2-c3e5d6ca1ccf\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-06e1267c-0022-4386-9bb2-c3e5d6ca1ccf')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-06e1267c-0022-4386-9bb2-c3e5d6ca1ccf button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_62e339c5-3a96-4294-94d1-4533d80e6896\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_62e339c5-3a96-4294-94d1-4533d80e6896 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 24,\n  \"fields\": [\n    {\n      \"column\": \"hidden_dims\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12740441145412124,\n        \"min\": 0.0,\n        \"max\": 0.3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          0.3,\n          0.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0002553769592276246,\n        \"min\": 0.0005,\n        \"max\": 0.001,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0005,\n          0.001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6709697850258542,\n        \"min\": 26.682077273521948,\n        \"max\": 29.596836338221664,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          27.355443775020852,\n          27.826153351116265\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6580763687098898,\n        \"min\": 32.23781016994658,\n        \"max\": 35.00859300019452,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          33.00328960096359,\n          33.40353899329045\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.3301108664983596,\n        \"min\": 79.0937777039602,\n        \"max\": 84.2766999107147,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          79.95739598334039,\n          84.25232823740984\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results = df_results.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
        "df_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBeB2DCGZ7TD"
      },
      "source": [
        "**LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZNiUxGZ3Q0j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import trange\n",
        "\n",
        "# --- 1. LSTM Model ---\n",
        "class LSTMForecastNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=64, num_layers=1, dropout=0.2, output_size=14):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
        "                            batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)  # [batch, seq, hidden]\n",
        "        out = self.fc(lstm_out[:, -1, :])  # [batch, output_size]\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjdxCVg43Q2j"
      },
      "outputs": [],
      "source": [
        "# --- 2. Dataset Class  ---\n",
        "class LSTMForecastDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)  # [samples, seq_len, features]\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnzO0OrY3Q4w"
      },
      "outputs": [],
      "source": [
        "# --- 3. Sliding Window Input Creation ---\n",
        "def create_lstm_input(X_scaled, y_scaled, sequence_len=30):\n",
        "    X_lstm = []\n",
        "    y_lstm = []\n",
        "    for i in range(sequence_len, len(X_scaled)):\n",
        "        X_window = X_scaled[i-sequence_len:i]\n",
        "        y_target = y_scaled[i]  # already a 14-element array\n",
        "        X_lstm.append(X_window)\n",
        "        y_lstm.append(y_target)\n",
        "    return np.array(X_lstm), np.array(y_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3Iw2sV4VTtA"
      },
      "outputs": [],
      "source": [
        "# --- 4. Training One Fold ---\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_one_fold(X_train, y_train, X_val, y_val,\n",
        "                   sequence_len=30, num_epochs=50, patience=5,\n",
        "                   batch_size=32, lr=0.001, hidden_size=64, num_layers=1, dropout=0.2):\n",
        "\n",
        "    # --- 1. Scaling ---\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_val_scaled = scaler_X.transform(X_val)\n",
        "\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train)  # shape: (n_samples, 14)\n",
        "    y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "    # --- 2. Create LSTM sequences ---\n",
        "    X_train_seq, y_train_seq = create_lstm_input(X_train_scaled, y_train_scaled, sequence_len)\n",
        "    X_val_seq, y_val_seq = create_lstm_input(X_val_scaled, y_val_scaled, sequence_len)\n",
        "\n",
        "    if X_val_seq.shape[0] == 0:\n",
        "        print(\"Skipping fold: Not enough validation sequences.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # --- 3. Datasets and Loaders ---\n",
        "    train_loader = DataLoader(\n",
        "        LSTMForecastDataset(X_train_seq, y_train_seq), batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        LSTMForecastDataset(X_val_seq, y_val_seq), batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "\n",
        "    # --- 4. Model setup ---\n",
        "    model = LSTMForecastNet(\n",
        "        input_size=X_train_seq.shape[2],\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # --- 5. Training Loop with Early Stopping ---\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)  # shape: (batch_size, 14)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_preds = torch.cat([model(xb.to(device)) for xb, _ in val_loader])\n",
        "            val_targets = torch.cat([yb.to(device) for _, yb in val_loader])\n",
        "            val_loss = criterion(val_preds, val_targets)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss.item() < best_val_loss - 1e-4:\n",
        "            best_val_loss = val_loss.item()\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                break # Early stopping is triggered here\n",
        "\n",
        "    # --- 6. Load best model ---\n",
        "    if best_model_state:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    # --- 7. Final Evaluation ---\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32).to(device)\n",
        "        preds_scaled = model(X_val_tensor).cpu().numpy()\n",
        "        targets_scaled = y_val_seq  # already NumPy array\n",
        "\n",
        "    # Inverse scaling\n",
        "    preds = scaler_y.inverse_transform(preds_scaled)\n",
        "    targets = scaler_y.inverse_transform(targets_scaled)\n",
        "\n",
        "    # --- 8. Metrics ---\n",
        "    mae = mean_absolute_error(targets.flatten(), preds.flatten())\n",
        "    rmse = np.sqrt(mean_squared_error(targets.flatten(), preds.flatten()))\n",
        "    mape = mean_absolute_percentage_error(targets.flatten(), preds.flatten()) * 100\n",
        "\n",
        "    return mae, rmse, mape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbarPWuYVTvG"
      },
      "outputs": [],
      "source": [
        "# --- 5. Expanding Window Cross Validation ---\n",
        "def expanding_window_cv(X, y, initial_train_size, horizon, step, sequence_len):\n",
        "    # Ensure we have enough data for multiple sequences in validation\n",
        "    min_val_size = 60  # Should be > sequence_len + horizon to allow multiple samples\n",
        "    max_idx = len(X)\n",
        "\n",
        "    for start in range(initial_train_size, max_idx - min_val_size + 1, step):\n",
        "        end_val = start + min_val_size\n",
        "        if end_val > max_idx:\n",
        "            break\n",
        "        X_tr, y_tr = X[:start], y[:start]\n",
        "        X_val, y_val = X[start:end_val], y[start:end_val]\n",
        "        yield X_tr, y_tr, X_val, y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ztUOgQr1VloZ",
        "outputId": "ab21bb4e-4584-4f2b-96b7-60c8538c0282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1\n",
            "  MAE: 10.5256, RMSE: 12.0631, MAPE: 39.04\n",
            "\n",
            "Fold 2\n",
            "  MAE: 10.1975, RMSE: 11.6274, MAPE: 56.95\n",
            "\n",
            "Fold 3\n",
            "  MAE: 7.9343, RMSE: 9.4685, MAPE: 48.71\n",
            "\n",
            "Fold 4\n",
            "  MAE: 5.5035, RMSE: 6.4717, MAPE: 15.51\n",
            "\n",
            "Fold 5\n",
            "  MAE: 5.3425, RMSE: 6.9173, MAPE: 18.99\n",
            "\n",
            "Fold 6\n",
            "  MAE: 6.4140, RMSE: 8.1413, MAPE: 24.17\n",
            "\n",
            "Fold 7\n",
            "  MAE: 9.6063, RMSE: 10.7867, MAPE: 25.57\n",
            "\n",
            "Fold 8\n",
            "  MAE: 10.9263, RMSE: 11.6769, MAPE: 24.53\n",
            "\n",
            "Fold 9\n",
            "  MAE: 11.5167, RMSE: 12.4942, MAPE: 24.96\n",
            "\n",
            "Fold 10\n",
            "  MAE: 12.1055, RMSE: 12.9934, MAPE: 24.87\n",
            "\n",
            "Fold 11\n",
            "  MAE: 15.8082, RMSE: 16.3377, MAPE: 29.60\n",
            "\n",
            "Fold 12\n",
            "  MAE: 13.2002, RMSE: 14.4323, MAPE: 23.76\n",
            "\n",
            "Fold 13\n",
            "  MAE: 10.9116, RMSE: 12.2685, MAPE: 18.87\n",
            "\n",
            "Fold 14\n",
            "  MAE: 16.2999, RMSE: 17.7054, MAPE: 27.40\n",
            "\n",
            "Fold 15\n",
            "  MAE: 10.4617, RMSE: 12.7986, MAPE: 24.28\n",
            "\n",
            "Fold 16\n",
            "  MAE: 11.3920, RMSE: 14.1156, MAPE: 33.23\n",
            "\n",
            "Fold 17\n",
            "  MAE: 13.1145, RMSE: 16.0565, MAPE: 32.65\n",
            "\n",
            "Fold 18\n",
            "  MAE: 11.0997, RMSE: 14.1720, MAPE: 23.12\n",
            "\n",
            "Fold 19\n",
            "  MAE: 11.8058, RMSE: 14.6532, MAPE: 21.82\n",
            "\n",
            "Fold 20\n",
            "  MAE: 14.5731, RMSE: 17.8377, MAPE: 27.63\n",
            "\n",
            "Fold 21\n",
            "  MAE: 13.1276, RMSE: 16.5607, MAPE: 30.82\n",
            "\n",
            "Fold 22\n",
            "  MAE: 10.8960, RMSE: 14.3259, MAPE: 45.18\n",
            "\n",
            "Fold 23\n",
            "  MAE: 10.0050, RMSE: 14.7427, MAPE: 48.86\n",
            "\n",
            "Fold 24\n",
            "  MAE: 11.2596, RMSE: 14.8566, MAPE: 34.36\n",
            "\n",
            "Fold 25\n",
            "  MAE: 12.5322, RMSE: 16.1245, MAPE: 37.05\n",
            "\n",
            "Fold 26\n",
            "  MAE: 6.8155, RMSE: 10.7634, MAPE: 33.15\n",
            "\n",
            "Fold 27\n",
            "  MAE: 6.7874, RMSE: 10.8170, MAPE: 50.20\n",
            "\n",
            "Fold 28\n",
            "  MAE: 9.4065, RMSE: 13.6040, MAPE: 86.77\n",
            "\n",
            "Fold 29\n",
            "  MAE: 6.5881, RMSE: 9.9483, MAPE: 61.58\n",
            "\n",
            "Fold 30\n",
            "  MAE: 4.6720, RMSE: 6.9848, MAPE: 16.72\n",
            "\n",
            "Fold 31\n",
            "  MAE: 7.8031, RMSE: 11.3552, MAPE: 30.88\n",
            "\n",
            "Fold 32\n",
            "  MAE: 6.9322, RMSE: 9.9199, MAPE: 25.38\n",
            "\n",
            "Fold 33\n",
            "  MAE: 6.5775, RMSE: 9.3567, MAPE: 28.33\n",
            "\n",
            "Fold 34\n",
            "  MAE: 6.9858, RMSE: 10.0144, MAPE: 37.23\n",
            "\n",
            "Fold 35\n",
            "  MAE: 6.9110, RMSE: 9.2150, MAPE: 33.19\n",
            "\n",
            "Fold 36\n",
            "  MAE: 5.4431, RMSE: 6.4873, MAPE: 17.17\n",
            "\n",
            "Fold 37\n",
            "  MAE: 7.4546, RMSE: 8.5880, MAPE: 18.73\n",
            "\n",
            "Fold 38\n",
            "  MAE: 8.4260, RMSE: 11.0644, MAPE: 29.34\n",
            "\n",
            "Fold 39\n",
            "  MAE: 9.0229, RMSE: 12.0245, MAPE: 35.01\n",
            "\n",
            "Fold 40\n",
            "  MAE: 7.2078, RMSE: 9.0928, MAPE: 23.83\n",
            "\n",
            "Fold 41\n",
            "  MAE: 8.4830, RMSE: 10.1934, MAPE: 27.63\n",
            "\n",
            "Fold 42\n",
            "  MAE: 6.2238, RMSE: 7.8958, MAPE: 20.67\n",
            "\n",
            "Fold 43\n",
            "  MAE: 6.3118, RMSE: 7.6199, MAPE: 18.59\n",
            "\n",
            "Fold 44\n",
            "  MAE: 6.9187, RMSE: 8.0995, MAPE: 18.12\n",
            "\n",
            "Fold 45\n",
            "  MAE: 6.8374, RMSE: 8.0743, MAPE: 16.08\n",
            "\n",
            "Fold 46\n",
            "  MAE: 6.7776, RMSE: 8.7582, MAPE: 29.43\n",
            "\n",
            "Fold 47\n",
            "  MAE: 8.0808, RMSE: 11.5659, MAPE: 71.42\n",
            "\n",
            "Fold 48\n",
            "  MAE: 6.8667, RMSE: 10.4489, MAPE: 62.88\n",
            "\n",
            "Fold 49\n",
            "  MAE: 5.1303, RMSE: 6.1112, MAPE: 18.80\n",
            "\n",
            "Fold 50\n",
            "  MAE: 6.4312, RMSE: 7.6324, MAPE: 31.62\n",
            "\n",
            "Fold 51\n",
            "  MAE: 8.7690, RMSE: 10.0698, MAPE: 71.44\n",
            "\n",
            "Fold 52\n",
            "  MAE: 9.7732, RMSE: 11.1777, MAPE: 153.85\n",
            "\n",
            "Fold 53\n",
            "  MAE: 10.9975, RMSE: 12.0006, MAPE: 153.45\n",
            "\n",
            "Fold 54\n",
            "  MAE: 9.9886, RMSE: 11.3954, MAPE: 93.46\n",
            "\n",
            "Fold 55\n",
            "  MAE: 9.3932, RMSE: 11.3768, MAPE: 112.46\n",
            "\n",
            "Fold 56\n",
            "  MAE: 8.5737, RMSE: 10.7975, MAPE: 113.56\n",
            "\n",
            "Fold 57\n",
            "  MAE: 6.9464, RMSE: 9.0353, MAPE: 75.54\n",
            "\n",
            "Fold 58\n",
            "  MAE: 6.1392, RMSE: 7.4258, MAPE: 58.35\n",
            "\n",
            "Fold 59\n",
            "  MAE: 6.2216, RMSE: 7.5377, MAPE: 67.19\n",
            "\n",
            "Fold 60\n",
            "  MAE: 7.4585, RMSE: 9.1500, MAPE: 54.26\n",
            "\n",
            "Fold 61\n",
            "  MAE: 9.1678, RMSE: 11.0930, MAPE: 53.56\n",
            "\n",
            "Fold 62\n",
            "  MAE: 9.8261, RMSE: 13.1781, MAPE: 128.73\n",
            "\n",
            "Fold 63\n",
            "  MAE: 10.7645, RMSE: 14.4661, MAPE: 161.01\n",
            "\n",
            "Fold 64\n",
            "  MAE: 8.8559, RMSE: 11.3595, MAPE: 89.97\n",
            "\n",
            "Fold 65\n",
            "  MAE: 8.4211, RMSE: 10.6730, MAPE: 64.25\n",
            "\n",
            "Fold 66\n",
            "  MAE: 8.2856, RMSE: 10.2107, MAPE: 25.55\n",
            "\n",
            "Fold 67\n",
            "  MAE: 9.1170, RMSE: 12.1425, MAPE: 36.94\n",
            "\n",
            "Fold 68\n",
            "  MAE: 13.2069, RMSE: 17.0269, MAPE: 108.55\n",
            "\n",
            "Fold 69\n",
            "  MAE: 11.2376, RMSE: 13.9506, MAPE: 97.93\n",
            "\n",
            "Fold 70\n",
            "  MAE: 10.0949, RMSE: 12.8707, MAPE: 108.07\n",
            "\n",
            "Fold 71\n",
            "  MAE: 14.5919, RMSE: 16.4333, MAPE: 278.34\n",
            "\n",
            "Fold 72\n",
            "  MAE: 18.7495, RMSE: 20.8023, MAPE: 453.13\n",
            "\n",
            "Fold 73\n",
            "  MAE: 18.5692, RMSE: 22.0128, MAPE: 312.91\n",
            "\n",
            "Fold 74\n",
            "  MAE: 15.4102, RMSE: 19.7279, MAPE: 55.82\n",
            "\n",
            "Fold 75\n",
            "  MAE: 17.8955, RMSE: 21.9384, MAPE: 58.49\n",
            "\n",
            "Fold 76\n",
            "  MAE: 17.3345, RMSE: 20.9909, MAPE: 37.10\n",
            "\n",
            "Fold 77\n",
            "  MAE: 11.9273, RMSE: 14.8757, MAPE: 24.61\n",
            "\n",
            "Fold 78\n",
            "  MAE: 9.9841, RMSE: 12.6450, MAPE: 21.37\n",
            "\n",
            "Fold 79\n",
            "  MAE: 9.3725, RMSE: 12.2938, MAPE: 26.23\n",
            "\n",
            "Fold 80\n",
            "  MAE: 12.5489, RMSE: 14.2544, MAPE: 31.86\n",
            "\n",
            "Fold 81\n",
            "  MAE: 12.4250, RMSE: 15.1598, MAPE: 43.94\n",
            "\n",
            "Fold 82\n",
            "  MAE: 14.5173, RMSE: 18.2514, MAPE: 47.50\n",
            "\n",
            "Fold 83\n",
            "  MAE: 15.0987, RMSE: 18.3684, MAPE: 38.70\n",
            "\n",
            "Fold 84\n",
            "  MAE: 16.0103, RMSE: 18.3834, MAPE: 39.74\n",
            "\n",
            "Fold 85\n",
            "  MAE: 16.1336, RMSE: 18.5821, MAPE: 45.48\n",
            "\n",
            "Fold 86\n",
            "  MAE: 19.7665, RMSE: 21.7642, MAPE: 38.56\n",
            "\n",
            "Fold 87\n",
            "  MAE: 25.8999, RMSE: 27.6068, MAPE: 35.35\n",
            "\n",
            "Fold 88\n",
            "  MAE: 32.5287, RMSE: 33.9329, MAPE: 38.70\n",
            "\n",
            "Fold 89\n",
            "  MAE: 32.1618, RMSE: 34.1532, MAPE: 37.79\n",
            "\n",
            "Fold 90\n",
            "  MAE: 22.9048, RMSE: 25.6174, MAPE: 32.23\n",
            "\n",
            "Fold 91\n",
            "  MAE: 21.1003, RMSE: 24.8133, MAPE: 30.34\n",
            "\n",
            "Fold 92\n",
            "  MAE: 24.2098, RMSE: 28.8270, MAPE: 25.95\n",
            "\n",
            "Fold 93\n",
            "  MAE: 42.6683, RMSE: 46.6098, MAPE: 34.46\n",
            "\n",
            "Fold 94\n",
            "  MAE: 47.5982, RMSE: 56.9650, MAPE: 35.29\n",
            "\n",
            "Fold 95\n",
            "  MAE: 48.2408, RMSE: 62.0208, MAPE: 36.57\n",
            "\n",
            "Fold 96\n",
            "  MAE: 45.8536, RMSE: 57.9667, MAPE: 45.97\n",
            "\n",
            "Fold 97\n",
            "  MAE: 49.8261, RMSE: 57.6497, MAPE: 58.53\n",
            "\n",
            "Fold 98\n",
            "  MAE: 49.4083, RMSE: 59.1030, MAPE: 38.88\n",
            "\n",
            "Fold 99\n",
            "  MAE: 54.6988, RMSE: 69.7331, MAPE: 32.36\n",
            "\n",
            "Fold 100\n",
            "  MAE: 82.9110, RMSE: 111.5780, MAPE: 38.65\n",
            "\n",
            "Fold 101\n",
            "  MAE: 71.6469, RMSE: 101.3233, MAPE: 52.82\n",
            "\n",
            "Fold 102\n",
            "  MAE: 43.0325, RMSE: 54.7682, MAPE: 64.48\n",
            "\n",
            "Fold 103\n",
            "  MAE: 39.7497, RMSE: 50.0453, MAPE: 62.17\n",
            "\n",
            "Fold 104\n",
            "  MAE: 39.2381, RMSE: 48.6515, MAPE: 52.45\n",
            "\n",
            "Fold 105\n",
            "  MAE: 91.2219, RMSE: 128.7515, MAPE: 51.14\n",
            "\n",
            "Fold 106\n",
            "  MAE: 110.6085, RMSE: 142.9517, MAPE: 48.05\n",
            "\n",
            "Fold 107\n",
            "  MAE: 77.0636, RMSE: 100.1447, MAPE: 43.25\n",
            "\n",
            "Fold 108\n",
            "  MAE: 56.7411, RMSE: 64.3661, MAPE: 40.39\n",
            "\n",
            "Fold 109\n",
            "  MAE: 56.6898, RMSE: 62.3571, MAPE: 36.63\n",
            "\n",
            "Fold 110\n",
            "  MAE: 47.8291, RMSE: 55.1676, MAPE: 24.09\n",
            "\n",
            "Fold 111\n",
            "  MAE: 39.3908, RMSE: 49.7363, MAPE: 39.73\n",
            "\n",
            "Fold 112\n",
            "  MAE: 42.7882, RMSE: 53.7564, MAPE: 43.10\n",
            "\n",
            "Fold 113\n",
            "  MAE: 60.5322, RMSE: 76.5020, MAPE: 30.45\n",
            "\n",
            "Fold 114\n",
            "  MAE: 84.6368, RMSE: 100.5572, MAPE: 32.40\n",
            "\n",
            "Fold 115\n",
            "  MAE: 111.3994, RMSE: 132.6711, MAPE: 37.87\n",
            "\n",
            "Fold 116\n",
            "  MAE: 146.3735, RMSE: 163.9459, MAPE: 43.55\n",
            "\n",
            "Fold 117\n",
            "  MAE: 208.3489, RMSE: 237.9710, MAPE: 47.50\n",
            "\n",
            "Fold 118\n",
            "  MAE: 262.2213, RMSE: 289.7690, MAPE: 51.53\n",
            "\n",
            "Fold 119\n",
            "  MAE: 192.6607, RMSE: 226.0904, MAPE: 50.02\n",
            "\n",
            "Fold 120\n",
            "  MAE: 98.4761, RMSE: 128.4984, MAPE: 106.27\n",
            "\n",
            "Fold 121\n",
            "  MAE: 97.1958, RMSE: 121.3497, MAPE: 135.08\n",
            "\n",
            "Fold 122\n",
            "  MAE: 75.9849, RMSE: 101.9597, MAPE: 114.76\n",
            "\n",
            "Fold 123\n",
            "  MAE: 58.6452, RMSE: 67.9794, MAPE: 275.24\n",
            "\n",
            "Fold 124\n",
            "  MAE: 91.0596, RMSE: 115.2210, MAPE: 259.20\n",
            "\n",
            "Fold 125\n",
            "  MAE: 130.7064, RMSE: 150.8309, MAPE: 88.80\n",
            "\n",
            "Fold 126\n",
            "  MAE: 150.6608, RMSE: 165.8677, MAPE: 161.50\n",
            "\n",
            "Fold 127\n",
            "  MAE: 99.4634, RMSE: 121.4357, MAPE: 223.87\n",
            "\n",
            "Fold 128\n",
            "  MAE: 66.2141, RMSE: 87.8944, MAPE: 185.40\n",
            "\n",
            "Fold 129\n",
            "  MAE: 41.6818, RMSE: 50.1711, MAPE: 57.53\n",
            "\n",
            "Fold 130\n",
            "  MAE: 57.5677, RMSE: 67.4551, MAPE: 57.47\n",
            "\n",
            "Fold 131\n",
            "  MAE: 41.0690, RMSE: 49.4104, MAPE: 44.49\n",
            "\n",
            "Fold 132\n",
            "  MAE: 58.4500, RMSE: 68.2337, MAPE: 77.17\n",
            "\n",
            "Fold 133\n",
            "  MAE: 36.7541, RMSE: 46.2735, MAPE: 66.22\n",
            "\n",
            "Fold 134\n",
            "  MAE: 32.2196, RMSE: 41.0829, MAPE: 56.65\n",
            "\n",
            "Fold 135\n",
            "  MAE: 36.7343, RMSE: 43.7969, MAPE: 47.17\n",
            "\n",
            "Fold 136\n",
            "  MAE: 51.8927, RMSE: 55.9313, MAPE: 68.26\n",
            "\n",
            "Fold 137\n",
            "  MAE: 43.6383, RMSE: 50.8504, MAPE: 81.55\n",
            "\n",
            "Fold 138\n",
            "  MAE: 32.9891, RMSE: 39.9240, MAPE: 61.06\n",
            "\n",
            "Fold 139\n",
            "  MAE: 27.5353, RMSE: 36.4558, MAPE: 38.58\n",
            "\n",
            "Fold 140\n",
            "  MAE: 37.8683, RMSE: 46.9160, MAPE: 62.25\n",
            "\n",
            "Fold 141\n",
            "  MAE: 49.5945, RMSE: 63.3960, MAPE: 159.93\n",
            "\n",
            "Fold 142\n",
            "  MAE: 26.0396, RMSE: 32.7208, MAPE: 163.90\n",
            "\n",
            "Fold 143\n",
            "  MAE: 32.5210, RMSE: 40.4036, MAPE: 148.39\n",
            "\n",
            "Fold 144\n",
            "  MAE: 32.8252, RMSE: 38.1696, MAPE: 84.38\n",
            "\n",
            "Fold 145\n",
            "  MAE: 23.5424, RMSE: 35.4196, MAPE: 481.95\n",
            "\n",
            "Fold 146\n",
            "  MAE: 44.3611, RMSE: 57.7316, MAPE: 1106.54\n",
            "\n",
            "Fold 147\n",
            "  MAE: 39.7297, RMSE: 48.2386, MAPE: 1948.08\n",
            "\n",
            "Fold 148\n",
            "  MAE: 42.9096, RMSE: 50.8594, MAPE: 2032.07\n",
            "\n",
            "Fold 149\n",
            "  MAE: 34.4973, RMSE: 41.5888, MAPE: 425.23\n",
            "\n",
            "Fold 150\n",
            "  MAE: 33.6089, RMSE: 41.3234, MAPE: 58.58\n",
            "\n",
            "Fold 151\n",
            "  MAE: 23.1002, RMSE: 30.3077, MAPE: 31.80\n",
            "\n",
            "Fold 152\n",
            "  MAE: 38.4693, RMSE: 47.4009, MAPE: 118.98\n",
            "\n",
            "Fold 153\n",
            "  MAE: 64.7570, RMSE: 75.7167, MAPE: 227.77\n",
            "\n",
            "Fold 154\n",
            "  MAE: 47.2427, RMSE: 61.1359, MAPE: 136.15\n",
            "\n",
            "Fold 155\n",
            "  MAE: 27.5200, RMSE: 32.7193, MAPE: 56.69\n",
            "\n",
            "Fold 156\n",
            "  MAE: 28.3336, RMSE: 34.7802, MAPE: 66.55\n",
            "\n",
            "Fold 157\n",
            "  MAE: 16.1771, RMSE: 20.2871, MAPE: 33.38\n",
            "\n",
            "Fold 158\n",
            "  MAE: 14.8757, RMSE: 18.8604, MAPE: 27.44\n",
            "\n",
            "Fold 159\n",
            "  MAE: 13.8427, RMSE: 16.7671, MAPE: 27.09\n",
            "\n",
            "Fold 160\n",
            "  MAE: 15.8169, RMSE: 19.5076, MAPE: 44.70\n",
            "\n",
            "Fold 161\n",
            "  MAE: 22.4777, RMSE: 27.2494, MAPE: 52.64\n",
            "\n",
            "Fold 162\n",
            "  MAE: 26.7715, RMSE: 31.5288, MAPE: 62.09\n",
            "\n",
            "Fold 163\n",
            "  MAE: 26.2545, RMSE: 31.5454, MAPE: 70.94\n",
            "\n",
            "Fold 164\n",
            "  MAE: 24.0454, RMSE: 29.4687, MAPE: 72.81\n",
            "\n",
            "Fold 165\n",
            "  MAE: 20.4450, RMSE: 25.7213, MAPE: 67.34\n",
            "\n",
            "Fold 166\n",
            "  MAE: 21.0902, RMSE: 26.7324, MAPE: 74.05\n",
            "\n",
            "Fold 167\n",
            "  MAE: 20.8133, RMSE: 25.8533, MAPE: 66.69\n",
            "\n",
            "Fold 168\n",
            "  MAE: 19.0201, RMSE: 22.4207, MAPE: 40.99\n",
            "\n",
            "Fold 169\n",
            "  MAE: 24.6669, RMSE: 28.3525, MAPE: 60.00\n",
            "\n",
            "Fold 170\n",
            "  MAE: 30.2080, RMSE: 34.9232, MAPE: 73.82\n",
            "\n",
            "Fold 171\n",
            "  MAE: 27.6738, RMSE: 34.0895, MAPE: 57.46\n",
            "\n",
            "Fold 172\n",
            "  MAE: 24.3608, RMSE: 31.2561, MAPE: 55.04\n",
            "\n",
            "Fold 173\n",
            "  MAE: 24.2706, RMSE: 28.6052, MAPE: 52.22\n",
            "\n",
            "Fold 174\n",
            "  MAE: 32.2066, RMSE: 36.9930, MAPE: 57.71\n",
            "\n",
            "Fold 175\n",
            "  MAE: 37.5322, RMSE: 43.0340, MAPE: 65.26\n",
            "\n",
            "Fold 176\n",
            "  MAE: 33.2725, RMSE: 42.1354, MAPE: 120.29\n",
            "\n",
            "Fold 177\n",
            "  MAE: 39.5335, RMSE: 63.4990, MAPE: 122.44\n",
            "\n",
            "Fold 178\n",
            "  MAE: 64.4466, RMSE: 87.2853, MAPE: 156.08\n",
            "\n",
            "Fold 179\n",
            "  MAE: 61.0573, RMSE: 79.0361, MAPE: 178.89\n",
            "\n",
            "Fold 180\n",
            "  MAE: 62.8001, RMSE: 75.7590, MAPE: 193.73\n",
            "\n",
            "Fold 181\n",
            "  MAE: 42.4813, RMSE: 53.6201, MAPE: 63.16\n",
            "\n",
            "Fold 182\n",
            "  MAE: 37.3589, RMSE: 47.0233, MAPE: 34.65\n",
            "\n",
            "Fold 183\n",
            "  MAE: 34.4038, RMSE: 40.9011, MAPE: 38.99\n",
            "\n",
            "Fold 184\n",
            "  MAE: 33.3608, RMSE: 38.8532, MAPE: 46.01\n",
            "\n",
            "Fold 185\n",
            "  MAE: 27.0994, RMSE: 32.9406, MAPE: 52.68\n",
            "\n",
            "Fold 186\n",
            "  MAE: 20.8645, RMSE: 26.2097, MAPE: 49.92\n",
            "\n",
            "Fold 187\n",
            "  MAE: 20.2848, RMSE: 24.0060, MAPE: 31.91\n",
            "\n",
            "Fold 188\n",
            "  MAE: 20.0249, RMSE: 24.1574, MAPE: 26.32\n",
            "\n",
            "Fold 189\n",
            "  MAE: 18.4234, RMSE: 23.0369, MAPE: 40.87\n",
            "\n",
            "Fold 190\n",
            "  MAE: 16.5052, RMSE: 20.7775, MAPE: 42.59\n",
            "\n",
            "Fold 191\n",
            "  MAE: 15.2797, RMSE: 19.4254, MAPE: 36.35\n",
            "\n",
            "Fold 192\n",
            "  MAE: 20.7814, RMSE: 26.1443, MAPE: 40.54\n",
            "\n",
            "Fold 193\n",
            "  MAE: 19.1663, RMSE: 25.1655, MAPE: 33.94\n",
            "\n",
            " Final CV Results:\n",
            "Average MAE:  32.2433\n",
            "Average RMSE: 39.3130\n",
            "Average MAPE: 94.5379\n"
          ]
        }
      ],
      "source": [
        "# --- 6. Main CV Execution ---\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"dropout option adds dropout after all but last recurrent layer\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"To copy construct from a tensor\")\n",
        "\n",
        "sequence_len = 30\n",
        "horizon = 14\n",
        "initial_train_size = 1095  # 3 years\n",
        "\n",
        "mae_scores = []\n",
        "rmse_scores = []\n",
        "mape_scores = []\n",
        "\n",
        "for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "    expanding_window_cv(X, y, initial_train_size=initial_train_size, horizon=horizon, step=horizon, sequence_len=sequence_len)\n",
        "):\n",
        "    print(f\"\\nFold {fold+1}\")\n",
        "    mae, rmse, mape = train_one_fold(\n",
        "        X_tr, y_tr, X_val, y_val, sequence_len=sequence_len\n",
        "    )\n",
        "    if mae is None:\n",
        "        continue  # Skip if fold was invalid\n",
        "    print(f\"  MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.4f}\")\n",
        "    mae_scores.append(mae)\n",
        "    rmse_scores.append(rmse)\n",
        "    mape_scores.append(mape)\n",
        "\n",
        "# --- 7. Final Results ---\n",
        "print(\"\\n Final CV Results:\")\n",
        "print(f\"Average MAE:  {np.mean(mae_scores):.4f}\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):.4f}\")\n",
        "print(f\"Average MAPE: {np.mean(mape_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWOLlbw59lU5"
      },
      "source": [
        "Hyperparamter tunning 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlCRNyGmVlqe",
        "outputId": "92a1b913-6421-41bc-8603-9996b4ffab74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Total combinations: 36\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "# Define hyperparameter grid\n",
        "hidden_sizes = [32, 64, 128]\n",
        "num_layers_list = [1, 2, 3]\n",
        "dropouts = [0.0, 0.1, 0.2, 0.3]\n",
        "learning_rates = [0.001] # subsistute with 0.0005\n",
        "batch_sizes = [32] # substitute with 64\n",
        "sequence_lens = [30]\n",
        "\n",
        "# Create grid of all combinations\n",
        "param_grid = list(itertools.product(\n",
        "    hidden_sizes,\n",
        "    num_layers_list,\n",
        "    dropouts,\n",
        "    learning_rates,\n",
        "    batch_sizes,\n",
        "    sequence_lens\n",
        "))\n",
        "\n",
        "print(f\"Total combinations: {len(param_grid)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1wXTpcNVls2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6d8cca-46a3-4c88-b2cf-f8dd6abee5bc",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Config 1/36: hidden=32, layers=1, dropout=0.0, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 2/36: hidden=32, layers=1, dropout=0.1, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 3/36: hidden=32, layers=1, dropout=0.2, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 4/36: hidden=32, layers=1, dropout=0.3, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 5/36: hidden=32, layers=2, dropout=0.0, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 6/36: hidden=32, layers=2, dropout=0.1, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 7/36: hidden=32, layers=2, dropout=0.2, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 8/36: hidden=32, layers=2, dropout=0.3, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 9/36: hidden=32, layers=3, dropout=0.0, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 10/36: hidden=32, layers=3, dropout=0.1, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 11/36: hidden=32, layers=3, dropout=0.2, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 12/36: hidden=32, layers=3, dropout=0.3, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 13/36: hidden=64, layers=1, dropout=0.0, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 14/36: hidden=64, layers=1, dropout=0.1, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 15/36: hidden=64, layers=1, dropout=0.2, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 16/36: hidden=64, layers=1, dropout=0.3, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 17/36: hidden=64, layers=2, dropout=0.0, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 18/36: hidden=64, layers=2, dropout=0.1, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 19/36: hidden=64, layers=2, dropout=0.2, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 20/36: hidden=64, layers=2, dropout=0.3, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 21/36: hidden=64, layers=3, dropout=0.0, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 22/36: hidden=64, layers=3, dropout=0.1, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 23/36: hidden=64, layers=3, dropout=0.2, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 24/36: hidden=64, layers=3, dropout=0.3, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 25/36: hidden=128, layers=1, dropout=0.0, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 26/36: hidden=128, layers=1, dropout=0.1, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 27/36: hidden=128, layers=1, dropout=0.2, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 28/36: hidden=128, layers=1, dropout=0.3, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 29/36: hidden=128, layers=2, dropout=0.0, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 30/36: hidden=128, layers=2, dropout=0.1, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 31/36: hidden=128, layers=2, dropout=0.2, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 32/36: hidden=128, layers=2, dropout=0.3, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 33/36: hidden=128, layers=3, dropout=0.0, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 34/36: hidden=128, layers=3, dropout=0.1, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 35/36: hidden=128, layers=3, dropout=0.2, lr=0.001, batch_size=32, seq_len=30\n",
            "\n",
            " Config 36/36: hidden=128, layers=3, dropout=0.3, lr=0.001, batch_size=32, seq_len=30\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "for i, (hidden_size, num_layers, dropout, lr, batch_size, sequence_len) in enumerate(param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(param_grid)}: hidden={hidden_size}, layers={num_layers}, dropout={dropout}, \"\n",
        "          f\"lr={lr}, batch_size={batch_size}, seq_len={sequence_len}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y,\n",
        "                            initial_train_size=1095,\n",
        "                            horizon=14,\n",
        "                            step=14,\n",
        "                            sequence_len=sequence_len)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            sequence_len=sequence_len,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue  # skip invalid fold\n",
        "\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    if mae_scores:  # only record results if at least one fold succeeded\n",
        "        results.append({\n",
        "            \"hidden_size\": hidden_size,\n",
        "            \"num_layers\": num_layers,\n",
        "            \"dropout\": dropout,\n",
        "            \"lr\": lr,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"sequence_len\": sequence_len,\n",
        "            \"MAE\": np.mean(mae_scores),\n",
        "            \"RMSE\": np.mean(rmse_scores),\n",
        "            \"MAPE\": np.mean(mape_scores)\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2328YGMVTxk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "75902660-ad57-4e98-f12a-9564f96f633c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    hidden_size  num_layers  dropout     lr  batch_size  sequence_len  \\\n",
              "0            32           1      0.3  0.001          32            30   \n",
              "1            64           1      0.3  0.001          32            30   \n",
              "2            64           1      0.0  0.001          32            30   \n",
              "3            32           1      0.2  0.001          32            30   \n",
              "4            32           1      0.0  0.001          32            30   \n",
              "5            64           1      0.2  0.001          32            30   \n",
              "6            32           1      0.1  0.001          32            30   \n",
              "7           128           1      0.0  0.001          32            30   \n",
              "8           128           1      0.3  0.001          32            30   \n",
              "9           128           1      0.1  0.001          32            30   \n",
              "10          128           1      0.2  0.001          32            30   \n",
              "11           64           1      0.1  0.001          32            30   \n",
              "12           64           2      0.1  0.001          32            30   \n",
              "13           32           2      0.1  0.001          32            30   \n",
              "14           32           2      0.0  0.001          32            30   \n",
              "15           32           2      0.2  0.001          32            30   \n",
              "16           64           2      0.2  0.001          32            30   \n",
              "17           32           2      0.3  0.001          32            30   \n",
              "18           64           2      0.0  0.001          32            30   \n",
              "19           64           2      0.3  0.001          32            30   \n",
              "20           32           3      0.0  0.001          32            30   \n",
              "21           32           3      0.3  0.001          32            30   \n",
              "22           32           3      0.1  0.001          32            30   \n",
              "23           32           3      0.2  0.001          32            30   \n",
              "24          128           2      0.0  0.001          32            30   \n",
              "25           64           3      0.0  0.001          32            30   \n",
              "26           64           3      0.2  0.001          32            30   \n",
              "27           64           3      0.3  0.001          32            30   \n",
              "28           64           3      0.1  0.001          32            30   \n",
              "29          128           2      0.1  0.001          32            30   \n",
              "30          128           2      0.2  0.001          32            30   \n",
              "31          128           2      0.3  0.001          32            30   \n",
              "32          128           3      0.3  0.001          32            30   \n",
              "33          128           3      0.2  0.001          32            30   \n",
              "34          128           3      0.0  0.001          32            30   \n",
              "35          128           3      0.1  0.001          32            30   \n",
              "\n",
              "          MAE       RMSE        MAPE  \n",
              "0   32.280940  39.277221   92.953494  \n",
              "1   32.253175  39.309602   94.525646  \n",
              "2   32.351470  39.368226   93.607509  \n",
              "3   32.369872  39.380065   93.521553  \n",
              "4   32.443834  39.389070   92.909008  \n",
              "5   32.419429  39.470682   93.462581  \n",
              "6   32.794088  39.782033   93.209023  \n",
              "7   32.874109  39.821238   93.028981  \n",
              "8   32.947169  39.893616   93.532364  \n",
              "9   32.908045  39.935081   96.058793  \n",
              "10  32.978963  40.001425   96.020287  \n",
              "11  33.057508  40.155621   96.357082  \n",
              "12  33.438300  40.545355   98.275359  \n",
              "13  33.853904  40.765311   97.112943  \n",
              "14  33.774139  40.823503   96.589702  \n",
              "15  33.918120  40.864017   97.084682  \n",
              "16  34.020254  41.036985   97.617062  \n",
              "17  34.271754  41.199883   95.880155  \n",
              "18  34.235987  41.231386   95.840192  \n",
              "19  34.523737  41.467906   98.070893  \n",
              "20  36.102304  42.967361  102.645145  \n",
              "21  36.416406  43.249631  103.928879  \n",
              "22  36.613949  43.499263  105.527482  \n",
              "23  36.896660  43.701712  103.670822  \n",
              "24  37.462980  44.370053  104.028830  \n",
              "25  38.008399  44.656838  110.307687  \n",
              "26  37.895804  44.671828  107.815356  \n",
              "27  37.901812  44.762440  105.579692  \n",
              "28  38.191107  45.137725  110.831364  \n",
              "29  38.671095  45.634678  103.505762  \n",
              "30  38.770956  45.671988  103.360516  \n",
              "31  39.234368  46.270968  103.311870  \n",
              "32  44.744204  51.969377  126.459300  \n",
              "33  45.350902  52.479784  129.049220  \n",
              "34  46.372382  53.331633  144.219761  \n",
              "35  46.433312  53.530145  126.064705  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5f194a66-7790-4326-b675-1bb6e4fa97a0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hidden_size</th>\n",
              "      <th>num_layers</th>\n",
              "      <th>dropout</th>\n",
              "      <th>lr</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>sequence_len</th>\n",
              "      <th>MAE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>32.280940</td>\n",
              "      <td>39.277221</td>\n",
              "      <td>92.953494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>32.253175</td>\n",
              "      <td>39.309602</td>\n",
              "      <td>94.525646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>32.351470</td>\n",
              "      <td>39.368226</td>\n",
              "      <td>93.607509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>32.369872</td>\n",
              "      <td>39.380065</td>\n",
              "      <td>93.521553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>32.443834</td>\n",
              "      <td>39.389070</td>\n",
              "      <td>92.909008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>32.419429</td>\n",
              "      <td>39.470682</td>\n",
              "      <td>93.462581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>32.794088</td>\n",
              "      <td>39.782033</td>\n",
              "      <td>93.209023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>32.874109</td>\n",
              "      <td>39.821238</td>\n",
              "      <td>93.028981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>32.947169</td>\n",
              "      <td>39.893616</td>\n",
              "      <td>93.532364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>32.908045</td>\n",
              "      <td>39.935081</td>\n",
              "      <td>96.058793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>32.978963</td>\n",
              "      <td>40.001425</td>\n",
              "      <td>96.020287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>33.057508</td>\n",
              "      <td>40.155621</td>\n",
              "      <td>96.357082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>33.438300</td>\n",
              "      <td>40.545355</td>\n",
              "      <td>98.275359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>33.853904</td>\n",
              "      <td>40.765311</td>\n",
              "      <td>97.112943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>33.774139</td>\n",
              "      <td>40.823503</td>\n",
              "      <td>96.589702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>33.918120</td>\n",
              "      <td>40.864017</td>\n",
              "      <td>97.084682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>34.020254</td>\n",
              "      <td>41.036985</td>\n",
              "      <td>97.617062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>34.271754</td>\n",
              "      <td>41.199883</td>\n",
              "      <td>95.880155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>34.235987</td>\n",
              "      <td>41.231386</td>\n",
              "      <td>95.840192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>34.523737</td>\n",
              "      <td>41.467906</td>\n",
              "      <td>98.070893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>36.102304</td>\n",
              "      <td>42.967361</td>\n",
              "      <td>102.645145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>36.416406</td>\n",
              "      <td>43.249631</td>\n",
              "      <td>103.928879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>36.613949</td>\n",
              "      <td>43.499263</td>\n",
              "      <td>105.527482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>36.896660</td>\n",
              "      <td>43.701712</td>\n",
              "      <td>103.670822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>37.462980</td>\n",
              "      <td>44.370053</td>\n",
              "      <td>104.028830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>38.008399</td>\n",
              "      <td>44.656838</td>\n",
              "      <td>110.307687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>37.895804</td>\n",
              "      <td>44.671828</td>\n",
              "      <td>107.815356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>37.901812</td>\n",
              "      <td>44.762440</td>\n",
              "      <td>105.579692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>38.191107</td>\n",
              "      <td>45.137725</td>\n",
              "      <td>110.831364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>38.671095</td>\n",
              "      <td>45.634678</td>\n",
              "      <td>103.505762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>38.770956</td>\n",
              "      <td>45.671988</td>\n",
              "      <td>103.360516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>39.234368</td>\n",
              "      <td>46.270968</td>\n",
              "      <td>103.311870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>44.744204</td>\n",
              "      <td>51.969377</td>\n",
              "      <td>126.459300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>45.350902</td>\n",
              "      <td>52.479784</td>\n",
              "      <td>129.049220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>46.372382</td>\n",
              "      <td>53.331633</td>\n",
              "      <td>144.219761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>46.433312</td>\n",
              "      <td>53.530145</td>\n",
              "      <td>126.064705</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f194a66-7790-4326-b675-1bb6e4fa97a0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5f194a66-7790-4326-b675-1bb6e4fa97a0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5f194a66-7790-4326-b675-1bb6e4fa97a0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2f17cd65-0da6-43c7-be17-c8f77988a8ef\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2f17cd65-0da6-43c7-be17-c8f77988a8ef')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2f17cd65-0da6-43c7-be17-c8f77988a8ef button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_9693e7ac-455c-4af9-ac3a-8034003aca90\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9693e7ac-455c-4af9-ac3a-8034003aca90 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 36,\n  \"fields\": [\n    {\n      \"column\": \"hidden_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 40,\n        \"min\": 32,\n        \"max\": 128,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          32,\n          64,\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11338934190276818,\n        \"min\": 0.0,\n        \"max\": 0.3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0,\n          0.1,\n          0.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.597490169246581e-19,\n        \"min\": 0.001,\n        \"max\": 0.001,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sequence_len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 30,\n        \"max\": 30,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          30\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.120142629768218,\n        \"min\": 32.253174778529555,\n        \"max\": 46.43331176059698,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          46.43331176059698\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.131024333041193,\n        \"min\": 39.27722075970803,\n        \"max\": 53.530145168655594,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          53.530145168655594\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11.79309947761761,\n        \"min\": 92.90900789201704,\n        \"max\": 144.21976070537744,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          126.06470478286016\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "df_results = pd.DataFrame(results)\n",
        "df_results = df_results.sort_values(\"RMSE\").reset_index(drop=True)\n",
        "df_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tunning 2"
      ],
      "metadata": {
        "id": "kH3FJfUe_HAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "# Define hyperparameter grid\n",
        "hidden_sizes = [32, 64, 128]\n",
        "num_layers_list = [1, 2, 3]\n",
        "dropouts = [0.0, 0.1, 0.2, 0.3]\n",
        "learning_rates = [0.0005]\n",
        "batch_sizes = [32] # substitute with 64\n",
        "sequence_lens = [30]\n",
        "\n",
        "# Create grid of all combinations\n",
        "param_grid = list(itertools.product(\n",
        "    hidden_sizes,\n",
        "    num_layers_list,\n",
        "    dropouts,\n",
        "    learning_rates,\n",
        "    batch_sizes,\n",
        "    sequence_lens\n",
        "))\n",
        "\n",
        "print(f\"Total combinations: {len(param_grid)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lop1gFmK_GLn",
        "outputId": "11c305b5-cbd9-4dad-98f0-4b08d7e4b8a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Total combinations: 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for i, (hidden_size, num_layers, dropout, lr, batch_size, sequence_len) in enumerate(param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(param_grid)}: hidden={hidden_size}, layers={num_layers}, dropout={dropout}, \"\n",
        "          f\"lr={lr}, batch_size={batch_size}, seq_len={sequence_len}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y,\n",
        "                            initial_train_size=1095,\n",
        "                            horizon=14,\n",
        "                            step=14,\n",
        "                            sequence_len=sequence_len)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            sequence_len=sequence_len,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue  # skip invalid fold\n",
        "\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    if mae_scores:  # only record results if at least one fold succeeded\n",
        "        results.append({\n",
        "            \"hidden_size\": hidden_size,\n",
        "            \"num_layers\": num_layers,\n",
        "            \"dropout\": dropout,\n",
        "            \"lr\": lr,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"sequence_len\": sequence_len,\n",
        "            \"MAE\": np.mean(mae_scores),\n",
        "            \"RMSE\": np.mean(rmse_scores),\n",
        "            \"MAPE\": np.mean(mape_scores)\n",
        "        })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM9E_dSZ_Gbs",
        "outputId": "59e335e9-9c40-4ae0-b3cc-476976d48f3a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Config 1/36: hidden=32, layers=1, dropout=0.0, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 2/36: hidden=32, layers=1, dropout=0.1, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 3/36: hidden=32, layers=1, dropout=0.2, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 4/36: hidden=32, layers=1, dropout=0.3, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 5/36: hidden=32, layers=2, dropout=0.0, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 6/36: hidden=32, layers=2, dropout=0.1, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 7/36: hidden=32, layers=2, dropout=0.2, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 8/36: hidden=32, layers=2, dropout=0.3, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 9/36: hidden=32, layers=3, dropout=0.0, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 10/36: hidden=32, layers=3, dropout=0.1, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 11/36: hidden=32, layers=3, dropout=0.2, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 12/36: hidden=32, layers=3, dropout=0.3, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 13/36: hidden=64, layers=1, dropout=0.0, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 14/36: hidden=64, layers=1, dropout=0.1, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 15/36: hidden=64, layers=1, dropout=0.2, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 16/36: hidden=64, layers=1, dropout=0.3, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 17/36: hidden=64, layers=2, dropout=0.0, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 18/36: hidden=64, layers=2, dropout=0.1, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 19/36: hidden=64, layers=2, dropout=0.2, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 20/36: hidden=64, layers=2, dropout=0.3, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 21/36: hidden=64, layers=3, dropout=0.0, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 22/36: hidden=64, layers=3, dropout=0.1, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 23/36: hidden=64, layers=3, dropout=0.2, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 24/36: hidden=64, layers=3, dropout=0.3, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 25/36: hidden=128, layers=1, dropout=0.0, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 26/36: hidden=128, layers=1, dropout=0.1, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 27/36: hidden=128, layers=1, dropout=0.2, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 28/36: hidden=128, layers=1, dropout=0.3, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 29/36: hidden=128, layers=2, dropout=0.0, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 30/36: hidden=128, layers=2, dropout=0.1, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 31/36: hidden=128, layers=2, dropout=0.2, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 32/36: hidden=128, layers=2, dropout=0.3, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 33/36: hidden=128, layers=3, dropout=0.0, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 34/36: hidden=128, layers=3, dropout=0.1, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 35/36: hidden=128, layers=3, dropout=0.2, lr=0.0005, batch_size=32, seq_len=30\n",
            "\n",
            " Config 36/36: hidden=128, layers=3, dropout=0.3, lr=0.0005, batch_size=32, seq_len=30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(results)\n",
        "df_results = df_results.sort_values(\"RMSE\").reset_index(drop=True)\n",
        "df_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gp0j-FSd_Gp4",
        "outputId": "2b84b230-166a-44b2-e8da-3e672b2d5d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    hidden_size  num_layers  dropout      lr  batch_size  sequence_len  \\\n",
              "0           128           1      0.1  0.0005          32            30   \n",
              "1           128           1      0.0  0.0005          32            30   \n",
              "2           128           1      0.3  0.0005          32            30   \n",
              "3            64           1      0.2  0.0005          32            30   \n",
              "4            64           1      0.0  0.0005          32            30   \n",
              "5            64           1      0.3  0.0005          32            30   \n",
              "6            64           1      0.1  0.0005          32            30   \n",
              "7           128           1      0.2  0.0005          32            30   \n",
              "8           128           2      0.2  0.0005          32            30   \n",
              "9           128           2      0.3  0.0005          32            30   \n",
              "10           32           1      0.3  0.0005          32            30   \n",
              "11          128           2      0.1  0.0005          32            30   \n",
              "12           32           1      0.2  0.0005          32            30   \n",
              "13          128           2      0.0  0.0005          32            30   \n",
              "14           32           1      0.0  0.0005          32            30   \n",
              "15           32           1      0.1  0.0005          32            30   \n",
              "16           64           2      0.3  0.0005          32            30   \n",
              "17           64           2      0.0  0.0005          32            30   \n",
              "18           64           2      0.1  0.0005          32            30   \n",
              "19           32           2      0.1  0.0005          32            30   \n",
              "20           64           2      0.2  0.0005          32            30   \n",
              "21           32           2      0.0  0.0005          32            30   \n",
              "22           32           2      0.3  0.0005          32            30   \n",
              "23           32           2      0.2  0.0005          32            30   \n",
              "24           32           3      0.0  0.0005          32            30   \n",
              "25           32           3      0.1  0.0005          32            30   \n",
              "26           32           3      0.2  0.0005          32            30   \n",
              "27           64           3      0.0  0.0005          32            30   \n",
              "28           64           3      0.1  0.0005          32            30   \n",
              "29           64           3      0.3  0.0005          32            30   \n",
              "30           64           3      0.2  0.0005          32            30   \n",
              "31          128           3      0.1  0.0005          32            30   \n",
              "32           32           3      0.3  0.0005          32            30   \n",
              "33          128           3      0.0  0.0005          32            30   \n",
              "34          128           3      0.2  0.0005          32            30   \n",
              "35          128           3      0.3  0.0005          32            30   \n",
              "\n",
              "          MAE       RMSE        MAPE  \n",
              "0   31.989463  39.010419   93.621934  \n",
              "1   32.030244  39.075393   93.287363  \n",
              "2   32.161362  39.191993   94.412604  \n",
              "3   32.201674  39.206890   93.123525  \n",
              "4   32.433503  39.375493   93.828892  \n",
              "5   32.473873  39.386544   92.980448  \n",
              "6   32.484350  39.408041   93.480679  \n",
              "7   32.428825  39.472570   95.636723  \n",
              "8   32.625675  39.621058   94.560428  \n",
              "9   32.781212  39.667261   93.740078  \n",
              "10  33.025885  40.040624   95.446486  \n",
              "11  33.103801  40.075120   94.559575  \n",
              "12  33.191227  40.127077   96.801622  \n",
              "13  33.112997  40.139673   97.323653  \n",
              "14  33.185560  40.207588   95.034886  \n",
              "15  33.190061  40.214029   95.594047  \n",
              "16  33.407246  40.366980   97.044410  \n",
              "17  33.692513  40.587726   96.901222  \n",
              "18  33.682446  40.632485   97.284512  \n",
              "19  33.702639  40.658456   96.111323  \n",
              "20  33.856654  40.789114   97.673471  \n",
              "21  34.045621  40.956271   98.495744  \n",
              "22  34.059325  41.060608   97.544679  \n",
              "23  34.301162  41.195200   98.617906  \n",
              "24  35.378255  42.366039  103.101310  \n",
              "25  35.493499  42.405205  101.888246  \n",
              "26  35.680123  42.670111  102.593593  \n",
              "27  35.927683  42.830084  102.652995  \n",
              "28  36.005173  42.848694  104.177978  \n",
              "29  36.094668  42.939248  103.375780  \n",
              "30  36.223514  43.120463  103.588567  \n",
              "31  36.673163  43.335077  103.306473  \n",
              "32  36.394971  43.394010  104.020887  \n",
              "33  36.760148  43.511986  103.101351  \n",
              "34  36.723122  43.573461  106.593648  \n",
              "35  37.149473  43.946250  102.851236  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f0e4cd47-c0a5-48f8-ab80-6f9dfe984dd1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hidden_size</th>\n",
              "      <th>num_layers</th>\n",
              "      <th>dropout</th>\n",
              "      <th>lr</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>sequence_len</th>\n",
              "      <th>MAE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>31.989463</td>\n",
              "      <td>39.010419</td>\n",
              "      <td>93.621934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>32.030244</td>\n",
              "      <td>39.075393</td>\n",
              "      <td>93.287363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>32.161362</td>\n",
              "      <td>39.191993</td>\n",
              "      <td>94.412604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>32.201674</td>\n",
              "      <td>39.206890</td>\n",
              "      <td>93.123525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>32.433503</td>\n",
              "      <td>39.375493</td>\n",
              "      <td>93.828892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>32.473873</td>\n",
              "      <td>39.386544</td>\n",
              "      <td>92.980448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>32.484350</td>\n",
              "      <td>39.408041</td>\n",
              "      <td>93.480679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>32.428825</td>\n",
              "      <td>39.472570</td>\n",
              "      <td>95.636723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>32.625675</td>\n",
              "      <td>39.621058</td>\n",
              "      <td>94.560428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>32.781212</td>\n",
              "      <td>39.667261</td>\n",
              "      <td>93.740078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>33.025885</td>\n",
              "      <td>40.040624</td>\n",
              "      <td>95.446486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>33.103801</td>\n",
              "      <td>40.075120</td>\n",
              "      <td>94.559575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>33.191227</td>\n",
              "      <td>40.127077</td>\n",
              "      <td>96.801622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>33.112997</td>\n",
              "      <td>40.139673</td>\n",
              "      <td>97.323653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>33.185560</td>\n",
              "      <td>40.207588</td>\n",
              "      <td>95.034886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>33.190061</td>\n",
              "      <td>40.214029</td>\n",
              "      <td>95.594047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>33.407246</td>\n",
              "      <td>40.366980</td>\n",
              "      <td>97.044410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>33.692513</td>\n",
              "      <td>40.587726</td>\n",
              "      <td>96.901222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>33.682446</td>\n",
              "      <td>40.632485</td>\n",
              "      <td>97.284512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>33.702639</td>\n",
              "      <td>40.658456</td>\n",
              "      <td>96.111323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>33.856654</td>\n",
              "      <td>40.789114</td>\n",
              "      <td>97.673471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>34.045621</td>\n",
              "      <td>40.956271</td>\n",
              "      <td>98.495744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>34.059325</td>\n",
              "      <td>41.060608</td>\n",
              "      <td>97.544679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>34.301162</td>\n",
              "      <td>41.195200</td>\n",
              "      <td>98.617906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>35.378255</td>\n",
              "      <td>42.366039</td>\n",
              "      <td>103.101310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>35.493499</td>\n",
              "      <td>42.405205</td>\n",
              "      <td>101.888246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>35.680123</td>\n",
              "      <td>42.670111</td>\n",
              "      <td>102.593593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>35.927683</td>\n",
              "      <td>42.830084</td>\n",
              "      <td>102.652995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>36.005173</td>\n",
              "      <td>42.848694</td>\n",
              "      <td>104.177978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>36.094668</td>\n",
              "      <td>42.939248</td>\n",
              "      <td>103.375780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>36.223514</td>\n",
              "      <td>43.120463</td>\n",
              "      <td>103.588567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>36.673163</td>\n",
              "      <td>43.335077</td>\n",
              "      <td>103.306473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>36.394971</td>\n",
              "      <td>43.394010</td>\n",
              "      <td>104.020887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>36.760148</td>\n",
              "      <td>43.511986</td>\n",
              "      <td>103.101351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>36.723122</td>\n",
              "      <td>43.573461</td>\n",
              "      <td>106.593648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30</td>\n",
              "      <td>37.149473</td>\n",
              "      <td>43.946250</td>\n",
              "      <td>102.851236</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f0e4cd47-c0a5-48f8-ab80-6f9dfe984dd1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f0e4cd47-c0a5-48f8-ab80-6f9dfe984dd1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f0e4cd47-c0a5-48f8-ab80-6f9dfe984dd1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-fedf2b43-3e38-48da-be0d-a4719e089d5b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fedf2b43-3e38-48da-be0d-a4719e089d5b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-fedf2b43-3e38-48da-be0d-a4719e089d5b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_47888b55-514d-479b-a47a-ee2d7a42383a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_47888b55-514d-479b-a47a-ee2d7a42383a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 36,\n  \"fields\": [\n    {\n      \"column\": \"hidden_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 40,\n        \"min\": 32,\n        \"max\": 128,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          128,\n          64,\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11338934190276816,\n        \"min\": 0.0,\n        \"max\": 0.3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0,\n          0.2,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2987450846232904e-19,\n        \"min\": 0.0005,\n        \"max\": 0.0005,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0005\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sequence_len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 30,\n        \"max\": 30,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          30\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.640606291409447,\n        \"min\": 31.98946295377919,\n        \"max\": 37.149472715036026,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          37.149472715036026\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.5835228451456977,\n        \"min\": 39.01041933854215,\n        \"max\": 43.94624970838601,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          43.94624970838601\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.096920066645727,\n        \"min\": 92.98044824944245,\n        \"max\": 106.59364789953983,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          102.85123564717912\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tunning 3"
      ],
      "metadata": {
        "id": "teHMQw8Z_X5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "# Define hyperparameter grid\n",
        "hidden_sizes = [32, 64, 128]\n",
        "num_layers_list = [1, 2, 3]\n",
        "dropouts = [0.0, 0.1, 0.2, 0.3]\n",
        "learning_rates = [0.001]\n",
        "batch_sizes = [64]\n",
        "sequence_lens = [30]\n",
        "\n",
        "# Create grid of all combinations\n",
        "param_grid = list(itertools.product(\n",
        "    hidden_sizes,\n",
        "    num_layers_list,\n",
        "    dropouts,\n",
        "    learning_rates,\n",
        "    batch_sizes,\n",
        "    sequence_lens\n",
        "))\n",
        "\n",
        "print(f\"Total combinations: {len(param_grid)}\")"
      ],
      "metadata": {
        "id": "hK_zRdLm_YTE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b64c781-08d4-4d4e-8fe1-b7ab63954017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Total combinations: 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for i, (hidden_size, num_layers, dropout, lr, batch_size, sequence_len) in enumerate(param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(param_grid)}: hidden={hidden_size}, layers={num_layers}, dropout={dropout}, \"\n",
        "          f\"lr={lr}, batch_size={batch_size}, seq_len={sequence_len}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y,\n",
        "                            initial_train_size=1095,\n",
        "                            horizon=14,\n",
        "                            step=14,\n",
        "                            sequence_len=sequence_len)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            sequence_len=sequence_len,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue  # skip invalid fold\n",
        "\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    if mae_scores:  # only record results if at least one fold succeeded\n",
        "        results.append({\n",
        "            \"hidden_size\": hidden_size,\n",
        "            \"num_layers\": num_layers,\n",
        "            \"dropout\": dropout,\n",
        "            \"lr\": lr,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"sequence_len\": sequence_len,\n",
        "            \"MAE\": np.mean(mae_scores),\n",
        "            \"RMSE\": np.mean(rmse_scores),\n",
        "            \"MAPE\": np.mean(mape_scores)\n",
        "        })"
      ],
      "metadata": {
        "id": "wug6QdiA_Yer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5749a87-c5c0-4826-98dd-954c2758cc16",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Config 1/36: hidden=32, layers=1, dropout=0.0, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 2/36: hidden=32, layers=1, dropout=0.1, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 3/36: hidden=32, layers=1, dropout=0.2, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 4/36: hidden=32, layers=1, dropout=0.3, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 5/36: hidden=32, layers=2, dropout=0.0, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 6/36: hidden=32, layers=2, dropout=0.1, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 7/36: hidden=32, layers=2, dropout=0.2, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 8/36: hidden=32, layers=2, dropout=0.3, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 9/36: hidden=32, layers=3, dropout=0.0, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 10/36: hidden=32, layers=3, dropout=0.1, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 11/36: hidden=32, layers=3, dropout=0.2, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 12/36: hidden=32, layers=3, dropout=0.3, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 13/36: hidden=64, layers=1, dropout=0.0, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 14/36: hidden=64, layers=1, dropout=0.1, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 15/36: hidden=64, layers=1, dropout=0.2, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 16/36: hidden=64, layers=1, dropout=0.3, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 17/36: hidden=64, layers=2, dropout=0.0, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 18/36: hidden=64, layers=2, dropout=0.1, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 19/36: hidden=64, layers=2, dropout=0.2, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 20/36: hidden=64, layers=2, dropout=0.3, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 21/36: hidden=64, layers=3, dropout=0.0, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 22/36: hidden=64, layers=3, dropout=0.1, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 23/36: hidden=64, layers=3, dropout=0.2, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 24/36: hidden=64, layers=3, dropout=0.3, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 25/36: hidden=128, layers=1, dropout=0.0, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 26/36: hidden=128, layers=1, dropout=0.1, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 27/36: hidden=128, layers=1, dropout=0.2, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 28/36: hidden=128, layers=1, dropout=0.3, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 29/36: hidden=128, layers=2, dropout=0.0, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 30/36: hidden=128, layers=2, dropout=0.1, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 31/36: hidden=128, layers=2, dropout=0.2, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 32/36: hidden=128, layers=2, dropout=0.3, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 33/36: hidden=128, layers=3, dropout=0.0, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 34/36: hidden=128, layers=3, dropout=0.1, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 35/36: hidden=128, layers=3, dropout=0.2, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 36/36: hidden=128, layers=3, dropout=0.3, lr=0.001, batch_size=64, seq_len=30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(results)\n",
        "df_results = df_results.sort_values(\"RMSE\").reset_index(drop=True)\n",
        "df_results"
      ],
      "metadata": {
        "id": "mdHlMRF2_Yo5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c7d61103-b176-412d-c6c8-aceabd3e3e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    hidden_size  num_layers  dropout     lr  batch_size  sequence_len  \\\n",
              "0            64           1      0.1  0.001          64            30   \n",
              "1            64           1      0.3  0.001          64            30   \n",
              "2            64           1      0.2  0.001          64            30   \n",
              "3           128           1      0.0  0.001          64            30   \n",
              "4           128           1      0.2  0.001          64            30   \n",
              "5           128           1      0.1  0.001          64            30   \n",
              "6            64           1      0.0  0.001          64            30   \n",
              "7           128           1      0.3  0.001          64            30   \n",
              "8            32           1      0.0  0.001          64            30   \n",
              "9            32           1      0.3  0.001          64            30   \n",
              "10           32           1      0.2  0.001          64            30   \n",
              "11           32           1      0.1  0.001          64            30   \n",
              "12           64           2      0.1  0.001          64            30   \n",
              "13          128           2      0.1  0.001          64            30   \n",
              "14           64           2      0.2  0.001          64            30   \n",
              "15          128           2      0.2  0.001          64            30   \n",
              "16           32           2      0.2  0.001          64            30   \n",
              "17           64           2      0.0  0.001          64            30   \n",
              "18           32           2      0.0  0.001          64            30   \n",
              "19           64           2      0.3  0.001          64            30   \n",
              "20           32           2      0.3  0.001          64            30   \n",
              "21          128           2      0.3  0.001          64            30   \n",
              "22           32           2      0.1  0.001          64            30   \n",
              "23          128           2      0.0  0.001          64            30   \n",
              "24           32           3      0.0  0.001          64            30   \n",
              "25           32           3      0.1  0.001          64            30   \n",
              "26           64           3      0.3  0.001          64            30   \n",
              "27           32           3      0.2  0.001          64            30   \n",
              "28          128           3      0.0  0.001          64            30   \n",
              "29           64           3      0.2  0.001          64            30   \n",
              "30          128           3      0.2  0.001          64            30   \n",
              "31           64           3      0.1  0.001          64            30   \n",
              "32           32           3      0.3  0.001          64            30   \n",
              "33           64           3      0.0  0.001          64            30   \n",
              "34          128           3      0.1  0.001          64            30   \n",
              "35          128           3      0.3  0.001          64            30   \n",
              "\n",
              "          MAE       RMSE        MAPE  \n",
              "0   31.740778  38.724843   90.856150  \n",
              "1   31.811634  38.816433   90.180240  \n",
              "2   31.803220  38.817081   89.708351  \n",
              "3   31.815912  38.843725   92.649494  \n",
              "4   31.888756  38.856672   93.157969  \n",
              "5   31.924930  38.976558   92.855464  \n",
              "6   32.027410  39.021642   91.803662  \n",
              "7   32.066919  39.081068   93.793686  \n",
              "8   32.522283  39.540283   93.827970  \n",
              "9   32.636247  39.672210   93.822617  \n",
              "10  32.743557  39.756206   93.661016  \n",
              "11  32.772259  39.776222   92.923529  \n",
              "12  33.141629  40.096377   97.034515  \n",
              "13  32.999596  40.099477   94.422385  \n",
              "14  33.224661  40.200900   95.830304  \n",
              "15  33.341554  40.259725   93.437829  \n",
              "16  33.347307  40.435781   94.181755  \n",
              "17  33.520134  40.454392   97.326402  \n",
              "18  33.469536  40.473293   95.570296  \n",
              "19  33.585932  40.489638   95.016324  \n",
              "20  33.617356  40.671213   95.021314  \n",
              "21  33.683924  40.683737   97.400143  \n",
              "22  33.778109  40.808130   95.769200  \n",
              "23  34.344326  41.327975   99.726500  \n",
              "24  35.150246  42.085416   99.388397  \n",
              "25  35.316704  42.238947  100.740767  \n",
              "26  35.809176  42.670237  101.136950  \n",
              "27  35.641282  42.720522  102.409256  \n",
              "28  36.048018  42.775807  101.466908  \n",
              "29  36.015126  42.987413  104.044825  \n",
              "30  36.308383  42.993543  102.072768  \n",
              "31  36.047297  43.073796   99.696448  \n",
              "32  36.252846  43.168244  102.070365  \n",
              "33  36.214965  43.181015  102.385816  \n",
              "34  36.442897  43.218167   99.928232  \n",
              "35  36.850796  43.489282   97.149657  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-552ac372-9f43-4cba-8898-ae4b737a7987\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hidden_size</th>\n",
              "      <th>num_layers</th>\n",
              "      <th>dropout</th>\n",
              "      <th>lr</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>sequence_len</th>\n",
              "      <th>MAE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>31.740778</td>\n",
              "      <td>38.724843</td>\n",
              "      <td>90.856150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>31.811634</td>\n",
              "      <td>38.816433</td>\n",
              "      <td>90.180240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>31.803220</td>\n",
              "      <td>38.817081</td>\n",
              "      <td>89.708351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>31.815912</td>\n",
              "      <td>38.843725</td>\n",
              "      <td>92.649494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>31.888756</td>\n",
              "      <td>38.856672</td>\n",
              "      <td>93.157969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>31.924930</td>\n",
              "      <td>38.976558</td>\n",
              "      <td>92.855464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>32.027410</td>\n",
              "      <td>39.021642</td>\n",
              "      <td>91.803662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>32.066919</td>\n",
              "      <td>39.081068</td>\n",
              "      <td>93.793686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>32.522283</td>\n",
              "      <td>39.540283</td>\n",
              "      <td>93.827970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>32.636247</td>\n",
              "      <td>39.672210</td>\n",
              "      <td>93.822617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>32.743557</td>\n",
              "      <td>39.756206</td>\n",
              "      <td>93.661016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>32.772259</td>\n",
              "      <td>39.776222</td>\n",
              "      <td>92.923529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.141629</td>\n",
              "      <td>40.096377</td>\n",
              "      <td>97.034515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>32.999596</td>\n",
              "      <td>40.099477</td>\n",
              "      <td>94.422385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.224661</td>\n",
              "      <td>40.200900</td>\n",
              "      <td>95.830304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.341554</td>\n",
              "      <td>40.259725</td>\n",
              "      <td>93.437829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.347307</td>\n",
              "      <td>40.435781</td>\n",
              "      <td>94.181755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.520134</td>\n",
              "      <td>40.454392</td>\n",
              "      <td>97.326402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.469536</td>\n",
              "      <td>40.473293</td>\n",
              "      <td>95.570296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.585932</td>\n",
              "      <td>40.489638</td>\n",
              "      <td>95.016324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.617356</td>\n",
              "      <td>40.671213</td>\n",
              "      <td>95.021314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.683924</td>\n",
              "      <td>40.683737</td>\n",
              "      <td>97.400143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.778109</td>\n",
              "      <td>40.808130</td>\n",
              "      <td>95.769200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>34.344326</td>\n",
              "      <td>41.327975</td>\n",
              "      <td>99.726500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>35.150246</td>\n",
              "      <td>42.085416</td>\n",
              "      <td>99.388397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>35.316704</td>\n",
              "      <td>42.238947</td>\n",
              "      <td>100.740767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>35.809176</td>\n",
              "      <td>42.670237</td>\n",
              "      <td>101.136950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>35.641282</td>\n",
              "      <td>42.720522</td>\n",
              "      <td>102.409256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>36.048018</td>\n",
              "      <td>42.775807</td>\n",
              "      <td>101.466908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>36.015126</td>\n",
              "      <td>42.987413</td>\n",
              "      <td>104.044825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>36.308383</td>\n",
              "      <td>42.993543</td>\n",
              "      <td>102.072768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>36.047297</td>\n",
              "      <td>43.073796</td>\n",
              "      <td>99.696448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>36.252846</td>\n",
              "      <td>43.168244</td>\n",
              "      <td>102.070365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>36.214965</td>\n",
              "      <td>43.181015</td>\n",
              "      <td>102.385816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>36.442897</td>\n",
              "      <td>43.218167</td>\n",
              "      <td>99.928232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>36.850796</td>\n",
              "      <td>43.489282</td>\n",
              "      <td>97.149657</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-552ac372-9f43-4cba-8898-ae4b737a7987')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-552ac372-9f43-4cba-8898-ae4b737a7987 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-552ac372-9f43-4cba-8898-ae4b737a7987');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-aaf42c37-df0e-4265-afe2-eb2ba1e0e698\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aaf42c37-df0e-4265-afe2-eb2ba1e0e698')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-aaf42c37-df0e-4265-afe2-eb2ba1e0e698 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_b9ffdf72-1a71-4293-928d-e1a601201fc6\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b9ffdf72-1a71-4293-928d-e1a601201fc6 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 36,\n  \"fields\": [\n    {\n      \"column\": \"hidden_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 40,\n        \"min\": 32,\n        \"max\": 128,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          64,\n          128,\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11338934190276818,\n        \"min\": 0.0,\n        \"max\": 0.3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.3,\n          0.0,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.597490169246581e-19,\n        \"min\": 0.001,\n        \"max\": 0.001,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sequence_len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 30,\n        \"max\": 30,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          30\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.6708825771264104,\n        \"min\": 31.74077819396353,\n        \"max\": 36.85079633011571,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          36.85079633011571\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.607782343652051,\n        \"min\": 38.72484254558212,\n        \"max\": 43.489281974171874,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          43.489281974171874\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.9374540228895505,\n        \"min\": 89.70835056813765,\n        \"max\": 104.0448252457766,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          97.14965688394642\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tunning 4"
      ],
      "metadata": {
        "id": "oXO6_j_q_Y7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "# Define hyperparameter grid\n",
        "hidden_sizes = [32, 64, 128]\n",
        "num_layers_list = [1, 2, 3]\n",
        "dropouts = [0.0, 0.1, 0.2, 0.3]\n",
        "learning_rates = [0.001]\n",
        "batch_sizes = [64]\n",
        "sequence_lens = [30]\n",
        "\n",
        "# Create grid of all combinations\n",
        "param_grid = list(itertools.product(\n",
        "    hidden_sizes,\n",
        "    num_layers_list,\n",
        "    dropouts,\n",
        "    learning_rates,\n",
        "    batch_sizes,\n",
        "    sequence_lens\n",
        "))\n",
        "\n",
        "print(f\"Total combinations: {len(param_grid)}\")"
      ],
      "metadata": {
        "id": "IUPbHUT6_ZTT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c419b92c-9904-45f6-f988-494878d7dbb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Total combinations: 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for i, (hidden_size, num_layers, dropout, lr, batch_size, sequence_len) in enumerate(param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(param_grid)}: hidden={hidden_size}, layers={num_layers}, dropout={dropout}, \"\n",
        "          f\"lr={lr}, batch_size={batch_size}, seq_len={sequence_len}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y,\n",
        "                            initial_train_size=1095,\n",
        "                            horizon=14,\n",
        "                            step=14,\n",
        "                            sequence_len=sequence_len)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            sequence_len=sequence_len,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue  # skip invalid fold\n",
        "\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    if mae_scores:  # only record results if at least one fold succeeded\n",
        "        results.append({\n",
        "            \"hidden_size\": hidden_size,\n",
        "            \"num_layers\": num_layers,\n",
        "            \"dropout\": dropout,\n",
        "            \"lr\": lr,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"sequence_len\": sequence_len,\n",
        "            \"MAE\": np.mean(mae_scores),\n",
        "            \"RMSE\": np.mean(rmse_scores),\n",
        "            \"MAPE\": np.mean(mape_scores)\n",
        "        })"
      ],
      "metadata": {
        "id": "rEWlFOeA_Zb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c2e9ac-04fd-464d-ccaf-18ea29ccf49c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Config 1/36: hidden=32, layers=1, dropout=0.0, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 2/36: hidden=32, layers=1, dropout=0.1, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 3/36: hidden=32, layers=1, dropout=0.2, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 4/36: hidden=32, layers=1, dropout=0.3, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 5/36: hidden=32, layers=2, dropout=0.0, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 6/36: hidden=32, layers=2, dropout=0.1, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 7/36: hidden=32, layers=2, dropout=0.2, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 8/36: hidden=32, layers=2, dropout=0.3, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 9/36: hidden=32, layers=3, dropout=0.0, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 10/36: hidden=32, layers=3, dropout=0.1, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 11/36: hidden=32, layers=3, dropout=0.2, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 12/36: hidden=32, layers=3, dropout=0.3, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 13/36: hidden=64, layers=1, dropout=0.0, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 14/36: hidden=64, layers=1, dropout=0.1, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 15/36: hidden=64, layers=1, dropout=0.2, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 16/36: hidden=64, layers=1, dropout=0.3, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 17/36: hidden=64, layers=2, dropout=0.0, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 18/36: hidden=64, layers=2, dropout=0.1, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 19/36: hidden=64, layers=2, dropout=0.2, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 20/36: hidden=64, layers=2, dropout=0.3, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 21/36: hidden=64, layers=3, dropout=0.0, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 22/36: hidden=64, layers=3, dropout=0.1, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 23/36: hidden=64, layers=3, dropout=0.2, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 24/36: hidden=64, layers=3, dropout=0.3, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 25/36: hidden=128, layers=1, dropout=0.0, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 26/36: hidden=128, layers=1, dropout=0.1, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 27/36: hidden=128, layers=1, dropout=0.2, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 28/36: hidden=128, layers=1, dropout=0.3, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 29/36: hidden=128, layers=2, dropout=0.0, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 30/36: hidden=128, layers=2, dropout=0.1, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 31/36: hidden=128, layers=2, dropout=0.2, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 32/36: hidden=128, layers=2, dropout=0.3, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 33/36: hidden=128, layers=3, dropout=0.0, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 34/36: hidden=128, layers=3, dropout=0.1, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 35/36: hidden=128, layers=3, dropout=0.2, lr=0.001, batch_size=64, seq_len=30\n",
            "\n",
            " Config 36/36: hidden=128, layers=3, dropout=0.3, lr=0.001, batch_size=64, seq_len=30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(results)\n",
        "df_results = df_results.sort_values(\"RMSE\").reset_index(drop=True)\n",
        "df_results"
      ],
      "metadata": {
        "id": "oZ-GWwQv_ZlI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "acd3a790-191b-4302-cd08-03431b1a4729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    hidden_size  num_layers  dropout     lr  batch_size  sequence_len  \\\n",
              "0            64           1      0.0  0.001          64            30   \n",
              "1            64           1      0.2  0.001          64            30   \n",
              "2            64           1      0.3  0.001          64            30   \n",
              "3           128           1      0.1  0.001          64            30   \n",
              "4            64           1      0.1  0.001          64            30   \n",
              "5           128           1      0.0  0.001          64            30   \n",
              "6           128           1      0.2  0.001          64            30   \n",
              "7           128           1      0.3  0.001          64            30   \n",
              "8            32           1      0.2  0.001          64            30   \n",
              "9            32           1      0.1  0.001          64            30   \n",
              "10           32           1      0.3  0.001          64            30   \n",
              "11           32           2      0.1  0.001          64            30   \n",
              "12          128           2      0.2  0.001          64            30   \n",
              "13           32           1      0.0  0.001          64            30   \n",
              "14          128           2      0.1  0.001          64            30   \n",
              "15           32           2      0.2  0.001          64            30   \n",
              "16           64           2      0.1  0.001          64            30   \n",
              "17          128           2      0.3  0.001          64            30   \n",
              "18           64           2      0.0  0.001          64            30   \n",
              "19           32           2      0.3  0.001          64            30   \n",
              "20           64           2      0.3  0.001          64            30   \n",
              "21           32           2      0.0  0.001          64            30   \n",
              "22           64           2      0.2  0.001          64            30   \n",
              "23          128           2      0.0  0.001          64            30   \n",
              "24           32           3      0.0  0.001          64            30   \n",
              "25           32           3      0.2  0.001          64            30   \n",
              "26           32           3      0.3  0.001          64            30   \n",
              "27          128           3      0.3  0.001          64            30   \n",
              "28          128           3      0.1  0.001          64            30   \n",
              "29           32           3      0.1  0.001          64            30   \n",
              "30           64           3      0.0  0.001          64            30   \n",
              "31           64           3      0.3  0.001          64            30   \n",
              "32          128           3      0.0  0.001          64            30   \n",
              "33           64           3      0.2  0.001          64            30   \n",
              "34           64           3      0.1  0.001          64            30   \n",
              "35          128           3      0.2  0.001          64            30   \n",
              "\n",
              "          MAE       RMSE        MAPE  \n",
              "0   31.570565  38.519481   90.762250  \n",
              "1   31.723004  38.625461   91.024280  \n",
              "2   31.694365  38.689136   91.036445  \n",
              "3   31.846351  38.803713   91.054587  \n",
              "4   31.865811  38.832720   91.207396  \n",
              "5   31.890369  38.942444   91.766658  \n",
              "6   32.035692  39.084214   91.924754  \n",
              "7   32.142804  39.170698   90.108185  \n",
              "8   32.411718  39.388347   92.897095  \n",
              "9   32.702341  39.814171   94.447595  \n",
              "10  33.069686  40.021199   93.811378  \n",
              "11  33.036312  40.047119   94.127534  \n",
              "12  33.225652  40.065875   91.777828  \n",
              "13  33.133658  40.148403   95.215605  \n",
              "14  33.276026  40.228176   92.906295  \n",
              "15  33.395619  40.353194   95.259515  \n",
              "16  33.444223  40.397067   95.768963  \n",
              "17  33.494032  40.464108   94.979768  \n",
              "18  33.553732  40.485295   96.422819  \n",
              "19  33.477430  40.511245   96.141090  \n",
              "20  33.741435  40.563395   94.106918  \n",
              "21  33.550186  40.609417   95.357852  \n",
              "22  33.724326  40.609632   94.614703  \n",
              "23  33.767547  40.652013   94.421758  \n",
              "24  35.215772  42.115704  101.514256  \n",
              "25  35.151972  42.119656   99.501731  \n",
              "26  35.970941  42.851787  100.503698  \n",
              "27  36.217581  43.017361   94.418427  \n",
              "28  36.307723  43.047435   99.997676  \n",
              "29  36.138014  43.064109   99.588706  \n",
              "30  36.204610  43.177408  100.326697  \n",
              "31  36.450730  43.334117  104.998346  \n",
              "32  36.780620  43.503702   99.734702  \n",
              "33  36.824994  43.763819  103.686911  \n",
              "34  36.967289  43.768853  103.453312  \n",
              "35  37.542539  44.220120  102.603593  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a224d2da-0f11-487c-b903-fb0440baf29e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hidden_size</th>\n",
              "      <th>num_layers</th>\n",
              "      <th>dropout</th>\n",
              "      <th>lr</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>sequence_len</th>\n",
              "      <th>MAE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>31.570565</td>\n",
              "      <td>38.519481</td>\n",
              "      <td>90.762250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>31.723004</td>\n",
              "      <td>38.625461</td>\n",
              "      <td>91.024280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>31.694365</td>\n",
              "      <td>38.689136</td>\n",
              "      <td>91.036445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>31.846351</td>\n",
              "      <td>38.803713</td>\n",
              "      <td>91.054587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>31.865811</td>\n",
              "      <td>38.832720</td>\n",
              "      <td>91.207396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>31.890369</td>\n",
              "      <td>38.942444</td>\n",
              "      <td>91.766658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>32.035692</td>\n",
              "      <td>39.084214</td>\n",
              "      <td>91.924754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>32.142804</td>\n",
              "      <td>39.170698</td>\n",
              "      <td>90.108185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>32.411718</td>\n",
              "      <td>39.388347</td>\n",
              "      <td>92.897095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>32.702341</td>\n",
              "      <td>39.814171</td>\n",
              "      <td>94.447595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.069686</td>\n",
              "      <td>40.021199</td>\n",
              "      <td>93.811378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.036312</td>\n",
              "      <td>40.047119</td>\n",
              "      <td>94.127534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.225652</td>\n",
              "      <td>40.065875</td>\n",
              "      <td>91.777828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.133658</td>\n",
              "      <td>40.148403</td>\n",
              "      <td>95.215605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.276026</td>\n",
              "      <td>40.228176</td>\n",
              "      <td>92.906295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.395619</td>\n",
              "      <td>40.353194</td>\n",
              "      <td>95.259515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.444223</td>\n",
              "      <td>40.397067</td>\n",
              "      <td>95.768963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.494032</td>\n",
              "      <td>40.464108</td>\n",
              "      <td>94.979768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.553732</td>\n",
              "      <td>40.485295</td>\n",
              "      <td>96.422819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.477430</td>\n",
              "      <td>40.511245</td>\n",
              "      <td>96.141090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.741435</td>\n",
              "      <td>40.563395</td>\n",
              "      <td>94.106918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.550186</td>\n",
              "      <td>40.609417</td>\n",
              "      <td>95.357852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.724326</td>\n",
              "      <td>40.609632</td>\n",
              "      <td>94.614703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.767547</td>\n",
              "      <td>40.652013</td>\n",
              "      <td>94.421758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>35.215772</td>\n",
              "      <td>42.115704</td>\n",
              "      <td>101.514256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>35.151972</td>\n",
              "      <td>42.119656</td>\n",
              "      <td>99.501731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>35.970941</td>\n",
              "      <td>42.851787</td>\n",
              "      <td>100.503698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>36.217581</td>\n",
              "      <td>43.017361</td>\n",
              "      <td>94.418427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>36.307723</td>\n",
              "      <td>43.047435</td>\n",
              "      <td>99.997676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>36.138014</td>\n",
              "      <td>43.064109</td>\n",
              "      <td>99.588706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>36.204610</td>\n",
              "      <td>43.177408</td>\n",
              "      <td>100.326697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>36.450730</td>\n",
              "      <td>43.334117</td>\n",
              "      <td>104.998346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>36.780620</td>\n",
              "      <td>43.503702</td>\n",
              "      <td>99.734702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>36.824994</td>\n",
              "      <td>43.763819</td>\n",
              "      <td>103.686911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>36.967289</td>\n",
              "      <td>43.768853</td>\n",
              "      <td>103.453312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>30</td>\n",
              "      <td>37.542539</td>\n",
              "      <td>44.220120</td>\n",
              "      <td>102.603593</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a224d2da-0f11-487c-b903-fb0440baf29e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a224d2da-0f11-487c-b903-fb0440baf29e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a224d2da-0f11-487c-b903-fb0440baf29e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-cbd8e5ef-1f31-407e-ad92-f66836282197\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cbd8e5ef-1f31-407e-ad92-f66836282197')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-cbd8e5ef-1f31-407e-ad92-f66836282197 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_ea77b88a-6273-4568-87da-5923916473b3\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ea77b88a-6273-4568-87da-5923916473b3 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 36,\n  \"fields\": [\n    {\n      \"column\": \"hidden_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 40,\n        \"min\": 32,\n        \"max\": 128,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          64,\n          128,\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11338934190276818,\n        \"min\": 0.0,\n        \"max\": 0.3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.2,\n          0.1,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.597490169246581e-19,\n        \"min\": 0.001,\n        \"max\": 0.001,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sequence_len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 30,\n        \"max\": 30,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          30\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.8241416898572174,\n        \"min\": 31.570564564270992,\n        \"max\": 37.54253894806417,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          37.54253894806417\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.7605910492169747,\n        \"min\": 38.5194806221684,\n        \"max\": 44.220120272527,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          44.220120272527\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.201094745717227,\n        \"min\": 90.10818462138235,\n        \"max\": 104.99834588693898,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          102.60359334101372\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "311MqJSaCE-y"
      },
      "source": [
        "More restricted grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1106
        },
        "id": "T3aaPhgICETl",
        "outputId": "22a63650-a48f-4f37-e1bf-3a9b327dae3e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Config 1/16  hidden: 64, layers: 1, lr: 0.001, dropout: 0.1, seq_len: 30\n",
            "\n",
            " Config 2/16  hidden: 64, layers: 1, lr: 0.001, dropout: 0.3, seq_len: 30\n",
            "\n",
            " Config 3/16  hidden: 64, layers: 1, lr: 0.0005, dropout: 0.1, seq_len: 30\n",
            "\n",
            " Config 4/16  hidden: 64, layers: 1, lr: 0.0005, dropout: 0.3, seq_len: 30\n",
            "\n",
            " Config 5/16  hidden: 64, layers: 2, lr: 0.001, dropout: 0.1, seq_len: 30\n",
            "\n",
            " Config 6/16  hidden: 64, layers: 2, lr: 0.001, dropout: 0.3, seq_len: 30\n",
            "\n",
            " Config 7/16  hidden: 64, layers: 2, lr: 0.0005, dropout: 0.1, seq_len: 30\n",
            "\n",
            " Config 8/16  hidden: 64, layers: 2, lr: 0.0005, dropout: 0.3, seq_len: 30\n",
            "\n",
            " Config 9/16  hidden: 128, layers: 1, lr: 0.001, dropout: 0.1, seq_len: 30\n",
            "\n",
            " Config 10/16  hidden: 128, layers: 1, lr: 0.001, dropout: 0.3, seq_len: 30\n",
            "\n",
            " Config 11/16  hidden: 128, layers: 1, lr: 0.0005, dropout: 0.1, seq_len: 30\n",
            "\n",
            " Config 12/16  hidden: 128, layers: 1, lr: 0.0005, dropout: 0.3, seq_len: 30\n",
            "\n",
            " Config 13/16  hidden: 128, layers: 2, lr: 0.001, dropout: 0.1, seq_len: 30\n",
            "\n",
            " Config 14/16  hidden: 128, layers: 2, lr: 0.001, dropout: 0.3, seq_len: 30\n",
            "\n",
            " Config 15/16  hidden: 128, layers: 2, lr: 0.0005, dropout: 0.1, seq_len: 30\n",
            "\n",
            " Config 16/16  hidden: 128, layers: 2, lr: 0.0005, dropout: 0.3, seq_len: 30\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    hidden_size  num_layers      lr  dropout  sequence_len        MAE  \\\n",
              "0           128           1  0.0005      0.1            30  31.597565   \n",
              "1           128           1  0.0010      0.1            30  31.627081   \n",
              "2            64           1  0.0010      0.1            30  31.612207   \n",
              "3            64           1  0.0010      0.3            30  31.668869   \n",
              "4           128           1  0.0005      0.3            30  31.709690   \n",
              "5           128           1  0.0010      0.3            30  32.159439   \n",
              "6            64           1  0.0005      0.1            30  32.484008   \n",
              "7            64           1  0.0005      0.3            30  32.692376   \n",
              "8            64           2  0.0005      0.1            30  32.996730   \n",
              "9           128           2  0.0010      0.3            30  33.184928   \n",
              "10          128           2  0.0005      0.3            30  33.201778   \n",
              "11           64           2  0.0005      0.3            30  33.150219   \n",
              "12           64           2  0.0010      0.3            30  33.391824   \n",
              "13          128           2  0.0005      0.1            30  33.390831   \n",
              "14           64           2  0.0010      0.1            30  33.582112   \n",
              "15          128           2  0.0010      0.1            30  33.706064   \n",
              "\n",
              "         RMSE       MAPE  \n",
              "0   38.555926  90.170319  \n",
              "1   38.561881  92.077852  \n",
              "2   38.619039  89.960110  \n",
              "3   38.688395  92.812037  \n",
              "4   38.689317  91.470480  \n",
              "5   39.110712  94.202051  \n",
              "6   39.427880  94.260430  \n",
              "7   39.714518  94.046512  \n",
              "8   40.008485  94.827266  \n",
              "9   40.073987  91.779999  \n",
              "10  40.133292  93.973839  \n",
              "11  40.167392  95.642329  \n",
              "12  40.281256  95.918650  \n",
              "13  40.310470  93.538706  \n",
              "14  40.581542  94.595322  \n",
              "15  40.611090  94.183894  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6943a452-e6f5-4dd6-92d7-d33f9f0cab90\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hidden_size</th>\n",
              "      <th>num_layers</th>\n",
              "      <th>lr</th>\n",
              "      <th>dropout</th>\n",
              "      <th>sequence_len</th>\n",
              "      <th>MAE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.1</td>\n",
              "      <td>30</td>\n",
              "      <td>31.597565</td>\n",
              "      <td>38.555926</td>\n",
              "      <td>90.170319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.1</td>\n",
              "      <td>30</td>\n",
              "      <td>31.627081</td>\n",
              "      <td>38.561881</td>\n",
              "      <td>92.077852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.1</td>\n",
              "      <td>30</td>\n",
              "      <td>31.612207</td>\n",
              "      <td>38.619039</td>\n",
              "      <td>89.960110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.3</td>\n",
              "      <td>30</td>\n",
              "      <td>31.668869</td>\n",
              "      <td>38.688395</td>\n",
              "      <td>92.812037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.3</td>\n",
              "      <td>30</td>\n",
              "      <td>31.709690</td>\n",
              "      <td>38.689317</td>\n",
              "      <td>91.470480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.3</td>\n",
              "      <td>30</td>\n",
              "      <td>32.159439</td>\n",
              "      <td>39.110712</td>\n",
              "      <td>94.202051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.1</td>\n",
              "      <td>30</td>\n",
              "      <td>32.484008</td>\n",
              "      <td>39.427880</td>\n",
              "      <td>94.260430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.3</td>\n",
              "      <td>30</td>\n",
              "      <td>32.692376</td>\n",
              "      <td>39.714518</td>\n",
              "      <td>94.046512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.1</td>\n",
              "      <td>30</td>\n",
              "      <td>32.996730</td>\n",
              "      <td>40.008485</td>\n",
              "      <td>94.827266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.3</td>\n",
              "      <td>30</td>\n",
              "      <td>33.184928</td>\n",
              "      <td>40.073987</td>\n",
              "      <td>91.779999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.3</td>\n",
              "      <td>30</td>\n",
              "      <td>33.201778</td>\n",
              "      <td>40.133292</td>\n",
              "      <td>93.973839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.3</td>\n",
              "      <td>30</td>\n",
              "      <td>33.150219</td>\n",
              "      <td>40.167392</td>\n",
              "      <td>95.642329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.3</td>\n",
              "      <td>30</td>\n",
              "      <td>33.391824</td>\n",
              "      <td>40.281256</td>\n",
              "      <td>95.918650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.1</td>\n",
              "      <td>30</td>\n",
              "      <td>33.390831</td>\n",
              "      <td>40.310470</td>\n",
              "      <td>93.538706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.1</td>\n",
              "      <td>30</td>\n",
              "      <td>33.582112</td>\n",
              "      <td>40.581542</td>\n",
              "      <td>94.595322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.1</td>\n",
              "      <td>30</td>\n",
              "      <td>33.706064</td>\n",
              "      <td>40.611090</td>\n",
              "      <td>94.183894</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6943a452-e6f5-4dd6-92d7-d33f9f0cab90')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6943a452-e6f5-4dd6-92d7-d33f9f0cab90 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6943a452-e6f5-4dd6-92d7-d33f9f0cab90');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ea4b9faa-f6c6-4133-a78a-ce922dbb361b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ea4b9faa-f6c6-4133-a78a-ce922dbb361b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ea4b9faa-f6c6-4133-a78a-ce922dbb361b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_08ba0c5a-6f7a-4ba2-a461-8a04f710730b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_lstm_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_08ba0c5a-6f7a-4ba2-a461-8a04f710730b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_lstm_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_lstm_results",
              "summary": "{\n  \"name\": \"df_lstm_results\",\n  \"rows\": 16,\n  \"fields\": [\n    {\n      \"column\": \"hidden_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 33,\n        \"min\": 64,\n        \"max\": 128,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          64,\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00025819888974716116,\n        \"min\": 0.0005,\n        \"max\": 0.001,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.001,\n          0.0005\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10327955589886445,\n        \"min\": 0.1,\n        \"max\": 0.3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.3,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sequence_len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 30,\n        \"max\": 30,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          30\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7911322831353437,\n        \"min\": 31.5975648987176,\n        \"max\": 33.706064354940736,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          31.5975648987176\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7756620967793064,\n        \"min\": 38.55592572985896,\n        \"max\": 40.61108959502709,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          38.55592572985896\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.793540288060421,\n        \"min\": 89.96010997046454,\n        \"max\": 95.91864967779448,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          90.17031919621745\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Restricted Grid\n",
        "hidden_sizes = [64, 128]\n",
        "num_layers_list = [1, 2]\n",
        "lrs = [0.001, 0.0005]\n",
        "dropouts = [0.1, 0.3]\n",
        "sequence_lens = [30]\n",
        "\n",
        "# Fixed parameters\n",
        "batch_size = 64\n",
        "num_epochs = 50\n",
        "patience = 5\n",
        "initial_train_size = 1095\n",
        "horizon = 14\n",
        "step = 14\n",
        "\n",
        "# All combinations\n",
        "param_grid = list(itertools.product(hidden_sizes, num_layers_list, lrs, dropouts, sequence_lens))\n",
        "\n",
        "results = []\n",
        "\n",
        "# Grid search loop\n",
        "for i, (hidden_size, num_layers, lr, dropout, sequence_len) in enumerate(param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(param_grid)}  hidden: {hidden_size}, layers: {num_layers}, \"\n",
        "          f\"lr: {lr}, dropout: {dropout}, seq_len: {sequence_len}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y, initial_train_size=initial_train_size, horizon=horizon, step=step, sequence_len=sequence_len)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            sequence_len=sequence_len,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            lr=lr,\n",
        "            batch_size=batch_size,\n",
        "            num_epochs=num_epochs,\n",
        "            patience=patience\n",
        "        )\n",
        "        if mae is not None:\n",
        "            mae_scores.append(mae)\n",
        "            rmse_scores.append(rmse)\n",
        "            mape_scores.append(mape)\n",
        "\n",
        "    # Aggregate fold results\n",
        "    results.append({\n",
        "        \"hidden_size\": hidden_size,\n",
        "        \"num_layers\": num_layers,\n",
        "        \"lr\": lr,\n",
        "        \"dropout\": dropout,\n",
        "        \"sequence_len\": sequence_len,\n",
        "        \"MAE\": np.mean(mae_scores),\n",
        "        \"RMSE\": np.mean(rmse_scores),\n",
        "        \"MAPE\": np.mean(mape_scores)\n",
        "    })\n",
        "\n",
        "# Final DataFrame\n",
        "df_lstm_results = pd.DataFrame(results)\n",
        "df_lstm_results = df_lstm_results.sort_values(\"RMSE\").reset_index(drop=True)\n",
        "df_lstm_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgdknN1ALEP1"
      },
      "source": [
        "GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvWgcmyQCEWO"
      },
      "outputs": [],
      "source": [
        "class GRUForecastNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=64, num_layers=1, dropout=0.2, output_size=14):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers=num_layers,\n",
        "                          batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gru_out, _ = self.gru(x)  # [batch, seq, hidden]\n",
        "        out = self.fc(gru_out[:, -1, :])  # [batch, output_size]\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo2QXyLkCEZn"
      },
      "outputs": [],
      "source": [
        "def train_one_gru_fold(X_train, y_train, X_val, y_val,\n",
        "                       sequence_len=30, num_epochs=50, patience=5,\n",
        "                       batch_size=32, lr=0.001, hidden_size=64, num_layers=1, dropout=0.2):\n",
        "\n",
        "    # --- 1. Scaling ---\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_val_scaled = scaler_X.transform(X_val)\n",
        "\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train)  # shape: (n_samples, 14)\n",
        "    y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "    # --- 2. Create sequences ---\n",
        "    X_train_seq, y_train_seq = create_lstm_input(X_train_scaled, y_train_scaled, sequence_len)\n",
        "    X_val_seq, y_val_seq = create_lstm_input(X_val_scaled, y_val_scaled, sequence_len)\n",
        "\n",
        "    if X_val_seq.shape[0] == 0:\n",
        "        print(\"Skipping fold: Not enough validation sequences.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # --- 3. Data loaders ---\n",
        "    train_loader = DataLoader(\n",
        "        LSTMForecastDataset(X_train_seq, y_train_seq), batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        LSTMForecastDataset(X_val_seq, y_val_seq), batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "\n",
        "    # --- 4. Model setup ---\n",
        "    model = GRUForecastNet(\n",
        "        input_size=X_train_seq.shape[2],\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    # --- 5. Training loop ---\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_preds = torch.cat([model(xb.to(device)) for xb, _ in val_loader])\n",
        "            val_targets = torch.cat([yb.to(device) for _, yb in val_loader])\n",
        "            val_loss = criterion(val_preds, val_targets)\n",
        "\n",
        "        if val_loss.item() < best_val_loss - 1e-4:\n",
        "            best_val_loss = val_loss.item()\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                break\n",
        "\n",
        "    # --- 6. Load best model ---\n",
        "    if best_model_state:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    # --- 7. Final evaluation ---\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32).to(device)\n",
        "        preds_scaled = model(X_val_tensor).cpu().numpy()\n",
        "        targets_scaled = y_val_seq\n",
        "\n",
        "    preds = scaler_y.inverse_transform(preds_scaled)\n",
        "    targets = scaler_y.inverse_transform(targets_scaled)\n",
        "\n",
        "    # --- 8. Metrics ---\n",
        "    mae = mean_absolute_error(targets.flatten(), preds.flatten())\n",
        "    rmse = np.sqrt(mean_squared_error(targets.flatten(), preds.flatten()))\n",
        "    mape = mean_absolute_percentage_error(targets.flatten(), preds.flatten()) * 100\n",
        "\n",
        "    return mae, rmse, mape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG5rko9UOFZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d160855b-1a13-4773-ff29-8f6a770c7ddd",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1\n",
            "  MAE: 9.6373, RMSE: 11.2689, MAPE: 36.96\n",
            "\n",
            "Fold 2\n",
            "  MAE: 9.2273, RMSE: 11.0179, MAPE: 58.69\n",
            "\n",
            "Fold 3\n",
            "  MAE: 7.5274, RMSE: 9.1175, MAPE: 47.51\n",
            "\n",
            "Fold 4\n",
            "  MAE: 5.1510, RMSE: 6.2126, MAPE: 14.64\n",
            "\n",
            "Fold 5\n",
            "  MAE: 5.5311, RMSE: 7.0806, MAPE: 19.67\n",
            "\n",
            "Fold 6\n",
            "  MAE: 6.6350, RMSE: 8.2622, MAPE: 24.63\n",
            "\n",
            "Fold 7\n",
            "  MAE: 9.1826, RMSE: 10.2782, MAPE: 24.70\n",
            "\n",
            "Fold 8\n",
            "  MAE: 9.9009, RMSE: 10.6315, MAPE: 22.38\n",
            "\n",
            "Fold 9\n",
            "  MAE: 11.3089, RMSE: 12.2748, MAPE: 24.45\n",
            "\n",
            "Fold 10\n",
            "  MAE: 14.9825, RMSE: 16.4640, MAPE: 30.42\n",
            "\n",
            "Fold 11\n",
            "  MAE: 14.5377, RMSE: 15.1059, MAPE: 27.21\n",
            "\n",
            "Fold 12\n",
            "  MAE: 11.9211, RMSE: 13.0414, MAPE: 21.44\n",
            "\n",
            "Fold 13\n",
            "  MAE: 11.3326, RMSE: 12.6223, MAPE: 19.66\n",
            "\n",
            "Fold 14\n",
            "  MAE: 11.8640, RMSE: 13.4460, MAPE: 19.75\n",
            "\n",
            "Fold 15\n",
            "  MAE: 11.6782, RMSE: 13.7131, MAPE: 25.11\n",
            "\n",
            "Fold 16\n",
            "  MAE: 11.3090, RMSE: 13.7987, MAPE: 32.41\n",
            "\n",
            "Fold 17\n",
            "  MAE: 12.5166, RMSE: 15.0245, MAPE: 33.31\n",
            "\n",
            "Fold 18\n",
            "  MAE: 10.1741, RMSE: 12.6082, MAPE: 26.39\n",
            "\n",
            "Fold 19\n",
            "  MAE: 9.5353, RMSE: 12.2500, MAPE: 18.04\n",
            "\n",
            "Fold 20\n",
            "  MAE: 11.4197, RMSE: 15.1076, MAPE: 22.04\n",
            "\n",
            "Fold 21\n",
            "  MAE: 11.9614, RMSE: 15.5951, MAPE: 29.51\n",
            "\n",
            "Fold 22\n",
            "  MAE: 11.3604, RMSE: 14.5211, MAPE: 45.17\n",
            "\n",
            "Fold 23\n",
            "  MAE: 10.8469, RMSE: 14.5365, MAPE: 48.59\n",
            "\n",
            "Fold 24\n",
            "  MAE: 9.8728, RMSE: 13.3693, MAPE: 32.01\n",
            "\n",
            "Fold 25\n",
            "  MAE: 10.1821, RMSE: 13.2562, MAPE: 32.11\n",
            "\n",
            "Fold 26\n",
            "  MAE: 6.2551, RMSE: 9.8918, MAPE: 31.58\n",
            "\n",
            "Fold 27\n",
            "  MAE: 7.1024, RMSE: 11.3537, MAPE: 53.39\n",
            "\n",
            "Fold 28\n",
            "  MAE: 7.8419, RMSE: 12.3869, MAPE: 82.63\n",
            "\n",
            "Fold 29\n",
            "  MAE: 4.9541, RMSE: 8.0271, MAPE: 50.48\n",
            "\n",
            "Fold 30\n",
            "  MAE: 4.3970, RMSE: 7.4969, MAPE: 17.22\n",
            "\n",
            "Fold 31\n",
            "  MAE: 7.1946, RMSE: 10.8827, MAPE: 29.12\n",
            "\n",
            "Fold 32\n",
            "  MAE: 7.2479, RMSE: 10.4753, MAPE: 26.91\n",
            "\n",
            "Fold 33\n",
            "  MAE: 6.6406, RMSE: 9.4878, MAPE: 28.77\n",
            "\n",
            "Fold 34\n",
            "  MAE: 7.5653, RMSE: 10.4251, MAPE: 39.05\n",
            "\n",
            "Fold 35\n",
            "  MAE: 6.7675, RMSE: 9.1286, MAPE: 32.76\n",
            "\n",
            "Fold 36\n",
            "  MAE: 5.2648, RMSE: 6.2366, MAPE: 16.78\n",
            "\n",
            "Fold 37\n",
            "  MAE: 5.9355, RMSE: 6.8288, MAPE: 15.55\n",
            "\n",
            "Fold 38\n",
            "  MAE: 7.5978, RMSE: 10.2211, MAPE: 26.73\n",
            "\n",
            "Fold 39\n",
            "  MAE: 7.5567, RMSE: 10.4099, MAPE: 29.89\n",
            "\n",
            "Fold 40\n",
            "  MAE: 6.9931, RMSE: 8.7375, MAPE: 23.45\n",
            "\n",
            "Fold 41\n",
            "  MAE: 6.6508, RMSE: 8.1580, MAPE: 21.44\n",
            "\n",
            "Fold 42\n",
            "  MAE: 6.3684, RMSE: 7.9337, MAPE: 20.89\n",
            "\n",
            "Fold 43\n",
            "  MAE: 6.1088, RMSE: 7.4038, MAPE: 18.25\n",
            "\n",
            "Fold 44\n",
            "  MAE: 6.5770, RMSE: 7.7901, MAPE: 17.21\n",
            "\n",
            "Fold 45\n",
            "  MAE: 6.4971, RMSE: 7.6191, MAPE: 15.26\n",
            "\n",
            "Fold 46\n",
            "  MAE: 6.3518, RMSE: 8.1074, MAPE: 27.48\n",
            "\n",
            "Fold 47\n",
            "  MAE: 7.3431, RMSE: 9.9549, MAPE: 56.87\n",
            "\n",
            "Fold 48\n",
            "  MAE: 6.4908, RMSE: 8.7231, MAPE: 51.65\n",
            "\n",
            "Fold 49\n",
            "  MAE: 5.2564, RMSE: 6.3930, MAPE: 19.45\n",
            "\n",
            "Fold 50\n",
            "  MAE: 5.9518, RMSE: 7.1219, MAPE: 29.15\n",
            "\n",
            "Fold 51\n",
            "  MAE: 7.8400, RMSE: 9.0671, MAPE: 63.37\n",
            "\n",
            "Fold 52\n",
            "  MAE: 9.7197, RMSE: 11.1564, MAPE: 156.79\n",
            "\n",
            "Fold 53\n",
            "  MAE: 10.7229, RMSE: 11.9643, MAPE: 159.09\n",
            "\n",
            "Fold 54\n",
            "  MAE: 8.7443, RMSE: 10.0820, MAPE: 84.36\n",
            "\n",
            "Fold 55\n",
            "  MAE: 9.9598, RMSE: 11.8224, MAPE: 118.38\n",
            "\n",
            "Fold 56\n",
            "  MAE: 8.8955, RMSE: 11.2025, MAPE: 118.28\n",
            "\n",
            "Fold 57\n",
            "  MAE: 7.4461, RMSE: 9.6837, MAPE: 82.10\n",
            "\n",
            "Fold 58\n",
            "  MAE: 5.9482, RMSE: 7.3121, MAPE: 57.60\n",
            "\n",
            "Fold 59\n",
            "  MAE: 6.3144, RMSE: 7.9480, MAPE: 69.68\n",
            "\n",
            "Fold 60\n",
            "  MAE: 7.1907, RMSE: 8.8766, MAPE: 60.45\n",
            "\n",
            "Fold 61\n",
            "  MAE: 7.7146, RMSE: 9.8677, MAPE: 53.52\n",
            "\n",
            "Fold 62\n",
            "  MAE: 9.4081, RMSE: 12.7181, MAPE: 125.22\n",
            "\n",
            "Fold 63\n",
            "  MAE: 9.3075, RMSE: 13.1104, MAPE: 154.56\n",
            "\n",
            "Fold 64\n",
            "  MAE: 8.1489, RMSE: 10.4169, MAPE: 84.01\n",
            "\n",
            "Fold 65\n",
            "  MAE: 7.6593, RMSE: 10.1866, MAPE: 62.98\n",
            "\n",
            "Fold 66\n",
            "  MAE: 7.0722, RMSE: 9.0167, MAPE: 23.63\n",
            "\n",
            "Fold 67\n",
            "  MAE: 9.1330, RMSE: 12.1250, MAPE: 38.01\n",
            "\n",
            "Fold 68\n",
            "  MAE: 11.7998, RMSE: 15.6270, MAPE: 96.48\n",
            "\n",
            "Fold 69\n",
            "  MAE: 11.2967, RMSE: 14.1301, MAPE: 99.24\n",
            "\n",
            "Fold 70\n",
            "  MAE: 9.7983, RMSE: 12.6613, MAPE: 104.80\n",
            "\n",
            "Fold 71\n",
            "  MAE: 14.2513, RMSE: 16.3161, MAPE: 280.64\n",
            "\n",
            "Fold 72\n",
            "  MAE: 18.9178, RMSE: 21.0060, MAPE: 433.09\n",
            "\n",
            "Fold 73\n",
            "  MAE: 18.4815, RMSE: 22.0415, MAPE: 300.47\n",
            "\n",
            "Fold 74\n",
            "  MAE: 15.2936, RMSE: 19.5107, MAPE: 53.73\n",
            "\n",
            "Fold 75\n",
            "  MAE: 17.3084, RMSE: 21.0631, MAPE: 57.85\n",
            "\n",
            "Fold 76\n",
            "  MAE: 17.2619, RMSE: 20.9167, MAPE: 36.89\n",
            "\n",
            "Fold 77\n",
            "  MAE: 11.9061, RMSE: 14.8877, MAPE: 24.70\n",
            "\n",
            "Fold 78\n",
            "  MAE: 9.9184, RMSE: 12.6929, MAPE: 21.63\n",
            "\n",
            "Fold 79\n",
            "  MAE: 9.6590, RMSE: 11.9930, MAPE: 23.67\n",
            "\n",
            "Fold 80\n",
            "  MAE: 12.0162, RMSE: 13.8873, MAPE: 31.69\n",
            "\n",
            "Fold 81\n",
            "  MAE: 12.8949, RMSE: 15.8812, MAPE: 46.50\n",
            "\n",
            "Fold 82\n",
            "  MAE: 14.8235, RMSE: 18.6728, MAPE: 49.51\n",
            "\n",
            "Fold 83\n",
            "  MAE: 14.6841, RMSE: 17.9690, MAPE: 37.94\n",
            "\n",
            "Fold 84\n",
            "  MAE: 15.1331, RMSE: 17.6615, MAPE: 38.48\n",
            "\n",
            "Fold 85\n",
            "  MAE: 15.7134, RMSE: 18.2673, MAPE: 45.04\n",
            "\n",
            "Fold 86\n",
            "  MAE: 18.8055, RMSE: 20.7587, MAPE: 37.08\n",
            "\n",
            "Fold 87\n",
            "  MAE: 25.4017, RMSE: 26.8857, MAPE: 34.68\n",
            "\n",
            "Fold 88\n",
            "  MAE: 28.9057, RMSE: 30.1916, MAPE: 34.61\n",
            "\n",
            "Fold 89\n",
            "  MAE: 26.4259, RMSE: 28.2793, MAPE: 31.45\n",
            "\n",
            "Fold 90\n",
            "  MAE: 22.1073, RMSE: 24.7269, MAPE: 31.06\n",
            "\n",
            "Fold 91\n",
            "  MAE: 16.2839, RMSE: 19.5339, MAPE: 25.56\n",
            "\n",
            "Fold 92\n",
            "  MAE: 18.5524, RMSE: 23.3345, MAPE: 20.80\n",
            "\n",
            "Fold 93\n",
            "  MAE: 35.1951, RMSE: 40.0384, MAPE: 27.84\n",
            "\n",
            "Fold 94\n",
            "  MAE: 47.4907, RMSE: 57.3700, MAPE: 35.04\n",
            "\n",
            "Fold 95\n",
            "  MAE: 52.1049, RMSE: 66.1519, MAPE: 38.10\n",
            "\n",
            "Fold 96\n",
            "  MAE: 49.0252, RMSE: 61.3877, MAPE: 48.13\n",
            "\n",
            "Fold 97\n",
            "  MAE: 49.9176, RMSE: 58.0670, MAPE: 58.15\n",
            "\n",
            "Fold 98\n",
            "  MAE: 49.5099, RMSE: 58.6700, MAPE: 38.82\n",
            "\n",
            "Fold 99\n",
            "  MAE: 54.1188, RMSE: 69.2612, MAPE: 32.09\n",
            "\n",
            "Fold 100\n",
            "  MAE: 82.9467, RMSE: 111.2769, MAPE: 38.69\n",
            "\n",
            "Fold 101\n",
            "  MAE: 75.2690, RMSE: 104.4047, MAPE: 51.81\n",
            "\n",
            "Fold 102\n",
            "  MAE: 43.1813, RMSE: 54.9050, MAPE: 64.53\n",
            "\n",
            "Fold 103\n",
            "  MAE: 39.8847, RMSE: 50.0783, MAPE: 62.10\n",
            "\n",
            "Fold 104\n",
            "  MAE: 42.5888, RMSE: 50.8841, MAPE: 57.02\n",
            "\n",
            "Fold 105\n",
            "  MAE: 91.1397, RMSE: 125.7945, MAPE: 53.01\n",
            "\n",
            "Fold 106\n",
            "  MAE: 112.4838, RMSE: 145.2528, MAPE: 48.12\n",
            "\n",
            "Fold 107\n",
            "  MAE: 75.9022, RMSE: 98.1494, MAPE: 43.37\n",
            "\n",
            "Fold 108\n",
            "  MAE: 53.8415, RMSE: 61.1267, MAPE: 40.20\n",
            "\n",
            "Fold 109\n",
            "  MAE: 57.9792, RMSE: 63.6699, MAPE: 36.49\n",
            "\n",
            "Fold 110\n",
            "  MAE: 54.6012, RMSE: 63.9913, MAPE: 26.93\n",
            "\n",
            "Fold 111\n",
            "  MAE: 39.9928, RMSE: 50.2311, MAPE: 39.98\n",
            "\n",
            "Fold 112\n",
            "  MAE: 51.1537, RMSE: 59.7314, MAPE: 42.95\n",
            "\n",
            "Fold 113\n",
            "  MAE: 67.1004, RMSE: 82.2778, MAPE: 32.77\n",
            "\n",
            "Fold 114\n",
            "  MAE: 84.9807, RMSE: 100.2490, MAPE: 32.50\n",
            "\n",
            "Fold 115\n",
            "  MAE: 107.4524, RMSE: 128.0897, MAPE: 36.88\n",
            "\n",
            "Fold 116\n",
            "  MAE: 144.4791, RMSE: 162.0894, MAPE: 43.01\n",
            "\n",
            "Fold 117\n",
            "  MAE: 202.2785, RMSE: 233.4215, MAPE: 45.98\n",
            "\n",
            "Fold 118\n",
            "  MAE: 248.7542, RMSE: 276.5332, MAPE: 48.77\n",
            "\n",
            "Fold 119\n",
            "  MAE: 193.2727, RMSE: 226.4856, MAPE: 49.46\n",
            "\n",
            "Fold 120\n",
            "  MAE: 99.9578, RMSE: 119.3806, MAPE: 90.62\n",
            "\n",
            "Fold 121\n",
            "  MAE: 94.3567, RMSE: 117.1242, MAPE: 126.88\n",
            "\n",
            "Fold 122\n",
            "  MAE: 77.2072, RMSE: 102.5632, MAPE: 112.59\n",
            "\n",
            "Fold 123\n",
            "  MAE: 48.5501, RMSE: 57.4805, MAPE: 213.36\n",
            "\n",
            "Fold 124\n",
            "  MAE: 85.6895, RMSE: 108.8364, MAPE: 216.19\n",
            "\n",
            "Fold 125\n",
            "  MAE: 128.1378, RMSE: 146.1649, MAPE: 82.96\n",
            "\n",
            "Fold 126\n",
            "  MAE: 146.3727, RMSE: 161.3832, MAPE: 156.87\n",
            "\n",
            "Fold 127\n",
            "  MAE: 117.1431, RMSE: 139.5047, MAPE: 265.13\n",
            "\n",
            "Fold 128\n",
            "  MAE: 49.8837, RMSE: 63.2601, MAPE: 130.36\n",
            "\n",
            "Fold 129\n",
            "  MAE: 51.0656, RMSE: 63.6123, MAPE: 75.55\n",
            "\n",
            "Fold 130\n",
            "  MAE: 31.5798, RMSE: 39.1174, MAPE: 29.76\n",
            "\n",
            "Fold 131\n",
            "  MAE: 24.2222, RMSE: 30.8861, MAPE: 26.16\n",
            "\n",
            "Fold 132\n",
            "  MAE: 28.8667, RMSE: 34.0052, MAPE: 31.57\n",
            "\n",
            "Fold 133\n",
            "  MAE: 38.2258, RMSE: 46.7679, MAPE: 65.29\n",
            "\n",
            "Fold 134\n",
            "  MAE: 36.9560, RMSE: 45.3274, MAPE: 59.64\n",
            "\n",
            "Fold 135\n",
            "  MAE: 20.6986, RMSE: 26.0856, MAPE: 26.65\n",
            "\n",
            "Fold 136\n",
            "  MAE: 32.8141, RMSE: 39.2249, MAPE: 44.32\n",
            "\n",
            "Fold 137\n",
            "  MAE: 22.5180, RMSE: 28.3243, MAPE: 45.09\n",
            "\n",
            "Fold 138\n",
            "  MAE: 31.6485, RMSE: 38.9382, MAPE: 63.17\n",
            "\n",
            "Fold 139\n",
            "  MAE: 27.0486, RMSE: 34.2089, MAPE: 34.80\n",
            "\n",
            "Fold 140\n",
            "  MAE: 35.5321, RMSE: 44.8635, MAPE: 65.02\n",
            "\n",
            "Fold 141\n",
            "  MAE: 27.6783, RMSE: 38.4961, MAPE: 111.86\n",
            "\n",
            "Fold 142\n",
            "  MAE: 30.9333, RMSE: 40.3251, MAPE: 207.68\n",
            "\n",
            "Fold 143\n",
            "  MAE: 30.6267, RMSE: 37.4363, MAPE: 142.50\n",
            "\n",
            "Fold 144\n",
            "  MAE: 27.3079, RMSE: 33.1334, MAPE: 66.66\n",
            "\n",
            "Fold 145\n",
            "  MAE: 26.6187, RMSE: 34.3353, MAPE: 360.23\n",
            "\n",
            "Fold 146\n",
            "  MAE: 34.3287, RMSE: 41.8717, MAPE: 711.23\n",
            "\n",
            "Fold 147\n",
            "  MAE: 40.3106, RMSE: 49.3492, MAPE: 1880.43\n",
            "\n",
            "Fold 148\n",
            "  MAE: 35.5427, RMSE: 42.5973, MAPE: 1633.71\n",
            "\n",
            "Fold 149\n",
            "  MAE: 27.6914, RMSE: 33.7177, MAPE: 381.18\n",
            "\n",
            "Fold 150\n",
            "  MAE: 27.4470, RMSE: 34.7009, MAPE: 49.03\n",
            "\n",
            "Fold 151\n",
            "  MAE: 26.3125, RMSE: 31.5748, MAPE: 28.39\n",
            "\n",
            "Fold 152\n",
            "  MAE: 85.7281, RMSE: 96.5057, MAPE: 237.46\n",
            "\n",
            "Fold 153\n",
            "  MAE: 40.7637, RMSE: 45.5948, MAPE: 124.96\n",
            "\n",
            "Fold 154\n",
            "  MAE: 50.5841, RMSE: 63.3679, MAPE: 135.32\n",
            "\n",
            "Fold 155\n",
            "  MAE: 34.4516, RMSE: 40.4153, MAPE: 72.51\n",
            "\n",
            "Fold 156\n",
            "  MAE: 26.4717, RMSE: 31.3498, MAPE: 60.78\n",
            "\n",
            "Fold 157\n",
            "  MAE: 17.9096, RMSE: 22.8307, MAPE: 36.12\n",
            "\n",
            "Fold 158\n",
            "  MAE: 13.3301, RMSE: 17.2656, MAPE: 24.97\n",
            "\n",
            "Fold 159\n",
            "  MAE: 15.1610, RMSE: 18.4533, MAPE: 32.25\n",
            "\n",
            "Fold 160\n",
            "  MAE: 17.7061, RMSE: 21.2966, MAPE: 50.31\n",
            "\n",
            "Fold 161\n",
            "  MAE: 21.3135, RMSE: 25.7147, MAPE: 54.77\n",
            "\n",
            "Fold 162\n",
            "  MAE: 22.5187, RMSE: 27.2578, MAPE: 61.87\n",
            "\n",
            "Fold 163\n",
            "  MAE: 26.0603, RMSE: 31.1568, MAPE: 69.54\n",
            "\n",
            "Fold 164\n",
            "  MAE: 25.4674, RMSE: 31.2255, MAPE: 73.41\n",
            "\n",
            "Fold 165\n",
            "  MAE: 20.1972, RMSE: 25.3074, MAPE: 66.61\n",
            "\n",
            "Fold 166\n",
            "  MAE: 20.7050, RMSE: 26.1086, MAPE: 76.82\n",
            "\n",
            "Fold 167\n",
            "  MAE: 19.6160, RMSE: 26.0993, MAPE: 68.38\n",
            "\n",
            "Fold 168\n",
            "  MAE: 17.5941, RMSE: 21.0791, MAPE: 40.01\n",
            "\n",
            "Fold 169\n",
            "  MAE: 24.1434, RMSE: 28.0832, MAPE: 60.08\n",
            "\n",
            "Fold 170\n",
            "  MAE: 29.6568, RMSE: 34.4465, MAPE: 74.52\n",
            "\n",
            "Fold 171\n",
            "  MAE: 27.0033, RMSE: 33.4469, MAPE: 57.66\n",
            "\n",
            "Fold 172\n",
            "  MAE: 23.7406, RMSE: 29.4106, MAPE: 50.49\n",
            "\n",
            "Fold 173\n",
            "  MAE: 24.0783, RMSE: 28.9656, MAPE: 49.92\n",
            "\n",
            "Fold 174\n",
            "  MAE: 31.6762, RMSE: 36.4213, MAPE: 56.98\n",
            "\n",
            "Fold 175\n",
            "  MAE: 37.2991, RMSE: 42.6738, MAPE: 64.73\n",
            "\n",
            "Fold 176\n",
            "  MAE: 37.4430, RMSE: 44.3650, MAPE: 100.72\n",
            "\n",
            "Fold 177\n",
            "  MAE: 40.7064, RMSE: 64.0354, MAPE: 106.51\n",
            "\n",
            "Fold 178\n",
            "  MAE: 63.1659, RMSE: 87.6062, MAPE: 146.77\n",
            "\n",
            "Fold 179\n",
            "  MAE: 58.9207, RMSE: 79.4625, MAPE: 170.18\n",
            "\n",
            "Fold 180\n",
            "  MAE: 45.3369, RMSE: 56.4620, MAPE: 143.73\n",
            "\n",
            "Fold 181\n",
            "  MAE: 40.2875, RMSE: 51.7464, MAPE: 47.07\n",
            "\n",
            "Fold 182\n",
            "  MAE: 34.7218, RMSE: 43.9427, MAPE: 34.43\n",
            "\n",
            "Fold 183\n",
            "  MAE: 35.2895, RMSE: 41.9880, MAPE: 37.80\n",
            "\n",
            "Fold 184\n",
            "  MAE: 34.3173, RMSE: 40.2536, MAPE: 45.93\n",
            "\n",
            "Fold 185\n",
            "  MAE: 31.9892, RMSE: 37.6574, MAPE: 53.11\n",
            "\n",
            "Fold 186\n",
            "  MAE: 25.5597, RMSE: 30.1955, MAPE: 47.60\n",
            "\n",
            "Fold 187\n",
            "  MAE: 22.1185, RMSE: 26.5371, MAPE: 33.05\n",
            "\n",
            "Fold 188\n",
            "  MAE: 18.7527, RMSE: 22.9515, MAPE: 25.04\n",
            "\n",
            "Fold 189\n",
            "  MAE: 17.3702, RMSE: 22.3477, MAPE: 38.93\n",
            "\n",
            "Fold 190\n",
            "  MAE: 18.4472, RMSE: 24.1992, MAPE: 48.71\n",
            "\n",
            "Fold 191\n",
            "  MAE: 17.0401, RMSE: 21.9790, MAPE: 42.42\n",
            "\n",
            "Fold 192\n",
            "  MAE: 20.7554, RMSE: 26.7722, MAPE: 45.75\n",
            "\n",
            "Fold 193\n",
            "  MAE: 18.9636, RMSE: 24.9191, MAPE: 35.09\n",
            "\n",
            " Final CV Results:\n",
            "Average MAE:  31.0655\n",
            "Average RMSE: 37.8411\n",
            "Average MAPE: 86.2136\n"
          ]
        }
      ],
      "source": [
        "sequence_len=30\n",
        "\n",
        "mae_scores = []\n",
        "rmse_scores = []\n",
        "mape_scores = []\n",
        "\n",
        "for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "    expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14, sequence_len=sequence_len)\n",
        "):\n",
        "    print(f\"\\nFold {fold+1}\")\n",
        "    mae, rmse, mape = train_one_gru_fold(\n",
        "        X_tr, y_tr, X_val, y_val, sequence_len=sequence_len\n",
        "    )\n",
        "    if mae is None:\n",
        "        continue  # Skip if fold was invalid\n",
        "    print(f\"  MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.2f}\")\n",
        "    mae_scores.append(mae)\n",
        "    rmse_scores.append(rmse)\n",
        "    mape_scores.append(mape)\n",
        "\n",
        "# --- Final CV Results ---\n",
        "print(\"\\n Final CV Results:\")\n",
        "print(f\"Average MAE:  {np.mean(mae_scores):.4f}\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):.4f}\")\n",
        "print(f\"Average MAPE: {np.mean(mape_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ve0l4fyPGE6"
      },
      "source": [
        "Hyperparamter tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dz73YRHZOFcK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f398e11-a751-4382-b05d-6a130054c1d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total GRU configurations: 144\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "\n",
        "# Hyperparameter grid for GRU\n",
        "hidden_sizes = [32, 64, 128]\n",
        "num_layers_list = [1, 2, 3]\n",
        "dropouts = [0.0, 0.1, 0.2, 0.3]\n",
        "learning_rates = [0.001, 0.0005]\n",
        "batch_sizes = [32, 64]\n",
        "\n",
        "# Cartesian product of all combinations\n",
        "gru_param_grid = list(itertools.product(hidden_sizes, num_layers_list, dropouts, learning_rates, batch_sizes))\n",
        "print(f\"Total GRU configurations: {len(gru_param_grid)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSxGjjW2OFd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fed4798d-3b94-4743-a414-1ece69410b3e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Config 1/144  hidden=32, layers=1, dropout=0.0, lr=0.001, batch_size=32\n",
            "Avg MAE: 31.0825 | RMSE: 37.8943 | MAPE: 87.36\n",
            "\n",
            " Config 2/144  hidden=32, layers=1, dropout=0.0, lr=0.001, batch_size=64\n",
            "Avg MAE: 31.4265 | RMSE: 38.2849 | MAPE: 91.81\n",
            "\n",
            " Config 3/144  hidden=32, layers=1, dropout=0.0, lr=0.0005, batch_size=32\n",
            "Avg MAE: 31.8264 | RMSE: 38.7084 | MAPE: 91.77\n",
            "\n",
            " Config 4/144  hidden=32, layers=1, dropout=0.0, lr=0.0005, batch_size=64\n",
            "Avg MAE: 32.0994 | RMSE: 39.0682 | MAPE: 94.95\n",
            "\n",
            " Config 5/144  hidden=32, layers=1, dropout=0.1, lr=0.001, batch_size=32\n",
            "Avg MAE: 31.2114 | RMSE: 37.9457 | MAPE: 85.04\n",
            "\n",
            " Config 6/144  hidden=32, layers=1, dropout=0.1, lr=0.001, batch_size=64\n",
            "Avg MAE: 31.4460 | RMSE: 38.2650 | MAPE: 90.28\n",
            "\n",
            " Config 7/144  hidden=32, layers=1, dropout=0.1, lr=0.0005, batch_size=32\n",
            "Avg MAE: 31.8464 | RMSE: 38.7196 | MAPE: 92.42\n",
            "\n",
            " Config 8/144  hidden=32, layers=1, dropout=0.1, lr=0.0005, batch_size=64\n",
            "Avg MAE: 32.0268 | RMSE: 38.9262 | MAPE: 94.39\n",
            "\n",
            " Config 9/144  hidden=32, layers=1, dropout=0.2, lr=0.001, batch_size=32\n",
            "Avg MAE: 31.3083 | RMSE: 38.1549 | MAPE: 89.87\n",
            "\n",
            " Config 10/144  hidden=32, layers=1, dropout=0.2, lr=0.001, batch_size=64\n",
            "Avg MAE: 31.3484 | RMSE: 38.2045 | MAPE: 91.06\n",
            "\n",
            " Config 11/144  hidden=32, layers=1, dropout=0.2, lr=0.0005, batch_size=32\n",
            "Avg MAE: 31.7192 | RMSE: 38.6318 | MAPE: 92.29\n",
            "\n",
            " Config 12/144  hidden=32, layers=1, dropout=0.2, lr=0.0005, batch_size=64\n",
            "Avg MAE: 32.1004 | RMSE: 38.9868 | MAPE: 93.09\n",
            "\n",
            " Config 13/144  hidden=32, layers=1, dropout=0.3, lr=0.001, batch_size=32\n",
            "Avg MAE: 31.5491 | RMSE: 38.4115 | MAPE: 89.81\n",
            "\n",
            " Config 14/144  hidden=32, layers=1, dropout=0.3, lr=0.001, batch_size=64\n",
            "Avg MAE: 31.2876 | RMSE: 38.1570 | MAPE: 90.41\n",
            "\n",
            " Config 15/144  hidden=32, layers=1, dropout=0.3, lr=0.0005, batch_size=32\n",
            "Avg MAE: 31.8862 | RMSE: 38.7852 | MAPE: 92.74\n",
            "\n",
            " Config 16/144  hidden=32, layers=1, dropout=0.3, lr=0.0005, batch_size=64\n",
            "Avg MAE: 32.4664 | RMSE: 39.3635 | MAPE: 94.83\n",
            "\n",
            " Config 17/144  hidden=32, layers=2, dropout=0.0, lr=0.001, batch_size=32\n",
            "Avg MAE: 31.6530 | RMSE: 38.5015 | MAPE: 88.07\n",
            "\n",
            " Config 18/144  hidden=32, layers=2, dropout=0.0, lr=0.001, batch_size=64\n",
            "Avg MAE: 32.2874 | RMSE: 39.1570 | MAPE: 91.68\n",
            "\n",
            " Config 19/144  hidden=32, layers=2, dropout=0.0, lr=0.0005, batch_size=32\n",
            "Avg MAE: 32.3064 | RMSE: 39.1036 | MAPE: 93.82\n",
            "\n",
            " Config 20/144  hidden=32, layers=2, dropout=0.0, lr=0.0005, batch_size=64\n",
            "Avg MAE: 32.2940 | RMSE: 39.2467 | MAPE: 97.03\n",
            "\n",
            " Config 21/144  hidden=32, layers=2, dropout=0.1, lr=0.001, batch_size=32\n",
            "Avg MAE: 31.6318 | RMSE: 38.3813 | MAPE: 88.62\n",
            "\n",
            " Config 22/144  hidden=32, layers=2, dropout=0.1, lr=0.001, batch_size=64\n",
            "Avg MAE: 31.9961 | RMSE: 38.8777 | MAPE: 92.66\n",
            "\n",
            " Config 23/144  hidden=32, layers=2, dropout=0.1, lr=0.0005, batch_size=32\n",
            "Avg MAE: 32.2290 | RMSE: 39.1035 | MAPE: 92.70\n",
            "\n",
            " Config 24/144  hidden=32, layers=2, dropout=0.1, lr=0.0005, batch_size=64\n",
            "Avg MAE: 32.7603 | RMSE: 39.7078 | MAPE: 97.42\n",
            "\n",
            " Config 25/144  hidden=32, layers=2, dropout=0.2, lr=0.001, batch_size=32\n",
            "Avg MAE: 32.1662 | RMSE: 38.8931 | MAPE: 90.69\n",
            "\n",
            " Config 26/144  hidden=32, layers=2, dropout=0.2, lr=0.001, batch_size=64\n",
            "Avg MAE: 32.0391 | RMSE: 38.8639 | MAPE: 91.28\n",
            "\n",
            " Config 27/144  hidden=32, layers=2, dropout=0.2, lr=0.0005, batch_size=32\n",
            "Avg MAE: 32.3877 | RMSE: 39.2005 | MAPE: 94.53\n",
            "\n",
            " Config 28/144  hidden=32, layers=2, dropout=0.2, lr=0.0005, batch_size=64\n",
            "Avg MAE: 32.6296 | RMSE: 39.6185 | MAPE: 96.73\n",
            "\n",
            " Config 29/144  hidden=32, layers=2, dropout=0.3, lr=0.001, batch_size=32\n",
            "Avg MAE: 31.7982 | RMSE: 38.7866 | MAPE: 91.10\n",
            "\n",
            " Config 30/144  hidden=32, layers=2, dropout=0.3, lr=0.001, batch_size=64\n",
            "Avg MAE: 32.3005 | RMSE: 39.1114 | MAPE: 91.96\n",
            "\n",
            " Config 31/144  hidden=32, layers=2, dropout=0.3, lr=0.0005, batch_size=32\n",
            "Avg MAE: 32.2907 | RMSE: 39.0553 | MAPE: 93.65\n",
            "\n",
            " Config 32/144  hidden=32, layers=2, dropout=0.3, lr=0.0005, batch_size=64\n",
            "Avg MAE: 32.7772 | RMSE: 39.7272 | MAPE: 98.39\n",
            "\n",
            " Config 33/144  hidden=32, layers=3, dropout=0.0, lr=0.001, batch_size=32\n",
            "Avg MAE: 32.9376 | RMSE: 39.9100 | MAPE: 91.61\n",
            "\n",
            " Config 34/144  hidden=32, layers=3, dropout=0.0, lr=0.001, batch_size=64\n",
            "Avg MAE: 32.8341 | RMSE: 39.7837 | MAPE: 93.79\n",
            "\n",
            " Config 35/144  hidden=32, layers=3, dropout=0.0, lr=0.0005, batch_size=32\n",
            "Avg MAE: 33.0040 | RMSE: 39.9061 | MAPE: 94.74\n",
            "\n",
            " Config 36/144  hidden=32, layers=3, dropout=0.0, lr=0.0005, batch_size=64\n",
            "Avg MAE: 33.6181 | RMSE: 40.4736 | MAPE: 98.32\n",
            "\n",
            " Config 37/144  hidden=32, layers=3, dropout=0.1, lr=0.001, batch_size=32\n",
            "Avg MAE: 33.5052 | RMSE: 40.4170 | MAPE: 94.35\n",
            "\n",
            " Config 38/144  hidden=32, layers=3, dropout=0.1, lr=0.001, batch_size=64\n",
            "Avg MAE: 32.6945 | RMSE: 39.5337 | MAPE: 93.54\n",
            "\n",
            " Config 39/144  hidden=32, layers=3, dropout=0.1, lr=0.0005, batch_size=32\n",
            "Avg MAE: 33.1849 | RMSE: 40.0274 | MAPE: 95.51\n",
            "\n",
            " Config 40/144  hidden=32, layers=3, dropout=0.1, lr=0.0005, batch_size=64\n",
            "Avg MAE: 33.2365 | RMSE: 40.1835 | MAPE: 98.39\n",
            "\n",
            " Config 41/144  hidden=32, layers=3, dropout=0.2, lr=0.001, batch_size=32\n",
            "Avg MAE: 33.2840 | RMSE: 40.1688 | MAPE: 90.17\n",
            "\n",
            " Config 42/144  hidden=32, layers=3, dropout=0.2, lr=0.001, batch_size=64\n"
          ]
        }
      ],
      "source": [
        "gru_results = []\n",
        "\n",
        "for i, (hidden_size, num_layers, dropout, lr, batch_size) in enumerate(gru_param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(gru_param_grid)}  hidden={hidden_size}, layers={num_layers}, dropout={dropout}, lr={lr}, batch_size={batch_size}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14, sequence_len=30)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_gru_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            lr=lr,\n",
        "            batch_size=batch_size,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            sequence_len=30\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    # Save results\n",
        "    avg_mae = np.mean(mae_scores)\n",
        "    avg_rmse = np.mean(rmse_scores)\n",
        "    avg_mape = np.mean(mape_scores)\n",
        "\n",
        "    gru_results.append({\n",
        "        \"hidden_size\": hidden_size,\n",
        "        \"num_layers\": num_layers,\n",
        "        \"dropout\": dropout,\n",
        "        \"lr\": lr,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"MAE\": avg_mae,\n",
        "        \"RMSE\": avg_rmse,\n",
        "        \"MAPE\": avg_mape\n",
        "    })\n",
        "\n",
        "    print(f\"Avg MAE: {avg_mae:.4f} | RMSE: {avg_rmse:.4f} | MAPE: {avg_mape:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D52kWBJrOFgN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "3aa2b856-b891-4bdb-8bd8-d1a3b9b436ec",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gru_results' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2784529486.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_gru_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgru_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_gru_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_gru_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RMSE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_gru_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gru_results' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_gru_results = pd.DataFrame(gru_results)\n",
        "df_gru_results = df_gru_results.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
        "df_gru_results.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More restrcited hyperparamter search (hidden size 32)"
      ],
      "metadata": {
        "id": "xzmjfKy4JK94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Hyperparameter grid for GRU\n",
        "hidden_sizes = [32]\n",
        "num_layers_list = [1, 2, 3]\n",
        "dropouts = [0.0, 0.1, 0.2, 0.3]\n",
        "learning_rates = [0.001, 0.0005]\n",
        "batch_sizes = [32]\n",
        "\n",
        "# Cartesian product of all combinations\n",
        "gru_param_grid = list(itertools.product(hidden_sizes, num_layers_list, dropouts, learning_rates, batch_sizes))\n",
        "print(f\"Total GRU configurations: {len(gru_param_grid)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_tzcMSCJJK3",
        "outputId": "a1a6d3ea-801e-478a-d17b-e702cdf1d1e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total GRU configurations: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gru_results = []\n",
        "\n",
        "for i, (hidden_size, num_layers, dropout, lr, batch_size) in enumerate(gru_param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(gru_param_grid)}  hidden={hidden_size}, layers={num_layers}, dropout={dropout}, lr={lr}, batch_size={batch_size}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14, sequence_len=30)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_gru_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            lr=lr,\n",
        "            batch_size=batch_size,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            sequence_len=30\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    # Save results\n",
        "    avg_mae = np.mean(mae_scores)\n",
        "    avg_rmse = np.mean(rmse_scores)\n",
        "    avg_mape = np.mean(mape_scores)\n",
        "\n",
        "    gru_results.append({\n",
        "        \"hidden_size\": hidden_size,\n",
        "        \"num_layers\": num_layers,\n",
        "        \"dropout\": dropout,\n",
        "        \"lr\": lr,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"MAE\": avg_mae,\n",
        "        \"RMSE\": avg_rmse,\n",
        "        \"MAPE\": avg_mape\n",
        "    })\n",
        "\n",
        "    print(f\"Avg MAE: {avg_mae:.4f} | RMSE: {avg_rmse:.4f} | MAPE: {avg_mape:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JObgGT46JJYa",
        "outputId": "bee45036-5ded-453d-aec3-5714fc6c3dc6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Config 1/24  hidden=32, layers=1, dropout=0.0, lr=0.001, batch_size=32\n",
            "Avg MAE: 31.0175 | RMSE: 37.8457 | MAPE: 88.0613\n",
            "\n",
            " Config 2/24  hidden=32, layers=1, dropout=0.0, lr=0.0005, batch_size=32\n",
            "Avg MAE: 32.0763 | RMSE: 38.9613 | MAPE: 92.3032\n",
            "\n",
            " Config 3/24  hidden=32, layers=1, dropout=0.1, lr=0.001, batch_size=32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg MAE: 31.6715 | RMSE: 38.5261 | MAPE: 88.2952\n",
            "\n",
            " Config 4/24  hidden=32, layers=1, dropout=0.1, lr=0.0005, batch_size=32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg MAE: 31.8365 | RMSE: 38.6667 | MAPE: 92.1337\n",
            "\n",
            " Config 5/24  hidden=32, layers=1, dropout=0.2, lr=0.001, batch_size=32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg MAE: 31.3444 | RMSE: 38.1610 | MAPE: 89.3920\n",
            "\n",
            " Config 6/24  hidden=32, layers=1, dropout=0.2, lr=0.0005, batch_size=32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg MAE: 31.7324 | RMSE: 38.6063 | MAPE: 91.9527\n",
            "\n",
            " Config 7/24  hidden=32, layers=1, dropout=0.3, lr=0.001, batch_size=32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg MAE: 31.1420 | RMSE: 37.9579 | MAPE: 87.8831\n",
            "\n",
            " Config 8/24  hidden=32, layers=1, dropout=0.3, lr=0.0005, batch_size=32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg MAE: 31.8329 | RMSE: 38.6968 | MAPE: 92.1387\n",
            "\n",
            " Config 9/24  hidden=32, layers=2, dropout=0.0, lr=0.001, batch_size=32\n",
            "Avg MAE: 32.0532 | RMSE: 38.9817 | MAPE: 88.8110\n",
            "\n",
            " Config 10/24  hidden=32, layers=2, dropout=0.0, lr=0.0005, batch_size=32\n",
            "Avg MAE: 32.1210 | RMSE: 38.9676 | MAPE: 93.1882\n",
            "\n",
            " Config 11/24  hidden=32, layers=2, dropout=0.1, lr=0.001, batch_size=32\n",
            "Avg MAE: 31.6814 | RMSE: 38.5619 | MAPE: 90.9705\n",
            "\n",
            " Config 12/24  hidden=32, layers=2, dropout=0.1, lr=0.0005, batch_size=32\n",
            "Avg MAE: 32.2432 | RMSE: 39.0620 | MAPE: 92.7419\n",
            "\n",
            " Config 13/24  hidden=32, layers=2, dropout=0.2, lr=0.001, batch_size=32\n",
            "Avg MAE: 31.8684 | RMSE: 38.6939 | MAPE: 91.4164\n",
            "\n",
            " Config 14/24  hidden=32, layers=2, dropout=0.2, lr=0.0005, batch_size=32\n",
            "Avg MAE: 32.5504 | RMSE: 39.4341 | MAPE: 93.3800\n",
            "\n",
            " Config 15/24  hidden=32, layers=2, dropout=0.3, lr=0.001, batch_size=32\n",
            "Avg MAE: 31.9075 | RMSE: 38.7952 | MAPE: 88.9838\n",
            "\n",
            " Config 16/24  hidden=32, layers=2, dropout=0.3, lr=0.0005, batch_size=32\n",
            "Avg MAE: 32.3479 | RMSE: 39.1187 | MAPE: 92.0154\n",
            "\n",
            " Config 17/24  hidden=32, layers=3, dropout=0.0, lr=0.001, batch_size=32\n",
            "Avg MAE: 33.1138 | RMSE: 40.1091 | MAPE: 92.4684\n",
            "\n",
            " Config 18/24  hidden=32, layers=3, dropout=0.0, lr=0.0005, batch_size=32\n",
            "Avg MAE: 33.2363 | RMSE: 40.1400 | MAPE: 96.3697\n",
            "\n",
            " Config 19/24  hidden=32, layers=3, dropout=0.1, lr=0.001, batch_size=32\n",
            "Avg MAE: 32.7119 | RMSE: 39.7024 | MAPE: 90.4822\n",
            "\n",
            " Config 20/24  hidden=32, layers=3, dropout=0.1, lr=0.0005, batch_size=32\n",
            "Avg MAE: 33.1661 | RMSE: 40.0327 | MAPE: 94.0145\n",
            "\n",
            " Config 21/24  hidden=32, layers=3, dropout=0.2, lr=0.001, batch_size=32\n",
            "Avg MAE: 33.0585 | RMSE: 40.0570 | MAPE: 92.0346\n",
            "\n",
            " Config 22/24  hidden=32, layers=3, dropout=0.2, lr=0.0005, batch_size=32\n",
            "Avg MAE: 33.3861 | RMSE: 40.2620 | MAPE: 95.4030\n",
            "\n",
            " Config 23/24  hidden=32, layers=3, dropout=0.3, lr=0.001, batch_size=32\n",
            "Avg MAE: 33.2492 | RMSE: 40.1722 | MAPE: 92.2205\n",
            "\n",
            " Config 24/24  hidden=32, layers=3, dropout=0.3, lr=0.0005, batch_size=32\n",
            "Avg MAE: 33.6747 | RMSE: 40.5541 | MAPE: 96.1179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_gru_results = pd.DataFrame(gru_results)\n",
        "df_gru_results = df_gru_results.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
        "df_gru_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "id": "aymVDgVHJJdK",
        "outputId": "cbc56eae-e2fe-414f-df4f-923396c554df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    hidden_size  num_layers  dropout      lr  batch_size        MAE  \\\n",
              "0            32           1      0.0  0.0010          32  31.017536   \n",
              "1            32           1      0.3  0.0010          32  31.141957   \n",
              "2            32           1      0.2  0.0010          32  31.344381   \n",
              "3            32           1      0.1  0.0010          32  31.671511   \n",
              "4            32           2      0.1  0.0010          32  31.681379   \n",
              "5            32           1      0.2  0.0005          32  31.732448   \n",
              "6            32           1      0.1  0.0005          32  31.836496   \n",
              "7            32           2      0.2  0.0010          32  31.868374   \n",
              "8            32           1      0.3  0.0005          32  31.832950   \n",
              "9            32           2      0.3  0.0010          32  31.907455   \n",
              "10           32           1      0.0  0.0005          32  32.076257   \n",
              "11           32           2      0.0  0.0005          32  32.120954   \n",
              "12           32           2      0.0  0.0010          32  32.053184   \n",
              "13           32           2      0.1  0.0005          32  32.243157   \n",
              "14           32           2      0.3  0.0005          32  32.347894   \n",
              "15           32           2      0.2  0.0005          32  32.550378   \n",
              "16           32           3      0.1  0.0010          32  32.711940   \n",
              "17           32           3      0.1  0.0005          32  33.166073   \n",
              "18           32           3      0.2  0.0010          32  33.058481   \n",
              "19           32           3      0.0  0.0010          32  33.113752   \n",
              "20           32           3      0.0  0.0005          32  33.236277   \n",
              "21           32           3      0.3  0.0010          32  33.249193   \n",
              "22           32           3      0.2  0.0005          32  33.386098   \n",
              "23           32           3      0.3  0.0005          32  33.674656   \n",
              "\n",
              "         RMSE       MAPE  \n",
              "0   37.845745  88.061268  \n",
              "1   37.957894  87.883125  \n",
              "2   38.161029  89.392035  \n",
              "3   38.526077  88.295192  \n",
              "4   38.561885  90.970484  \n",
              "5   38.606335  91.952732  \n",
              "6   38.666732  92.133740  \n",
              "7   38.693949  91.416412  \n",
              "8   38.696790  92.138664  \n",
              "9   38.795204  88.983806  \n",
              "10  38.961292  92.303179  \n",
              "11  38.967635  93.188199  \n",
              "12  38.981662  88.811010  \n",
              "13  39.061978  92.741898  \n",
              "14  39.118677  92.015390  \n",
              "15  39.434082  93.380019  \n",
              "16  39.702434  90.482206  \n",
              "17  40.032677  94.014475  \n",
              "18  40.057025  92.034609  \n",
              "19  40.109108  92.468383  \n",
              "20  40.139991  96.369682  \n",
              "21  40.172232  92.220454  \n",
              "22  40.262042  95.403038  \n",
              "23  40.554054  96.117949  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-43bb3a11-fe89-4d2a-a570-3322eb431cef\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hidden_size</th>\n",
              "      <th>num_layers</th>\n",
              "      <th>dropout</th>\n",
              "      <th>lr</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>MAE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>31.017536</td>\n",
              "      <td>37.845745</td>\n",
              "      <td>88.061268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>31.141957</td>\n",
              "      <td>37.957894</td>\n",
              "      <td>87.883125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>31.344381</td>\n",
              "      <td>38.161029</td>\n",
              "      <td>89.392035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>31.671511</td>\n",
              "      <td>38.526077</td>\n",
              "      <td>88.295192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>31.681379</td>\n",
              "      <td>38.561885</td>\n",
              "      <td>90.970484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>31.732448</td>\n",
              "      <td>38.606335</td>\n",
              "      <td>91.952732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>31.836496</td>\n",
              "      <td>38.666732</td>\n",
              "      <td>92.133740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>31.868374</td>\n",
              "      <td>38.693949</td>\n",
              "      <td>91.416412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>31.832950</td>\n",
              "      <td>38.696790</td>\n",
              "      <td>92.138664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>31.907455</td>\n",
              "      <td>38.795204</td>\n",
              "      <td>88.983806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>32.076257</td>\n",
              "      <td>38.961292</td>\n",
              "      <td>92.303179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>32.120954</td>\n",
              "      <td>38.967635</td>\n",
              "      <td>93.188199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>32.053184</td>\n",
              "      <td>38.981662</td>\n",
              "      <td>88.811010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>32.243157</td>\n",
              "      <td>39.061978</td>\n",
              "      <td>92.741898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>32.347894</td>\n",
              "      <td>39.118677</td>\n",
              "      <td>92.015390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>32.550378</td>\n",
              "      <td>39.434082</td>\n",
              "      <td>93.380019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>32.711940</td>\n",
              "      <td>39.702434</td>\n",
              "      <td>90.482206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>33.166073</td>\n",
              "      <td>40.032677</td>\n",
              "      <td>94.014475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>33.058481</td>\n",
              "      <td>40.057025</td>\n",
              "      <td>92.034609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>33.113752</td>\n",
              "      <td>40.109108</td>\n",
              "      <td>92.468383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>33.236277</td>\n",
              "      <td>40.139991</td>\n",
              "      <td>96.369682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>33.249193</td>\n",
              "      <td>40.172232</td>\n",
              "      <td>92.220454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>33.386098</td>\n",
              "      <td>40.262042</td>\n",
              "      <td>95.403038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>33.674656</td>\n",
              "      <td>40.554054</td>\n",
              "      <td>96.117949</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-43bb3a11-fe89-4d2a-a570-3322eb431cef')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-43bb3a11-fe89-4d2a-a570-3322eb431cef button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-43bb3a11-fe89-4d2a-a570-3322eb431cef');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-37ace871-f96d-4c79-bda8-56f065e36c84\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-37ace871-f96d-4c79-bda8-56f065e36c84')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-37ace871-f96d-4c79-bda8-56f065e36c84 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_gru_results",
              "summary": "{\n  \"name\": \"df_gru_results\",\n  \"rows\": 24,\n  \"fields\": [\n    {\n      \"column\": \"hidden_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11420804814403214,\n        \"min\": 0.0,\n        \"max\": 0.3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0002553769592276246,\n        \"min\": 0.0005,\n        \"max\": 0.001,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0005\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.751411186825447,\n        \"min\": 31.017535847776614,\n        \"max\": 33.674655920160575,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          31.832949652282498\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7841168299138168,\n        \"min\": 37.845744790354054,\n        \"max\": 40.554054356672616,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          38.69678983401745\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.3845712844509617,\n        \"min\": 87.88312478981054,\n        \"max\": 96.36968155068051,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          92.13866384085665\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hidden size = 64"
      ],
      "metadata": {
        "id": "x5uBkuSbNjF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Hyperparameter grid for GRU\n",
        "hidden_sizes = [64]\n",
        "num_layers_list = [1, 2, 3]\n",
        "dropouts = [0.0, 0.1, 0.2, 0.3]\n",
        "learning_rates = [0.001, 0.0005]\n",
        "batch_sizes = [32]\n",
        "\n",
        "# Cartesian product of all combinations\n",
        "gru_param_grid = list(itertools.product(hidden_sizes, num_layers_list, dropouts, learning_rates, batch_sizes))\n",
        "print(f\"Total GRU configurations: {len(gru_param_grid)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYb5frKIJJi9",
        "outputId": "9b952049-6e40-4b62-cf18-7699cd2b73f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total GRU configurations: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gru_results = []\n",
        "\n",
        "for i, (hidden_size, num_layers, dropout, lr, batch_size) in enumerate(gru_param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(gru_param_grid)}  hidden={hidden_size}, layers={num_layers}, dropout={dropout}, lr={lr}, batch_size={batch_size}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14, sequence_len=30)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_gru_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            lr=lr,\n",
        "            batch_size=batch_size,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            sequence_len=30\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    # Save results\n",
        "    avg_mae = np.mean(mae_scores)\n",
        "    avg_rmse = np.mean(rmse_scores)\n",
        "    avg_mape = np.mean(mape_scores)\n",
        "\n",
        "    gru_results.append({\n",
        "        \"hidden_size\": hidden_size,\n",
        "        \"num_layers\": num_layers,\n",
        "        \"dropout\": dropout,\n",
        "        \"lr\": lr,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"MAE\": avg_mae,\n",
        "        \"RMSE\": avg_rmse,\n",
        "        \"MAPE\": avg_mape\n",
        "    })\n",
        "\n",
        "    print(f\"Avg MAE: {avg_mae:.4f} | RMSE: {avg_rmse:.4f} | MAPE: {avg_mape:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbX0M46cJJoy",
        "outputId": "d3b019cf-52aa-4b7f-803e-f66d01d966d0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Config 1/24  hidden=64, layers=1, dropout=0.0, lr=0.001, batch_size=32\n",
            "Avg MAE: 30.8349 | RMSE: 37.7222 | MAPE: 87.8089\n",
            "\n",
            " Config 2/24  hidden=64, layers=1, dropout=0.0, lr=0.0005, batch_size=32\n",
            "Avg MAE: 30.8407 | RMSE: 37.6596 | MAPE: 88.4100\n",
            "\n",
            " Config 3/24  hidden=64, layers=1, dropout=0.1, lr=0.001, batch_size=32\n",
            "Avg MAE: 30.7921 | RMSE: 37.6373 | MAPE: 88.2749\n",
            "\n",
            " Config 4/24  hidden=64, layers=1, dropout=0.1, lr=0.0005, batch_size=32\n",
            "Avg MAE: 30.9867 | RMSE: 37.8077 | MAPE: 88.2092\n",
            "\n",
            " Config 5/24  hidden=64, layers=1, dropout=0.2, lr=0.001, batch_size=32\n",
            "Avg MAE: 30.9566 | RMSE: 37.8194 | MAPE: 87.3977\n",
            "\n",
            " Config 6/24  hidden=64, layers=1, dropout=0.2, lr=0.0005, batch_size=32\n",
            "Avg MAE: 30.9272 | RMSE: 37.7386 | MAPE: 88.5315\n",
            "\n",
            " Config 7/24  hidden=64, layers=1, dropout=0.3, lr=0.001, batch_size=32\n",
            "Avg MAE: 30.6823 | RMSE: 37.5069 | MAPE: 86.5780\n",
            "\n",
            " Config 8/24  hidden=64, layers=1, dropout=0.3, lr=0.0005, batch_size=32\n",
            "Avg MAE: 30.8600 | RMSE: 37.6720 | MAPE: 89.0100\n",
            "\n",
            " Config 9/24  hidden=64, layers=2, dropout=0.0, lr=0.001, batch_size=32\n",
            "Avg MAE: 32.0735 | RMSE: 38.8698 | MAPE: 88.0598\n",
            "\n",
            " Config 10/24  hidden=64, layers=2, dropout=0.0, lr=0.0005, batch_size=32\n",
            "Avg MAE: 31.4310 | RMSE: 38.3100 | MAPE: 89.4502\n",
            "\n",
            " Config 11/24  hidden=64, layers=2, dropout=0.1, lr=0.001, batch_size=32\n",
            "Avg MAE: 32.5799 | RMSE: 39.4300 | MAPE: 90.9843\n",
            "\n",
            " Config 12/24  hidden=64, layers=2, dropout=0.1, lr=0.0005, batch_size=32\n",
            "Avg MAE: 31.4088 | RMSE: 38.1984 | MAPE: 88.4584\n",
            "\n",
            " Config 13/24  hidden=64, layers=2, dropout=0.2, lr=0.001, batch_size=32\n",
            "Avg MAE: 32.4723 | RMSE: 39.3074 | MAPE: 91.8292\n",
            "\n",
            " Config 14/24  hidden=64, layers=2, dropout=0.2, lr=0.0005, batch_size=32\n",
            "Avg MAE: 31.9304 | RMSE: 38.7917 | MAPE: 89.6573\n",
            "\n",
            " Config 15/24  hidden=64, layers=2, dropout=0.3, lr=0.001, batch_size=32\n",
            "Avg MAE: 31.9531 | RMSE: 38.7625 | MAPE: 89.9388\n",
            "\n",
            " Config 16/24  hidden=64, layers=2, dropout=0.3, lr=0.0005, batch_size=32\n",
            "Avg MAE: 31.3623 | RMSE: 38.1432 | MAPE: 88.5333\n",
            "\n",
            " Config 17/24  hidden=64, layers=3, dropout=0.0, lr=0.001, batch_size=32\n",
            "Avg MAE: 33.7016 | RMSE: 40.7137 | MAPE: 91.9927\n",
            "\n",
            " Config 18/24  hidden=64, layers=3, dropout=0.0, lr=0.0005, batch_size=32\n",
            "Avg MAE: 32.6879 | RMSE: 39.4918 | MAPE: 90.2516\n",
            "\n",
            " Config 19/24  hidden=64, layers=3, dropout=0.1, lr=0.001, batch_size=32\n",
            "Avg MAE: 33.6756 | RMSE: 40.7112 | MAPE: 89.8099\n",
            "\n",
            " Config 20/24  hidden=64, layers=3, dropout=0.1, lr=0.0005, batch_size=32\n",
            "Avg MAE: 32.1115 | RMSE: 39.0906 | MAPE: 90.2206\n",
            "\n",
            " Config 21/24  hidden=64, layers=3, dropout=0.2, lr=0.001, batch_size=32\n",
            "Avg MAE: 34.2464 | RMSE: 41.2357 | MAPE: 94.6426\n",
            "\n",
            " Config 22/24  hidden=64, layers=3, dropout=0.2, lr=0.0005, batch_size=32\n",
            "Avg MAE: 32.4316 | RMSE: 39.2724 | MAPE: 90.0479\n",
            "\n",
            " Config 23/24  hidden=64, layers=3, dropout=0.3, lr=0.001, batch_size=32\n",
            "Avg MAE: 34.3881 | RMSE: 41.3861 | MAPE: 94.6911\n",
            "\n",
            " Config 24/24  hidden=64, layers=3, dropout=0.3, lr=0.0005, batch_size=32\n",
            "Avg MAE: 31.8182 | RMSE: 38.7235 | MAPE: 93.3067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_gru_results = pd.DataFrame(gru_results)\n",
        "df_gru_results = df_gru_results.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
        "df_gru_results"
      ],
      "metadata": {
        "id": "5nulbSSUJKXj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "outputId": "9b9d31cc-f86a-4426-a8b0-86463b8c0c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    hidden_size  num_layers  dropout      lr  batch_size        MAE  \\\n",
              "0            64           1      0.3  0.0010          32  30.682320   \n",
              "1            64           1      0.1  0.0010          32  30.792061   \n",
              "2            64           1      0.0  0.0005          32  30.840682   \n",
              "3            64           1      0.3  0.0005          32  30.859953   \n",
              "4            64           1      0.0  0.0010          32  30.834857   \n",
              "5            64           1      0.2  0.0005          32  30.927238   \n",
              "6            64           1      0.1  0.0005          32  30.986737   \n",
              "7            64           1      0.2  0.0010          32  30.956565   \n",
              "8            64           2      0.3  0.0005          32  31.362332   \n",
              "9            64           2      0.1  0.0005          32  31.408799   \n",
              "10           64           2      0.0  0.0005          32  31.430974   \n",
              "11           64           3      0.3  0.0005          32  31.818214   \n",
              "12           64           2      0.3  0.0010          32  31.953103   \n",
              "13           64           2      0.2  0.0005          32  31.930403   \n",
              "14           64           2      0.0  0.0010          32  32.073473   \n",
              "15           64           3      0.1  0.0005          32  32.111539   \n",
              "16           64           3      0.2  0.0005          32  32.431612   \n",
              "17           64           2      0.2  0.0010          32  32.472289   \n",
              "18           64           2      0.1  0.0010          32  32.579938   \n",
              "19           64           3      0.0  0.0005          32  32.687936   \n",
              "20           64           3      0.1  0.0010          32  33.675579   \n",
              "21           64           3      0.0  0.0010          32  33.701630   \n",
              "22           64           3      0.2  0.0010          32  34.246434   \n",
              "23           64           3      0.3  0.0010          32  34.388064   \n",
              "\n",
              "         RMSE       MAPE  \n",
              "0   37.506927  86.578029  \n",
              "1   37.637256  88.274853  \n",
              "2   37.659643  88.409994  \n",
              "3   37.671996  89.009969  \n",
              "4   37.722187  87.808941  \n",
              "5   37.738610  88.531531  \n",
              "6   37.807722  88.209191  \n",
              "7   37.819388  87.397746  \n",
              "8   38.143227  88.533290  \n",
              "9   38.198385  88.458445  \n",
              "10  38.310000  89.450249  \n",
              "11  38.723482  93.306709  \n",
              "12  38.762548  89.938776  \n",
              "13  38.791686  89.657337  \n",
              "14  38.869782  88.059803  \n",
              "15  39.090619  90.220557  \n",
              "16  39.272384  90.047946  \n",
              "17  39.307424  91.829159  \n",
              "18  39.429994  90.984309  \n",
              "19  39.491842  90.251593  \n",
              "20  40.711179  89.809865  \n",
              "21  40.713732  91.992736  \n",
              "22  41.235730  94.642597  \n",
              "23  41.386137  94.691110  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-554c6c7f-dd12-4793-b37b-213023f2c9b1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hidden_size</th>\n",
              "      <th>num_layers</th>\n",
              "      <th>dropout</th>\n",
              "      <th>lr</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>MAE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>30.682320</td>\n",
              "      <td>37.506927</td>\n",
              "      <td>86.578029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>30.792061</td>\n",
              "      <td>37.637256</td>\n",
              "      <td>88.274853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.840682</td>\n",
              "      <td>37.659643</td>\n",
              "      <td>88.409994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.859953</td>\n",
              "      <td>37.671996</td>\n",
              "      <td>89.009969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>30.834857</td>\n",
              "      <td>37.722187</td>\n",
              "      <td>87.808941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.927238</td>\n",
              "      <td>37.738610</td>\n",
              "      <td>88.531531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.986737</td>\n",
              "      <td>37.807722</td>\n",
              "      <td>88.209191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>30.956565</td>\n",
              "      <td>37.819388</td>\n",
              "      <td>87.397746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>31.362332</td>\n",
              "      <td>38.143227</td>\n",
              "      <td>88.533290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>31.408799</td>\n",
              "      <td>38.198385</td>\n",
              "      <td>88.458445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>31.430974</td>\n",
              "      <td>38.310000</td>\n",
              "      <td>89.450249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>31.818214</td>\n",
              "      <td>38.723482</td>\n",
              "      <td>93.306709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>31.953103</td>\n",
              "      <td>38.762548</td>\n",
              "      <td>89.938776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>31.930403</td>\n",
              "      <td>38.791686</td>\n",
              "      <td>89.657337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>32.073473</td>\n",
              "      <td>38.869782</td>\n",
              "      <td>88.059803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>32.111539</td>\n",
              "      <td>39.090619</td>\n",
              "      <td>90.220557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>32.431612</td>\n",
              "      <td>39.272384</td>\n",
              "      <td>90.047946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>32.472289</td>\n",
              "      <td>39.307424</td>\n",
              "      <td>91.829159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>32.579938</td>\n",
              "      <td>39.429994</td>\n",
              "      <td>90.984309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>32.687936</td>\n",
              "      <td>39.491842</td>\n",
              "      <td>90.251593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>33.675579</td>\n",
              "      <td>40.711179</td>\n",
              "      <td>89.809865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>33.701630</td>\n",
              "      <td>40.713732</td>\n",
              "      <td>91.992736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>34.246434</td>\n",
              "      <td>41.235730</td>\n",
              "      <td>94.642597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>34.388064</td>\n",
              "      <td>41.386137</td>\n",
              "      <td>94.691110</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-554c6c7f-dd12-4793-b37b-213023f2c9b1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-554c6c7f-dd12-4793-b37b-213023f2c9b1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-554c6c7f-dd12-4793-b37b-213023f2c9b1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6391c47b-b0df-4f6a-b0d4-4e505a312024\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6391c47b-b0df-4f6a-b0d4-4e505a312024')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6391c47b-b0df-4f6a-b0d4-4e505a312024 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_ba9a943d-c170-4b77-b17a-c702c8ac7d47\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_gru_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ba9a943d-c170-4b77-b17a-c702c8ac7d47 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_gru_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_gru_results",
              "summary": "{\n  \"name\": \"df_gru_results\",\n  \"rows\": 24,\n  \"fields\": [\n    {\n      \"column\": \"hidden_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 64,\n        \"max\": 64,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11420804814403214,\n        \"min\": 0.0,\n        \"max\": 0.3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0002553769592276246,\n        \"min\": 0.0005,\n        \"max\": 0.001,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0005\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1275648913332237,\n        \"min\": 30.68231967046523,\n        \"max\": 34.38806444682831,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          31.362332417432835\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1837035635793058,\n        \"min\": 37.50692694744155,\n        \"max\": 41.38613653274971,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          38.14322689274511\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.148211721451499,\n        \"min\": 86.57802895099792,\n        \"max\": 94.69111002436877,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          88.53329006058539\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "hidden size = 128"
      ],
      "metadata": {
        "id": "H8dBGrPYQyH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Hyperparameter grid for GRU\n",
        "hidden_sizes = [128]\n",
        "num_layers_list = [1, 2, 3]\n",
        "dropouts = [0.0, 0.1, 0.2, 0.3]\n",
        "learning_rates = [0.001, 0.0005]\n",
        "batch_sizes = [32]\n",
        "\n",
        "# Cartesian product of all combinations\n",
        "gru_param_grid = list(itertools.product(hidden_sizes, num_layers_list, dropouts, learning_rates, batch_sizes))\n",
        "print(f\"Total GRU configurations: {len(gru_param_grid)}\")"
      ],
      "metadata": {
        "id": "SeOAYd5FQxYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e350228-b9a4-47fc-8af3-94f0136af5f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total GRU configurations: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gru_results = []\n",
        "\n",
        "for i, (hidden_size, num_layers, dropout, lr, batch_size) in enumerate(gru_param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(gru_param_grid)}  hidden={hidden_size}, layers={num_layers}, dropout={dropout}, lr={lr}, batch_size={batch_size}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14, sequence_len=30)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_gru_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            lr=lr,\n",
        "            batch_size=batch_size,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            sequence_len=30\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    # Save results\n",
        "    avg_mae = np.mean(mae_scores)\n",
        "    avg_rmse = np.mean(rmse_scores)\n",
        "    avg_mape = np.mean(mape_scores)\n",
        "\n",
        "    gru_results.append({\n",
        "        \"hidden_size\": hidden_size,\n",
        "        \"num_layers\": num_layers,\n",
        "        \"dropout\": dropout,\n",
        "        \"lr\": lr,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"MAE\": avg_mae,\n",
        "        \"RMSE\": avg_rmse,\n",
        "        \"MAPE\": avg_mape\n",
        "    })\n",
        "\n",
        "    print(f\"Avg MAE: {avg_mae:.4f} | RMSE: {avg_rmse:.4f} | MAPE: {avg_mape:.4f}\")"
      ],
      "metadata": {
        "id": "BiDpr_ZhQxm2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e922b017-7445-4fd4-c27f-d2256f87e5d4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Config 1/24  hidden=128, layers=1, dropout=0.0, lr=0.001, batch_size=32\n",
            "Avg MAE: 31.2240 | RMSE: 38.2978 | MAPE: 88.8398\n",
            "\n",
            " Config 2/24  hidden=128, layers=1, dropout=0.0, lr=0.0005, batch_size=32\n",
            "Avg MAE: 30.6073 | RMSE: 37.4164 | MAPE: 87.0896\n",
            "\n",
            " Config 3/24  hidden=128, layers=1, dropout=0.1, lr=0.001, batch_size=32\n",
            "Avg MAE: 30.3729 | RMSE: 37.2731 | MAPE: 87.6090\n",
            "\n",
            " Config 4/24  hidden=128, layers=1, dropout=0.1, lr=0.0005, batch_size=32\n",
            "Avg MAE: 30.4489 | RMSE: 37.3187 | MAPE: 86.9825\n",
            "\n",
            " Config 5/24  hidden=128, layers=1, dropout=0.2, lr=0.001, batch_size=32\n",
            "Avg MAE: 31.4025 | RMSE: 38.4069 | MAPE: 89.0326\n",
            "\n",
            " Config 6/24  hidden=128, layers=1, dropout=0.2, lr=0.0005, batch_size=32\n",
            "Avg MAE: 30.5978 | RMSE: 37.4325 | MAPE: 85.9272\n",
            "\n",
            " Config 7/24  hidden=128, layers=1, dropout=0.3, lr=0.001, batch_size=32\n",
            "Avg MAE: 31.4151 | RMSE: 38.3928 | MAPE: 91.2921\n",
            "\n",
            " Config 8/24  hidden=128, layers=1, dropout=0.3, lr=0.0005, batch_size=32\n",
            "Avg MAE: 30.7724 | RMSE: 37.5631 | MAPE: 88.7518\n",
            "\n",
            " Config 9/24  hidden=128, layers=2, dropout=0.0, lr=0.001, batch_size=32\n",
            "Avg MAE: 31.9846 | RMSE: 38.8876 | MAPE: 89.8239\n",
            "\n",
            " Config 10/24  hidden=128, layers=2, dropout=0.0, lr=0.0005, batch_size=32\n",
            "Avg MAE: 31.1196 | RMSE: 37.9784 | MAPE: 88.0585\n",
            "\n",
            " Config 11/24  hidden=128, layers=2, dropout=0.1, lr=0.001, batch_size=32\n",
            "Avg MAE: 32.6848 | RMSE: 39.5783 | MAPE: 90.6503\n",
            "\n",
            " Config 12/24  hidden=128, layers=2, dropout=0.1, lr=0.0005, batch_size=32\n",
            "Avg MAE: 30.8844 | RMSE: 37.6816 | MAPE: 87.0474\n",
            "\n",
            " Config 13/24  hidden=128, layers=2, dropout=0.2, lr=0.001, batch_size=32\n",
            "Avg MAE: 32.5400 | RMSE: 39.5401 | MAPE: 91.6524\n",
            "\n",
            " Config 14/24  hidden=128, layers=2, dropout=0.2, lr=0.0005, batch_size=32\n",
            "Avg MAE: 31.4490 | RMSE: 38.1790 | MAPE: 88.1062\n",
            "\n",
            " Config 15/24  hidden=128, layers=2, dropout=0.3, lr=0.001, batch_size=32\n",
            "Avg MAE: 32.8212 | RMSE: 39.6460 | MAPE: 92.4260\n",
            "\n",
            " Config 16/24  hidden=128, layers=2, dropout=0.3, lr=0.0005, batch_size=32\n",
            "Avg MAE: 31.3293 | RMSE: 38.2905 | MAPE: 90.0333\n",
            "\n",
            " Config 17/24  hidden=128, layers=3, dropout=0.0, lr=0.001, batch_size=32\n",
            "Avg MAE: 39.7914 | RMSE: 46.9781 | MAPE: 113.4724\n",
            "\n",
            " Config 18/24  hidden=128, layers=3, dropout=0.0, lr=0.0005, batch_size=32\n",
            "Avg MAE: 33.0768 | RMSE: 40.0444 | MAPE: 87.7435\n",
            "\n",
            " Config 19/24  hidden=128, layers=3, dropout=0.1, lr=0.001, batch_size=32\n",
            "Avg MAE: 39.8624 | RMSE: 47.1870 | MAPE: 118.8699\n",
            "\n",
            " Config 20/24  hidden=128, layers=3, dropout=0.1, lr=0.0005, batch_size=32\n",
            "Avg MAE: 33.0635 | RMSE: 39.9115 | MAPE: 90.3622\n",
            "\n",
            " Config 21/24  hidden=128, layers=3, dropout=0.2, lr=0.001, batch_size=32\n",
            "Avg MAE: 38.9051 | RMSE: 46.2354 | MAPE: 117.7926\n",
            "\n",
            " Config 22/24  hidden=128, layers=3, dropout=0.2, lr=0.0005, batch_size=32\n",
            "Avg MAE: 33.6619 | RMSE: 40.5487 | MAPE: 89.1949\n",
            "\n",
            " Config 23/24  hidden=128, layers=3, dropout=0.3, lr=0.001, batch_size=32\n",
            "Avg MAE: 39.0510 | RMSE: 46.4148 | MAPE: 120.1433\n",
            "\n",
            " Config 24/24  hidden=128, layers=3, dropout=0.3, lr=0.0005, batch_size=32\n",
            "Avg MAE: 32.8938 | RMSE: 39.7958 | MAPE: 89.4162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_gru_results = pd.DataFrame(gru_results)\n",
        "df_gru_results = df_gru_results.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
        "df_gru_results"
      ],
      "metadata": {
        "id": "5hDfd_IfQxxr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "outputId": "df71d143-bd45-4011-ecc3-195cc30f277d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    hidden_size  num_layers  dropout      lr  batch_size        MAE  \\\n",
              "0           128           1      0.1  0.0010          32  30.372874   \n",
              "1           128           1      0.1  0.0005          32  30.448919   \n",
              "2           128           1      0.0  0.0005          32  30.607306   \n",
              "3           128           1      0.2  0.0005          32  30.597763   \n",
              "4           128           1      0.3  0.0005          32  30.772374   \n",
              "5           128           2      0.1  0.0005          32  30.884410   \n",
              "6           128           2      0.0  0.0005          32  31.119612   \n",
              "7           128           2      0.2  0.0005          32  31.448987   \n",
              "8           128           2      0.3  0.0005          32  31.329255   \n",
              "9           128           1      0.0  0.0010          32  31.224048   \n",
              "10          128           1      0.3  0.0010          32  31.415118   \n",
              "11          128           1      0.2  0.0010          32  31.402523   \n",
              "12          128           2      0.0  0.0010          32  31.984559   \n",
              "13          128           2      0.2  0.0010          32  32.540010   \n",
              "14          128           2      0.1  0.0010          32  32.684845   \n",
              "15          128           2      0.3  0.0010          32  32.821232   \n",
              "16          128           3      0.3  0.0005          32  32.893814   \n",
              "17          128           3      0.1  0.0005          32  33.063549   \n",
              "18          128           3      0.0  0.0005          32  33.076766   \n",
              "19          128           3      0.2  0.0005          32  33.661930   \n",
              "20          128           3      0.2  0.0010          32  38.905061   \n",
              "21          128           3      0.3  0.0010          32  39.051023   \n",
              "22          128           3      0.0  0.0010          32  39.791450   \n",
              "23          128           3      0.1  0.0010          32  39.862433   \n",
              "\n",
              "         RMSE        MAPE  \n",
              "0   37.273085   87.608985  \n",
              "1   37.318707   86.982484  \n",
              "2   37.416392   87.089638  \n",
              "3   37.432491   85.927161  \n",
              "4   37.563075   88.751821  \n",
              "5   37.681634   87.047427  \n",
              "6   37.978445   88.058530  \n",
              "7   38.179006   88.106218  \n",
              "8   38.290526   90.033279  \n",
              "9   38.297774   88.839790  \n",
              "10  38.392818   91.292148  \n",
              "11  38.406860   89.032604  \n",
              "12  38.887646   89.823892  \n",
              "13  39.540110   91.652395  \n",
              "14  39.578280   90.650255  \n",
              "15  39.645955   92.425950  \n",
              "16  39.795848   89.416159  \n",
              "17  39.911457   90.362173  \n",
              "18  40.044416   87.743484  \n",
              "19  40.548679   89.194876  \n",
              "20  46.235393  117.792612  \n",
              "21  46.414784  120.143261  \n",
              "22  46.978111  113.472361  \n",
              "23  47.186968  118.869877  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-13b22a55-8131-4113-8e29-dac3bb294140\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hidden_size</th>\n",
              "      <th>num_layers</th>\n",
              "      <th>dropout</th>\n",
              "      <th>lr</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>MAE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>30.372874</td>\n",
              "      <td>37.273085</td>\n",
              "      <td>87.608985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.448919</td>\n",
              "      <td>37.318707</td>\n",
              "      <td>86.982484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.607306</td>\n",
              "      <td>37.416392</td>\n",
              "      <td>87.089638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.597763</td>\n",
              "      <td>37.432491</td>\n",
              "      <td>85.927161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.772374</td>\n",
              "      <td>37.563075</td>\n",
              "      <td>88.751821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.884410</td>\n",
              "      <td>37.681634</td>\n",
              "      <td>87.047427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>31.119612</td>\n",
              "      <td>37.978445</td>\n",
              "      <td>88.058530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>31.448987</td>\n",
              "      <td>38.179006</td>\n",
              "      <td>88.106218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>31.329255</td>\n",
              "      <td>38.290526</td>\n",
              "      <td>90.033279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>31.224048</td>\n",
              "      <td>38.297774</td>\n",
              "      <td>88.839790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>31.415118</td>\n",
              "      <td>38.392818</td>\n",
              "      <td>91.292148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>31.402523</td>\n",
              "      <td>38.406860</td>\n",
              "      <td>89.032604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>31.984559</td>\n",
              "      <td>38.887646</td>\n",
              "      <td>89.823892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>32.540010</td>\n",
              "      <td>39.540110</td>\n",
              "      <td>91.652395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>32.684845</td>\n",
              "      <td>39.578280</td>\n",
              "      <td>90.650255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>32.821232</td>\n",
              "      <td>39.645955</td>\n",
              "      <td>92.425950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>32.893814</td>\n",
              "      <td>39.795848</td>\n",
              "      <td>89.416159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>33.063549</td>\n",
              "      <td>39.911457</td>\n",
              "      <td>90.362173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>33.076766</td>\n",
              "      <td>40.044416</td>\n",
              "      <td>87.743484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>33.661930</td>\n",
              "      <td>40.548679</td>\n",
              "      <td>89.194876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>38.905061</td>\n",
              "      <td>46.235393</td>\n",
              "      <td>117.792612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>39.051023</td>\n",
              "      <td>46.414784</td>\n",
              "      <td>120.143261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>39.791450</td>\n",
              "      <td>46.978111</td>\n",
              "      <td>113.472361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>39.862433</td>\n",
              "      <td>47.186968</td>\n",
              "      <td>118.869877</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13b22a55-8131-4113-8e29-dac3bb294140')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-13b22a55-8131-4113-8e29-dac3bb294140 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-13b22a55-8131-4113-8e29-dac3bb294140');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-befdbdb1-2ba6-4c85-a11b-298c6a393036\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-befdbdb1-2ba6-4c85-a11b-298c6a393036')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-befdbdb1-2ba6-4c85-a11b-298c6a393036 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_0bf858a3-da03-4d46-ad2f-19e0a3679997\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_gru_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_0bf858a3-da03-4d46-ad2f-19e0a3679997 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_gru_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_gru_results",
              "summary": "{\n  \"name\": \"df_gru_results\",\n  \"rows\": 24,\n  \"fields\": [\n    {\n      \"column\": \"hidden_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 128,\n        \"max\": 128,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11420804814403214,\n        \"min\": 0.0,\n        \"max\": 0.3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0002553769592276246,\n        \"min\": 0.0005,\n        \"max\": 0.001,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0005\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.0774702668276634,\n        \"min\": 30.372873702927905,\n        \"max\": 39.862433304806174,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          31.329254880216034\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.230214365910277,\n        \"min\": 37.27308498755094,\n        \"max\": 47.18696808987748,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          38.290526077669846\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11.038132848541947,\n        \"min\": 85.92716144173401,\n        \"max\": 120.14326093984987,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          90.03327929644006\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1D CNN"
      ],
      "metadata": {
        "id": "0nTSPOfM6vaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CNN1DForecastNet(nn.Module):\n",
        "    def __init__(self, input_channels, seq_len, num_outputs=14, num_filters=64, kernel_size=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=num_filters, kernel_size=kernel_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Compute output length after conv: L_out = L_in - kernel_size + 1\n",
        "        conv_out_len = seq_len - kernel_size + 1\n",
        "\n",
        "        self.flatten_dim = num_filters * conv_out_len\n",
        "        self.fc = nn.Linear(self.flatten_dim, num_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_len, input_channels]\n",
        "        x = x.permute(0, 2, 1)  # to shape [batch_size, input_channels, seq_len]\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.size(0), -1)  # flatten\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "H9JAneA833Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_cnn_fold(X_train, y_train, X_val, y_val,\n",
        "                       sequence_len=30,\n",
        "                       num_outputs=14,\n",
        "                       num_filters=64,\n",
        "                       kernel_size=3,\n",
        "                       dropout=0.2,\n",
        "                       num_epochs=50,\n",
        "                       patience=5,\n",
        "                       batch_size=32,\n",
        "                       lr=0.001):\n",
        "\n",
        "    # --- Scaling ---\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_val_scaled = scaler_X.transform(X_val)\n",
        "\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "    y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "    # --- Sequence creation ---\n",
        "    X_train_seq, y_train_seq = create_lstm_input(X_train_scaled, y_train_scaled, sequence_len)\n",
        "    X_val_seq, y_val_seq = create_lstm_input(X_val_scaled, y_val_scaled, sequence_len)\n",
        "\n",
        "    if X_val_seq.shape[0] == 0:\n",
        "        print(\"Skipping fold: Not enough validation sequences.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # --- DataLoader ---\n",
        "    train_loader = DataLoader(LSTMForecastDataset(X_train_seq, y_train_seq), batch_size=batch_size, shuffle=False)\n",
        "    val_loader = DataLoader(LSTMForecastDataset(X_val_seq, y_val_seq), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # --- Model ---\n",
        "    model = CNN1DForecastNet(\n",
        "        input_channels=X_train_seq.shape[2],\n",
        "        seq_len=sequence_len,\n",
        "        num_outputs=num_outputs,\n",
        "        num_filters=num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dropout=dropout\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    patience_counter = 0\n",
        "\n",
        "    # --- Training ---\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_preds = torch.cat([model(xb.to(device)) for xb, _ in val_loader])\n",
        "            val_targets = torch.cat([yb.to(device) for _, yb in val_loader])\n",
        "            val_loss = criterion(val_preds, val_targets)\n",
        "\n",
        "        if val_loss.item() < best_val_loss - 1e-4:\n",
        "            best_val_loss = val_loss.item()\n",
        "            best_model_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                break\n",
        "\n",
        "    # --- Load best model ---\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "    # --- Final Evaluation ---\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32).to(device)\n",
        "        preds_scaled = model(X_val_tensor).cpu().numpy()\n",
        "\n",
        "    preds = scaler_y.inverse_transform(preds_scaled)\n",
        "    targets = scaler_y.inverse_transform(y_val_seq)\n",
        "\n",
        "    mae = mean_absolute_error(targets.flatten(), preds.flatten())\n",
        "    rmse = np.sqrt(mean_squared_error(targets.flatten(), preds.flatten()))\n",
        "    mape = mean_absolute_percentage_error(targets.flatten(), preds.flatten()) * 100\n",
        "\n",
        "    return mae, rmse, mape"
      ],
      "metadata": {
        "id": "QwYvlrKr33MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_len = 30\n",
        "horizon = 14\n",
        "initial_train_size = 1095\n",
        "\n",
        "mae_scores = []\n",
        "rmse_scores = []\n",
        "mape_scores = []\n",
        "\n",
        "for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "    expanding_window_cv(X, y, initial_train_size=initial_train_size, horizon=horizon, step=horizon, sequence_len=sequence_len)\n",
        "):\n",
        "    print(f\"\\nFold {fold+1}\")\n",
        "    mae, rmse, mape = train_one_cnn_fold(\n",
        "        X_tr, y_tr, X_val, y_val,\n",
        "        sequence_len=sequence_len,\n",
        "        num_epochs=50,\n",
        "        patience=5,\n",
        "        batch_size=32,\n",
        "        lr=0.001,\n",
        "        num_filters=64,\n",
        "        kernel_size=3,\n",
        "        dropout=0.2\n",
        "    )\n",
        "\n",
        "    if mae is None:\n",
        "        continue  # skip invalid fold\n",
        "\n",
        "    print(f\"  MAE: {mae:.4f} | RMSE: {rmse:.4f} | MAPE: {mape:.4f}\")\n",
        "    mae_scores.append(mae)\n",
        "    rmse_scores.append(rmse)\n",
        "    mape_scores.append(mape)\n",
        "\n",
        "# --- 7. Final CV Results ---\n",
        "print(\"\\n Final CNN CV Results:\")\n",
        "print(f\"Average MAE:  {np.mean(mae_scores):.4f}\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):.4f}\")\n",
        "print(f\"Average MAPE: {np.mean(mape_scores):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awEUX2Ju600v",
        "outputId": "14ec531d-c84e-46a3-bd80-21aea61c3729",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1\n",
            "  MAE: 9.2065 | RMSE: 10.9951 | MAPE: 37.3955\n",
            "\n",
            "Fold 2\n",
            "  MAE: 8.1604 | RMSE: 10.4018 | MAPE: 60.6735\n",
            "\n",
            "Fold 3\n",
            "  MAE: 6.2092 | RMSE: 8.4651 | MAPE: 47.6519\n",
            "\n",
            "Fold 4\n",
            "  MAE: 4.1062 | RMSE: 5.2466 | MAPE: 12.1599\n",
            "\n",
            "Fold 5\n",
            "  MAE: 5.6599 | RMSE: 7.6750 | MAPE: 21.4291\n",
            "\n",
            "Fold 6\n",
            "  MAE: 6.3010 | RMSE: 8.3912 | MAPE: 25.3544\n",
            "\n",
            "Fold 7\n",
            "  MAE: 7.4965 | RMSE: 8.7249 | MAPE: 21.7351\n",
            "\n",
            "Fold 8\n",
            "  MAE: 7.8186 | RMSE: 8.7940 | MAPE: 18.4438\n",
            "\n",
            "Fold 9\n",
            "  MAE: 8.5261 | RMSE: 9.6369 | MAPE: 19.4468\n",
            "\n",
            "Fold 10\n",
            "  MAE: 4.7684 | RMSE: 6.9899 | MAPE: 11.5839\n",
            "\n",
            "Fold 11\n",
            "  MAE: 2.3379 | RMSE: 3.1121 | MAPE: 4.4672\n",
            "\n",
            "Fold 12\n",
            "  MAE: 6.2168 | RMSE: 8.0020 | MAPE: 12.0485\n",
            "\n",
            "Fold 13\n",
            "  MAE: 5.0222 | RMSE: 6.4630 | MAPE: 9.6629\n",
            "\n",
            "Fold 14\n",
            "  MAE: 5.8223 | RMSE: 7.6754 | MAPE: 11.0123\n",
            "\n",
            "Fold 15\n",
            "  MAE: 11.8153 | RMSE: 15.6474 | MAPE: 31.2153\n",
            "\n",
            "Fold 16\n",
            "  MAE: 16.0109 | RMSE: 19.5513 | MAPE: 48.2376\n",
            "\n",
            "Fold 17\n",
            "  MAE: 12.8769 | RMSE: 15.7129 | MAPE: 34.9465\n",
            "\n",
            "Fold 18\n",
            "  MAE: 9.2976 | RMSE: 11.6386 | MAPE: 23.7148\n",
            "\n",
            "Fold 19\n",
            "  MAE: 9.2851 | RMSE: 11.6799 | MAPE: 17.7513\n",
            "\n",
            "Fold 20\n",
            "  MAE: 15.4721 | RMSE: 18.5106 | MAPE: 28.8451\n",
            "\n",
            "Fold 21\n",
            "  MAE: 12.3270 | RMSE: 15.8288 | MAPE: 28.4653\n",
            "\n",
            "Fold 22\n",
            "  MAE: 9.5557 | RMSE: 14.9340 | MAPE: 48.1442\n",
            "\n",
            "Fold 23\n",
            "  MAE: 10.0108 | RMSE: 14.5973 | MAPE: 49.5808\n",
            "\n",
            "Fold 24\n",
            "  MAE: 10.7690 | RMSE: 14.0922 | MAPE: 35.1725\n",
            "\n",
            "Fold 25\n",
            "  MAE: 14.1545 | RMSE: 16.8740 | MAPE: 34.5767\n",
            "\n",
            "Fold 26\n",
            "  MAE: 6.7316 | RMSE: 10.4200 | MAPE: 32.3260\n",
            "\n",
            "Fold 27\n",
            "  MAE: 9.1672 | RMSE: 12.6898 | MAPE: 59.1995\n",
            "\n",
            "Fold 28\n",
            "  MAE: 10.4209 | RMSE: 14.5147 | MAPE: 92.1111\n",
            "\n",
            "Fold 29\n",
            "  MAE: 6.4379 | RMSE: 8.3049 | MAPE: 46.1260\n",
            "\n",
            "Fold 30\n",
            "  MAE: 8.4710 | RMSE: 9.7933 | MAPE: 24.7964\n",
            "\n",
            "Fold 31\n",
            "  MAE: 7.4715 | RMSE: 10.4793 | MAPE: 28.6936\n",
            "\n",
            "Fold 32\n",
            "  MAE: 5.4701 | RMSE: 8.3674 | MAPE: 20.5675\n",
            "\n",
            "Fold 33\n",
            "  MAE: 9.2691 | RMSE: 11.5218 | MAPE: 35.7314\n",
            "\n",
            "Fold 34\n",
            "  MAE: 5.9738 | RMSE: 8.8443 | MAPE: 31.9540\n",
            "\n",
            "Fold 35\n",
            "  MAE: 6.4411 | RMSE: 8.7031 | MAPE: 31.4289\n",
            "\n",
            "Fold 36\n",
            "  MAE: 6.5928 | RMSE: 8.1399 | MAPE: 22.4330\n",
            "\n",
            "Fold 37\n",
            "  MAE: 10.2486 | RMSE: 11.7445 | MAPE: 29.7457\n",
            "\n",
            "Fold 38\n",
            "  MAE: 8.2878 | RMSE: 9.4452 | MAPE: 23.8451\n",
            "\n",
            "Fold 39\n",
            "  MAE: 4.9054 | RMSE: 6.7653 | MAPE: 18.4896\n",
            "\n",
            "Fold 40\n",
            "  MAE: 6.5382 | RMSE: 8.1094 | MAPE: 21.5903\n",
            "\n",
            "Fold 41\n",
            "  MAE: 5.7060 | RMSE: 7.1662 | MAPE: 18.1163\n",
            "\n",
            "Fold 42\n",
            "  MAE: 4.8258 | RMSE: 6.1436 | MAPE: 15.6986\n",
            "\n",
            "Fold 43\n",
            "  MAE: 5.5169 | RMSE: 6.6871 | MAPE: 16.4073\n",
            "\n",
            "Fold 44\n",
            "  MAE: 6.3381 | RMSE: 7.6049 | MAPE: 16.5706\n",
            "\n",
            "Fold 45\n",
            "  MAE: 6.2376 | RMSE: 7.3172 | MAPE: 14.7392\n",
            "\n",
            "Fold 46\n",
            "  MAE: 6.8184 | RMSE: 8.5393 | MAPE: 27.1194\n",
            "\n",
            "Fold 47\n",
            "  MAE: 7.2204 | RMSE: 9.8156 | MAPE: 57.7237\n",
            "\n",
            "Fold 48\n",
            "  MAE: 7.7286 | RMSE: 10.2484 | MAPE: 60.2393\n",
            "\n",
            "Fold 49\n",
            "  MAE: 8.3489 | RMSE: 9.6687 | MAPE: 30.8069\n",
            "\n",
            "Fold 50\n",
            "  MAE: 9.5732 | RMSE: 10.8339 | MAPE: 45.3232\n",
            "\n",
            "Fold 51\n",
            "  MAE: 14.1722 | RMSE: 15.5051 | MAPE: 104.8666\n",
            "\n",
            "Fold 52\n",
            "  MAE: 13.2520 | RMSE: 15.2200 | MAPE: 199.8205\n",
            "\n",
            "Fold 53\n",
            "  MAE: 11.6710 | RMSE: 14.0933 | MAPE: 197.2711\n",
            "\n",
            "Fold 54\n",
            "  MAE: 11.0555 | RMSE: 12.8925 | MAPE: 120.5169\n",
            "\n",
            "Fold 55\n",
            "  MAE: 11.4323 | RMSE: 13.0018 | MAPE: 129.0743\n",
            "\n",
            "Fold 56\n",
            "  MAE: 14.3427 | RMSE: 16.3351 | MAPE: 171.3724\n",
            "\n",
            "Fold 57\n",
            "  MAE: 11.7486 | RMSE: 14.2701 | MAPE: 122.6844\n",
            "\n",
            "Fold 58\n",
            "  MAE: 9.6073 | RMSE: 11.2475 | MAPE: 88.5974\n",
            "\n",
            "Fold 59\n",
            "  MAE: 8.4441 | RMSE: 9.9177 | MAPE: 96.6287\n",
            "\n",
            "Fold 60\n",
            "  MAE: 7.7030 | RMSE: 9.2300 | MAPE: 57.1864\n",
            "\n",
            "Fold 61\n",
            "  MAE: 7.9176 | RMSE: 9.8687 | MAPE: 51.8323\n",
            "\n",
            "Fold 62\n",
            "  MAE: 9.6919 | RMSE: 12.5423 | MAPE: 124.2722\n",
            "\n",
            "Fold 63\n",
            "  MAE: 9.5276 | RMSE: 12.9519 | MAPE: 153.4675\n",
            "\n",
            "Fold 64\n",
            "  MAE: 7.6666 | RMSE: 10.6446 | MAPE: 93.4625\n",
            "\n",
            "Fold 65\n",
            "  MAE: 7.7070 | RMSE: 10.2027 | MAPE: 65.7473\n",
            "\n",
            "Fold 66\n",
            "  MAE: 7.7792 | RMSE: 9.1878 | MAPE: 24.2718\n",
            "\n",
            "Fold 67\n",
            "  MAE: 9.4077 | RMSE: 12.2300 | MAPE: 36.1045\n",
            "\n",
            "Fold 68\n",
            "  MAE: 12.5511 | RMSE: 15.5014 | MAPE: 99.8237\n",
            "\n",
            "Fold 69\n",
            "  MAE: 12.3916 | RMSE: 14.9778 | MAPE: 108.1869\n",
            "\n",
            "Fold 70\n",
            "  MAE: 13.1477 | RMSE: 15.9371 | MAPE: 134.7731\n",
            "\n",
            "Fold 71\n",
            "  MAE: 16.2984 | RMSE: 18.9963 | MAPE: 353.8717\n",
            "\n",
            "Fold 72\n",
            "  MAE: 18.4040 | RMSE: 20.6000 | MAPE: 457.8177\n",
            "\n",
            "Fold 73\n",
            "  MAE: 19.0682 | RMSE: 22.2826 | MAPE: 293.9222\n",
            "\n",
            "Fold 74\n",
            "  MAE: 15.6299 | RMSE: 19.8295 | MAPE: 54.9581\n",
            "\n",
            "Fold 75\n",
            "  MAE: 14.5734 | RMSE: 18.9384 | MAPE: 50.7366\n",
            "\n",
            "Fold 76\n",
            "  MAE: 18.4355 | RMSE: 21.6994 | MAPE: 39.2146\n",
            "\n",
            "Fold 77\n",
            "  MAE: 15.0551 | RMSE: 17.7814 | MAPE: 29.4350\n",
            "\n",
            "Fold 78\n",
            "  MAE: 11.6068 | RMSE: 14.4958 | MAPE: 23.5050\n",
            "\n",
            "Fold 79\n",
            "  MAE: 10.7580 | RMSE: 12.9234 | MAPE: 24.0500\n",
            "\n",
            "Fold 80\n",
            "  MAE: 14.4450 | RMSE: 16.3773 | MAPE: 33.2829\n",
            "\n",
            "Fold 81\n",
            "  MAE: 15.1011 | RMSE: 17.7245 | MAPE: 44.0961\n",
            "\n",
            "Fold 82\n",
            "  MAE: 15.9960 | RMSE: 19.4619 | MAPE: 46.5941\n",
            "\n",
            "Fold 83\n",
            "  MAE: 16.0352 | RMSE: 19.3526 | MAPE: 38.9932\n",
            "\n",
            "Fold 84\n",
            "  MAE: 22.3025 | RMSE: 25.0271 | MAPE: 43.8342\n",
            "\n",
            "Fold 85\n",
            "  MAE: 21.6139 | RMSE: 23.5338 | MAPE: 48.1901\n",
            "\n",
            "Fold 86\n",
            "  MAE: 18.2400 | RMSE: 19.9027 | MAPE: 38.3318\n",
            "\n",
            "Fold 87\n",
            "  MAE: 22.5519 | RMSE: 23.8014 | MAPE: 31.3856\n",
            "\n",
            "Fold 88\n",
            "  MAE: 25.5175 | RMSE: 26.6267 | MAPE: 30.8070\n",
            "\n",
            "Fold 89\n",
            "  MAE: 26.8785 | RMSE: 28.7285 | MAPE: 31.9247\n",
            "\n",
            "Fold 90\n",
            "  MAE: 32.0142 | RMSE: 35.3810 | MAPE: 40.9803\n",
            "\n",
            "Fold 91\n",
            "  MAE: 36.1085 | RMSE: 39.3338 | MAPE: 45.3210\n",
            "\n",
            "Fold 92\n",
            "  MAE: 25.1956 | RMSE: 30.0472 | MAPE: 26.1946\n",
            "\n",
            "Fold 93\n",
            "  MAE: 47.1984 | RMSE: 51.0941 | MAPE: 38.1449\n",
            "\n",
            "Fold 94\n",
            "  MAE: 45.1763 | RMSE: 53.1092 | MAPE: 34.8026\n",
            "\n",
            "Fold 95\n",
            "  MAE: 38.0881 | RMSE: 50.6076 | MAPE: 37.0882\n",
            "\n",
            "Fold 96\n",
            "  MAE: 53.8553 | RMSE: 65.5706 | MAPE: 73.0179\n",
            "\n",
            "Fold 97\n",
            "  MAE: 55.4260 | RMSE: 65.0494 | MAPE: 78.7592\n",
            "\n",
            "Fold 98\n",
            "  MAE: 44.6431 | RMSE: 52.3159 | MAPE: 38.0620\n",
            "\n",
            "Fold 99\n",
            "  MAE: 50.9655 | RMSE: 63.7796 | MAPE: 30.8138\n",
            "\n",
            "Fold 100\n",
            "  MAE: 70.8625 | RMSE: 94.3404 | MAPE: 41.9084\n",
            "\n",
            "Fold 101\n",
            "  MAE: 81.3500 | RMSE: 105.2472 | MAPE: 79.2711\n",
            "\n",
            "Fold 102\n",
            "  MAE: 103.7034 | RMSE: 124.4812 | MAPE: 154.5016\n",
            "\n",
            "Fold 103\n",
            "  MAE: 121.2220 | RMSE: 136.8674 | MAPE: 170.2920\n",
            "\n",
            "Fold 104\n",
            "  MAE: 65.8585 | RMSE: 80.3224 | MAPE: 83.8915\n",
            "\n",
            "Fold 105\n",
            "  MAE: 90.4323 | RMSE: 123.6067 | MAPE: 56.4258\n",
            "\n",
            "Fold 106\n",
            "  MAE: 122.4621 | RMSE: 161.1450 | MAPE: 49.8531\n",
            "\n",
            "Fold 107\n",
            "  MAE: 84.9325 | RMSE: 112.4104 | MAPE: 57.3647\n",
            "\n",
            "Fold 108\n",
            "  MAE: 80.9752 | RMSE: 92.7165 | MAPE: 46.5651\n",
            "\n",
            "Fold 109\n",
            "  MAE: 70.9712 | RMSE: 81.9332 | MAPE: 40.8028\n",
            "\n",
            "Fold 110\n",
            "  MAE: 60.4438 | RMSE: 75.6344 | MAPE: 37.7767\n",
            "\n",
            "Fold 111\n",
            "  MAE: 42.2560 | RMSE: 55.8311 | MAPE: 45.8799\n",
            "\n",
            "Fold 112\n",
            "  MAE: 109.7563 | RMSE: 119.8466 | MAPE: 63.2818\n",
            "\n",
            "Fold 113\n",
            "  MAE: 64.7106 | RMSE: 82.1762 | MAPE: 34.0485\n",
            "\n",
            "Fold 114\n",
            "  MAE: 85.6599 | RMSE: 101.0290 | MAPE: 33.3476\n",
            "\n",
            "Fold 115\n",
            "  MAE: 140.3990 | RMSE: 159.9113 | MAPE: 47.4661\n",
            "\n",
            "Fold 116\n",
            "  MAE: 108.2166 | RMSE: 126.5794 | MAPE: 32.8677\n",
            "\n",
            "Fold 117\n",
            "  MAE: 167.6408 | RMSE: 205.0324 | MAPE: 38.0157\n",
            "\n",
            "Fold 118\n",
            "  MAE: 209.4237 | RMSE: 242.0853 | MAPE: 40.5803\n",
            "\n",
            "Fold 119\n",
            "  MAE: 159.3016 | RMSE: 207.1253 | MAPE: 47.1440\n",
            "\n",
            "Fold 120\n",
            "  MAE: 206.8094 | RMSE: 237.7457 | MAPE: 189.6036\n",
            "\n",
            "Fold 121\n",
            "  MAE: 240.2694 | RMSE: 266.3210 | MAPE: 280.3244\n",
            "\n",
            "Fold 122\n",
            "  MAE: 153.8790 | RMSE: 179.0250 | MAPE: 202.2418\n",
            "\n",
            "Fold 123\n",
            "  MAE: 130.5182 | RMSE: 142.6110 | MAPE: 423.4237\n",
            "\n",
            "Fold 124\n",
            "  MAE: 99.9646 | RMSE: 120.1581 | MAPE: 321.5279\n",
            "\n",
            "Fold 125\n",
            "  MAE: 165.9000 | RMSE: 189.2275 | MAPE: 101.2203\n",
            "\n",
            "Fold 126\n",
            "  MAE: 172.0672 | RMSE: 193.7786 | MAPE: 148.6098\n",
            "\n",
            "Fold 127\n",
            "  MAE: 154.8036 | RMSE: 172.4384 | MAPE: 345.7088\n",
            "\n",
            "Fold 128\n",
            "  MAE: 78.8918 | RMSE: 98.0642 | MAPE: 213.2451\n",
            "\n",
            "Fold 129\n",
            "  MAE: 39.1288 | RMSE: 50.0947 | MAPE: 65.1618\n",
            "\n",
            "Fold 130\n",
            "  MAE: 34.2929 | RMSE: 45.3580 | MAPE: 28.4272\n",
            "\n",
            "Fold 131\n",
            "  MAE: 25.0937 | RMSE: 30.4966 | MAPE: 22.9026\n",
            "\n",
            "Fold 132\n",
            "  MAE: 24.2169 | RMSE: 32.6231 | MAPE: 37.0322\n",
            "\n",
            "Fold 133\n",
            "  MAE: 26.9501 | RMSE: 35.5915 | MAPE: 51.1242\n",
            "\n",
            "Fold 134\n",
            "  MAE: 29.6035 | RMSE: 38.4102 | MAPE: 52.6204\n",
            "\n",
            "Fold 135\n",
            "  MAE: 20.5349 | RMSE: 26.8798 | MAPE: 25.5435\n",
            "\n",
            "Fold 136\n",
            "  MAE: 32.6296 | RMSE: 37.7617 | MAPE: 44.5306\n",
            "\n",
            "Fold 137\n",
            "  MAE: 49.8158 | RMSE: 55.4696 | MAPE: 95.1588\n",
            "\n",
            "Fold 138\n",
            "  MAE: 63.5910 | RMSE: 70.1364 | MAPE: 126.6092\n",
            "\n",
            "Fold 139\n",
            "  MAE: 26.0703 | RMSE: 35.5388 | MAPE: 45.7321\n",
            "\n",
            "Fold 140\n",
            "  MAE: 32.1618 | RMSE: 46.9313 | MAPE: 79.0686\n",
            "\n",
            "Fold 141\n",
            "  MAE: 42.9700 | RMSE: 54.0637 | MAPE: 169.5351\n",
            "\n",
            "Fold 142\n",
            "  MAE: 51.8712 | RMSE: 59.3556 | MAPE: 289.0238\n",
            "\n",
            "Fold 143\n",
            "  MAE: 51.6341 | RMSE: 62.4376 | MAPE: 271.8489\n",
            "\n",
            "Fold 144\n",
            "  MAE: 36.2571 | RMSE: 44.2694 | MAPE: 128.8260\n",
            "\n",
            "Fold 145\n",
            "  MAE: 40.2786 | RMSE: 48.1608 | MAPE: 557.1446\n",
            "\n",
            "Fold 146\n",
            "  MAE: 45.8415 | RMSE: 56.6653 | MAPE: 1050.0560\n",
            "\n",
            "Fold 147\n",
            "  MAE: 66.6967 | RMSE: 78.1028 | MAPE: 3041.4649\n",
            "\n",
            "Fold 148\n",
            "  MAE: 50.1584 | RMSE: 60.3041 | MAPE: 2447.2885\n",
            "\n",
            "Fold 149\n",
            "  MAE: 40.8245 | RMSE: 49.9109 | MAPE: 788.9555\n",
            "\n",
            "Fold 150\n",
            "  MAE: 32.1333 | RMSE: 40.3163 | MAPE: 55.7220\n",
            "\n",
            "Fold 151\n",
            "  MAE: 32.3419 | RMSE: 38.9811 | MAPE: 31.4471\n",
            "\n",
            "Fold 152\n",
            "  MAE: 32.0246 | RMSE: 40.2986 | MAPE: 98.5455\n",
            "\n",
            "Fold 153\n",
            "  MAE: 38.0548 | RMSE: 44.6477 | MAPE: 139.2652\n",
            "\n",
            "Fold 154\n",
            "  MAE: 37.4051 | RMSE: 49.0378 | MAPE: 113.8369\n",
            "\n",
            "Fold 155\n",
            "  MAE: 23.9818 | RMSE: 30.0084 | MAPE: 51.7947\n",
            "\n",
            "Fold 156\n",
            "  MAE: 23.9842 | RMSE: 27.7693 | MAPE: 54.7498\n",
            "\n",
            "Fold 157\n",
            "  MAE: 38.8897 | RMSE: 43.2305 | MAPE: 73.2237\n",
            "\n",
            "Fold 158\n",
            "  MAE: 27.4829 | RMSE: 31.4435 | MAPE: 51.0528\n",
            "\n",
            "Fold 159\n",
            "  MAE: 39.9627 | RMSE: 42.3996 | MAPE: 79.9025\n",
            "\n",
            "Fold 160\n",
            "  MAE: 27.2420 | RMSE: 31.1623 | MAPE: 80.0163\n",
            "\n",
            "Fold 161\n",
            "  MAE: 24.6070 | RMSE: 28.7102 | MAPE: 77.1073\n",
            "\n",
            "Fold 162\n",
            "  MAE: 26.8800 | RMSE: 34.7741 | MAPE: 89.0723\n",
            "\n",
            "Fold 163\n",
            "  MAE: 35.1433 | RMSE: 44.6014 | MAPE: 115.2158\n",
            "\n",
            "Fold 164\n",
            "  MAE: 32.0941 | RMSE: 40.9290 | MAPE: 112.7899\n",
            "\n",
            "Fold 165\n",
            "  MAE: 27.4329 | RMSE: 34.7546 | MAPE: 95.1133\n",
            "\n",
            "Fold 166\n",
            "  MAE: 27.8239 | RMSE: 35.7525 | MAPE: 103.3840\n",
            "\n",
            "Fold 167\n",
            "  MAE: 32.9273 | RMSE: 40.1293 | MAPE: 106.8141\n",
            "\n",
            "Fold 168\n",
            "  MAE: 22.8122 | RMSE: 30.2665 | MAPE: 63.5276\n",
            "\n",
            "Fold 169\n",
            "  MAE: 22.3814 | RMSE: 32.6366 | MAPE: 78.4960\n",
            "\n",
            "Fold 170\n",
            "  MAE: 29.1271 | RMSE: 34.2394 | MAPE: 71.4404\n",
            "\n",
            "Fold 171\n",
            "  MAE: 27.4603 | RMSE: 35.6063 | MAPE: 71.2082\n",
            "\n",
            "Fold 172\n",
            "  MAE: 27.9778 | RMSE: 35.5563 | MAPE: 74.3779\n",
            "\n",
            "Fold 173\n",
            "  MAE: 30.9818 | RMSE: 38.1008 | MAPE: 86.6988\n",
            "\n",
            "Fold 174\n",
            "  MAE: 29.6450 | RMSE: 36.0836 | MAPE: 77.5723\n",
            "\n",
            "Fold 175\n",
            "  MAE: 37.7431 | RMSE: 43.4797 | MAPE: 65.3221\n",
            "\n",
            "Fold 176\n",
            "  MAE: 48.9180 | RMSE: 60.6231 | MAPE: 178.8606\n",
            "\n",
            "Fold 177\n",
            "  MAE: 49.6023 | RMSE: 68.0612 | MAPE: 168.5383\n",
            "\n",
            "Fold 178\n",
            "  MAE: 56.9863 | RMSE: 82.7738 | MAPE: 131.3807\n",
            "\n",
            "Fold 179\n",
            "  MAE: 66.6718 | RMSE: 83.0394 | MAPE: 220.8629\n",
            "\n",
            "Fold 180\n",
            "  MAE: 50.5706 | RMSE: 62.5811 | MAPE: 158.2712\n",
            "\n",
            "Fold 181\n",
            "  MAE: 39.5145 | RMSE: 52.8487 | MAPE: 46.5495\n",
            "\n",
            "Fold 182\n",
            "  MAE: 32.3798 | RMSE: 40.1997 | MAPE: 34.1912\n",
            "\n",
            "Fold 183\n",
            "  MAE: 33.9840 | RMSE: 43.3691 | MAPE: 49.7174\n",
            "\n",
            "Fold 184\n",
            "  MAE: 28.5026 | RMSE: 35.7847 | MAPE: 48.8717\n",
            "\n",
            "Fold 185\n",
            "  MAE: 26.1034 | RMSE: 34.3266 | MAPE: 68.2703\n",
            "\n",
            "Fold 186\n",
            "  MAE: 29.1586 | RMSE: 36.5871 | MAPE: 74.4801\n",
            "\n",
            "Fold 187\n",
            "  MAE: 17.4569 | RMSE: 21.6807 | MAPE: 31.9904\n",
            "\n",
            "Fold 188\n",
            "  MAE: 14.4585 | RMSE: 18.0254 | MAPE: 22.3147\n",
            "\n",
            "Fold 189\n",
            "  MAE: 28.7521 | RMSE: 35.1477 | MAPE: 64.3918\n",
            "\n",
            "Fold 190\n",
            "  MAE: 40.2120 | RMSE: 44.6775 | MAPE: 93.6333\n",
            "\n",
            "Fold 191\n",
            "  MAE: 15.1278 | RMSE: 18.8454 | MAPE: 35.0907\n",
            "\n",
            "Fold 192\n",
            "  MAE: 19.0549 | RMSE: 24.9689 | MAPE: 42.8424\n",
            "\n",
            "Fold 193\n",
            "  MAE: 18.3574 | RMSE: 24.7569 | MAPE: 36.0286\n",
            "\n",
            " Final CNN CV Results:\n",
            "Average MAE:  36.5649\n",
            "Average RMSE: 44.1508\n",
            "Average MAPE: 115.6623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial 1-layer hyperparameter tunning"
      ],
      "metadata": {
        "id": "Rq5sU1V5R-gM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "num_filters_list = [32, 64, 128]\n",
        "kernel_sizes = [2, 3, 5]\n",
        "dropouts = [0.0, 0.1, 0.2]\n",
        "learning_rates = [0.001]\n",
        "batch_sizes = [32]\n",
        "\n",
        "cnn_grid = list(itertools.product(num_filters_list, kernel_sizes, dropouts, learning_rates, batch_sizes))\n",
        "print(f\"Total CNN configurations: {len(cnn_grid)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVIjRNYf606f",
        "outputId": "8e4bc7f1-4913-4350-fb65-39938ba3f157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Total CNN configurations: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_results = []\n",
        "\n",
        "for i, (num_filters, kernel_size, dropout, lr, batch_size) in enumerate(cnn_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(cnn_grid)}  filters={num_filters}, kernel={kernel_size}, dropout={dropout}, lr={lr}, batch={batch_size}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14, sequence_len=30)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_cnn_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            sequence_len=30,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            num_filters=num_filters,\n",
        "            kernel_size=kernel_size,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        if mae is not None:\n",
        "            mae_scores.append(mae)\n",
        "            rmse_scores.append(rmse)\n",
        "            mape_scores.append(mape)\n",
        "\n",
        "    if mae_scores:\n",
        "        cnn_results.append({\n",
        "            \"filters\": num_filters,\n",
        "            \"kernel\": kernel_size,\n",
        "            \"dropout\": dropout,\n",
        "            \"lr\": lr,\n",
        "            \"batch\": batch_size,\n",
        "            \"MAE\": np.mean(mae_scores),\n",
        "            \"RMSE\": np.mean(rmse_scores),\n",
        "            \"MAPE\": np.mean(mape_scores)\n",
        "        })\n",
        "\n",
        "        print(f\"Avg MAE: {np.mean(mae_scores):.2f} | RMSE: {np.mean(rmse_scores):.2f} | MAPE: {np.mean(mape_scores):.2f}\")\n",
        "    else:\n",
        "        print(\"Skipped config due to missing fold results\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoc9-VWS608d",
        "outputId": "555fac82-80ec-4083-cdfc-ee19c60566e4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Config 1/27  filters=32, kernel=2, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 35.10 | RMSE: 42.50 | MAPE: 109.17\n",
            "\n",
            " Config 2/27  filters=32, kernel=2, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 33.66 | RMSE: 40.99 | MAPE: 103.34\n",
            "\n",
            " Config 3/27  filters=32, kernel=2, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 34.17 | RMSE: 41.41 | MAPE: 106.29\n",
            "\n",
            " Config 4/27  filters=32, kernel=3, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 34.11 | RMSE: 41.54 | MAPE: 103.44\n",
            "\n",
            " Config 5/27  filters=32, kernel=3, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 34.67 | RMSE: 42.13 | MAPE: 102.34\n",
            "\n",
            " Config 6/27  filters=32, kernel=3, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 34.01 | RMSE: 41.49 | MAPE: 106.65\n",
            "\n",
            " Config 7/27  filters=32, kernel=5, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 35.52 | RMSE: 42.68 | MAPE: 106.52\n",
            "\n",
            " Config 8/27  filters=32, kernel=5, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 35.54 | RMSE: 42.78 | MAPE: 105.66\n",
            "\n",
            " Config 9/27  filters=32, kernel=5, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 34.48 | RMSE: 41.85 | MAPE: 108.40\n",
            "\n",
            " Config 10/27  filters=64, kernel=2, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 34.68 | RMSE: 42.21 | MAPE: 114.72\n",
            "\n",
            " Config 11/27  filters=64, kernel=2, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 36.41 | RMSE: 44.05 | MAPE: 112.10\n",
            "\n",
            " Config 12/27  filters=64, kernel=2, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 36.44 | RMSE: 44.05 | MAPE: 114.13\n",
            "\n",
            " Config 13/27  filters=64, kernel=3, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 36.58 | RMSE: 44.19 | MAPE: 119.64\n",
            "\n",
            " Config 14/27  filters=64, kernel=3, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 36.78 | RMSE: 44.05 | MAPE: 114.51\n",
            "\n",
            " Config 15/27  filters=64, kernel=3, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 36.21 | RMSE: 43.67 | MAPE: 113.96\n",
            "\n",
            " Config 16/27  filters=64, kernel=5, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 36.15 | RMSE: 43.50 | MAPE: 109.21\n",
            "\n",
            " Config 17/27  filters=64, kernel=5, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 35.88 | RMSE: 43.52 | MAPE: 113.44\n",
            "\n",
            " Config 18/27  filters=64, kernel=5, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 35.36 | RMSE: 42.74 | MAPE: 111.65\n",
            "\n",
            " Config 19/27  filters=128, kernel=2, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 37.36 | RMSE: 44.70 | MAPE: 122.72\n",
            "\n",
            " Config 20/27  filters=128, kernel=2, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 36.57 | RMSE: 44.27 | MAPE: 127.54\n",
            "\n",
            " Config 21/27  filters=128, kernel=2, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 37.45 | RMSE: 45.02 | MAPE: 122.22\n",
            "\n",
            " Config 22/27  filters=128, kernel=3, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 37.48 | RMSE: 44.89 | MAPE: 120.62\n",
            "\n",
            " Config 23/27  filters=128, kernel=3, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 37.00 | RMSE: 44.48 | MAPE: 120.09\n",
            "\n",
            " Config 24/27  filters=128, kernel=3, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 37.02 | RMSE: 44.53 | MAPE: 122.54\n",
            "\n",
            " Config 25/27  filters=128, kernel=5, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 35.93 | RMSE: 43.39 | MAPE: 119.43\n",
            "\n",
            " Config 26/27  filters=128, kernel=5, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 36.94 | RMSE: 44.53 | MAPE: 115.64\n",
            "\n",
            " Config 27/27  filters=128, kernel=5, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 36.83 | RMSE: 44.48 | MAPE: 115.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cnn_results = pd.DataFrame(cnn_results)\n",
        "df_cnn_results = df_cnn_results.sort_values(\"RMSE\").reset_index(drop=True)\n",
        "df_cnn_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "xucyFBeF60-j",
        "outputId": "69cfcc90-4293-455e-f4ab-e779eceb83b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    filters  kernel  dropout     lr  batch        MAE       RMSE        MAPE\n",
              "0        32       2      0.1  0.001     32  33.663449  40.987976  103.338586\n",
              "1        32       2      0.2  0.001     32  34.172721  41.412398  106.291753\n",
              "2        32       3      0.2  0.001     32  34.007828  41.491944  106.654283\n",
              "3        32       3      0.0  0.001     32  34.106265  41.537635  103.440944\n",
              "4        32       5      0.2  0.001     32  34.477077  41.847702  108.403090\n",
              "5        32       3      0.1  0.001     32  34.669038  42.128338  102.341143\n",
              "6        64       2      0.0  0.001     32  34.677114  42.208715  114.720858\n",
              "7        32       2      0.0  0.001     32  35.098770  42.499077  109.169390\n",
              "8        32       5      0.0  0.001     32  35.522639  42.679479  106.515662\n",
              "9        64       5      0.2  0.001     32  35.362657  42.741849  111.646540\n",
              "10       32       5      0.1  0.001     32  35.542114  42.784629  105.655749\n",
              "11      128       5      0.0  0.001     32  35.929120  43.389657  119.426138\n",
              "12       64       5      0.0  0.001     32  36.147504  43.497699  109.211355\n",
              "13       64       5      0.1  0.001     32  35.877282  43.518596  113.438227\n",
              "14       64       3      0.2  0.001     32  36.207367  43.665632  113.957045\n",
              "15       64       2      0.2  0.001     32  36.437182  44.046900  114.132023\n",
              "16       64       3      0.1  0.001     32  36.777503  44.053892  114.506243\n",
              "17       64       2      0.1  0.001     32  36.413354  44.054793  112.103584\n",
              "18       64       3      0.0  0.001     32  36.578873  44.186847  119.636768\n",
              "19      128       2      0.1  0.001     32  36.568024  44.269103  127.544872\n",
              "20      128       5      0.2  0.001     32  36.830420  44.475404  115.117513\n",
              "21      128       3      0.1  0.001     32  37.004079  44.482920  120.091106\n",
              "22      128       5      0.1  0.001     32  36.937162  44.528866  115.637258\n",
              "23      128       3      0.2  0.001     32  37.016393  44.531234  122.542126\n",
              "24      128       2      0.0  0.001     32  37.356270  44.700186  122.718609\n",
              "25      128       3      0.0  0.001     32  37.477009  44.888546  120.621622\n",
              "26      128       2      0.2  0.001     32  37.448144  45.023004  122.221838"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-96085398-ff49-44ee-b65d-bba6ba8741f6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filters</th>\n",
              "      <th>kernel</th>\n",
              "      <th>dropout</th>\n",
              "      <th>lr</th>\n",
              "      <th>batch</th>\n",
              "      <th>MAE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>33.663449</td>\n",
              "      <td>40.987976</td>\n",
              "      <td>103.338586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.172721</td>\n",
              "      <td>41.412398</td>\n",
              "      <td>106.291753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.007828</td>\n",
              "      <td>41.491944</td>\n",
              "      <td>106.654283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.106265</td>\n",
              "      <td>41.537635</td>\n",
              "      <td>103.440944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>32</td>\n",
              "      <td>5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.477077</td>\n",
              "      <td>41.847702</td>\n",
              "      <td>108.403090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.669038</td>\n",
              "      <td>42.128338</td>\n",
              "      <td>102.341143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.677114</td>\n",
              "      <td>42.208715</td>\n",
              "      <td>114.720858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>35.098770</td>\n",
              "      <td>42.499077</td>\n",
              "      <td>109.169390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>32</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>35.522639</td>\n",
              "      <td>42.679479</td>\n",
              "      <td>106.515662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>64</td>\n",
              "      <td>5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>35.362657</td>\n",
              "      <td>42.741849</td>\n",
              "      <td>111.646540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>32</td>\n",
              "      <td>5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>35.542114</td>\n",
              "      <td>42.784629</td>\n",
              "      <td>105.655749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>128</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>35.929120</td>\n",
              "      <td>43.389657</td>\n",
              "      <td>119.426138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>64</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>36.147504</td>\n",
              "      <td>43.497699</td>\n",
              "      <td>109.211355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>64</td>\n",
              "      <td>5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>35.877282</td>\n",
              "      <td>43.518596</td>\n",
              "      <td>113.438227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>36.207367</td>\n",
              "      <td>43.665632</td>\n",
              "      <td>113.957045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>36.437182</td>\n",
              "      <td>44.046900</td>\n",
              "      <td>114.132023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>36.777503</td>\n",
              "      <td>44.053892</td>\n",
              "      <td>114.506243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>36.413354</td>\n",
              "      <td>44.054793</td>\n",
              "      <td>112.103584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>36.578873</td>\n",
              "      <td>44.186847</td>\n",
              "      <td>119.636768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>36.568024</td>\n",
              "      <td>44.269103</td>\n",
              "      <td>127.544872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>128</td>\n",
              "      <td>5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>36.830420</td>\n",
              "      <td>44.475404</td>\n",
              "      <td>115.117513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>37.004079</td>\n",
              "      <td>44.482920</td>\n",
              "      <td>120.091106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>128</td>\n",
              "      <td>5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>36.937162</td>\n",
              "      <td>44.528866</td>\n",
              "      <td>115.637258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>37.016393</td>\n",
              "      <td>44.531234</td>\n",
              "      <td>122.542126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>37.356270</td>\n",
              "      <td>44.700186</td>\n",
              "      <td>122.718609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>128</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>37.477009</td>\n",
              "      <td>44.888546</td>\n",
              "      <td>120.621622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>37.448144</td>\n",
              "      <td>45.023004</td>\n",
              "      <td>122.221838</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-96085398-ff49-44ee-b65d-bba6ba8741f6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-96085398-ff49-44ee-b65d-bba6ba8741f6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-96085398-ff49-44ee-b65d-bba6ba8741f6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-81f8d9b7-a409-4019-94f8-d1c4d1c1c4f7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-81f8d9b7-a409-4019-94f8-d1c4d1c1c4f7')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-81f8d9b7-a409-4019-94f8-d1c4d1c1c4f7 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_6252c251-7998-4223-a47b-8797cb264aa2\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_cnn_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6252c251-7998-4223-a47b-8797cb264aa2 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_cnn_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_cnn_results",
              "summary": "{\n  \"name\": \"df_cnn_results\",\n  \"rows\": 27,\n  \"fields\": [\n    {\n      \"column\": \"filters\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 40,\n        \"min\": 32,\n        \"max\": 128,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          32,\n          64,\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"kernel\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 2,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          3,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08320502943378438,\n        \"min\": 0.0,\n        \"max\": 0.2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.1,\n          0.2,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.629132989485078e-19,\n        \"min\": 0.001,\n        \"max\": 0.001,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.156601480700045,\n        \"min\": 33.66344877010166,\n        \"max\": 37.47700850771345,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          35.52263874661609\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2162258761037466,\n        \"min\": 40.987975509077366,\n        \"max\": 45.023003611291784,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          42.67947856855319\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.859814900120842,\n        \"min\": 102.34114317659146,\n        \"max\": 127.54487209168605,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          106.51566160682721\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-layer CNN"
      ],
      "metadata": {
        "id": "6zwWZjhtXV41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TwoLayerCNNForecastNet(nn.Module):\n",
        "    def __init__(self, input_channels, seq_len,\n",
        "                 num_filters1=64, kernel_size1=3,\n",
        "                 num_filters2=32, kernel_size2=2,\n",
        "                 dropout=0.0, output_size=14):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_channels,\n",
        "                               out_channels=num_filters1,\n",
        "                               kernel_size=kernel_size1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(in_channels=num_filters1,\n",
        "                               out_channels=num_filters2,\n",
        "                               kernel_size=kernel_size2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.AdaptiveAvgPool1d(1)  # Global pooling\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(num_filters2, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # [batch, channels, seq_len]\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))  # [batch, channels, 1]\n",
        "        x = x.squeeze(2)  # [batch, channels]\n",
        "        x = self.dropout(x)\n",
        "        out = self.fc(x)  # [batch, output_size]\n",
        "        return out"
      ],
      "metadata": {
        "id": "h1R0idXl61C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "def train_one_cnn_fold(X_train, y_train, X_val, y_val,\n",
        "                       sequence_len=30, num_epochs=50, patience=5,\n",
        "                       batch_size=32, lr=0.001,\n",
        "                       num_filters1=64, kernel_size1=3,\n",
        "                       num_filters2=32, kernel_size2=2,\n",
        "                       dropout=0.1):\n",
        "\n",
        "    # 1. Scaling\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_val_scaled = scaler_X.transform(X_val)\n",
        "\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "    y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "    # 2. Create CNN sequences\n",
        "    X_train_seq, y_train_seq = create_lstm_input(X_train_scaled, y_train_scaled, sequence_len)\n",
        "    X_val_seq, y_val_seq = create_lstm_input(X_val_scaled, y_val_scaled, sequence_len)\n",
        "\n",
        "    if X_val_seq.shape[0] == 0:\n",
        "        print(\"Skipping fold: Not enough validation sequences.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # 3. Dataset & Dataloader\n",
        "    train_loader = DataLoader(LSTMForecastDataset(X_train_seq, y_train_seq), batch_size=batch_size, shuffle=False)\n",
        "    val_loader = DataLoader(LSTMForecastDataset(X_val_seq, y_val_seq), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # 4. Model setup\n",
        "    model = TwoLayerCNNForecastNet(\n",
        "        input_channels=X_train_seq.shape[2],\n",
        "        seq_len=sequence_len,\n",
        "        num_filters1=num_filters1,\n",
        "        kernel_size1=kernel_size1,\n",
        "        num_filters2=num_filters2,\n",
        "        kernel_size2=kernel_size2,\n",
        "        dropout=dropout,\n",
        "        output_size=y_train_seq.shape[1]\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    # 5. Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_preds = torch.cat([model(xb.to(device)) for xb, _ in val_loader])\n",
        "            val_targets = torch.cat([yb.to(device) for _, yb in val_loader])\n",
        "            val_loss = criterion(val_preds, val_targets)\n",
        "\n",
        "        if val_loss.item() < best_val_loss - 1e-4:\n",
        "            best_val_loss = val_loss.item()\n",
        "            best_model_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                break\n",
        "\n",
        "    # 6. Load best model\n",
        "    if best_model_state:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    # 7. Final evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32).to(device)\n",
        "        preds_scaled = model(X_val_tensor).cpu().numpy()\n",
        "        targets_scaled = y_val_seq\n",
        "\n",
        "    preds = scaler_y.inverse_transform(preds_scaled)\n",
        "    targets = scaler_y.inverse_transform(targets_scaled)\n",
        "\n",
        "    # 8. Metrics\n",
        "    mae = mean_absolute_error(targets.flatten(), preds.flatten())\n",
        "    rmse = np.sqrt(mean_squared_error(targets.flatten(), preds.flatten()))\n",
        "    mape = mean_absolute_percentage_error(targets.flatten(), preds.flatten()) * 100\n",
        "\n",
        "    return mae, rmse, mape"
      ],
      "metadata": {
        "id": "CLFdhFkx61J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_len = 30\n",
        "horizon = 14\n",
        "initial_train_size = 1095\n",
        "\n",
        "mae_scores = []\n",
        "rmse_scores = []\n",
        "mape_scores = []\n",
        "\n",
        "for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "    expanding_window_cv(X, y, initial_train_size=initial_train_size, horizon=horizon, step=horizon, sequence_len=sequence_len)\n",
        "):\n",
        "    print(f\"\\n Fold {fold+1}\")\n",
        "    mae, rmse, mape = train_one_cnn_fold(\n",
        "        X_tr, y_tr, X_val, y_val,\n",
        "        sequence_len=sequence_len,\n",
        "        num_epochs=50,\n",
        "        patience=5,\n",
        "        batch_size=32,\n",
        "        lr=0.001,\n",
        "        num_filters1=64,\n",
        "        kernel_size1=3,\n",
        "        num_filters2=32,\n",
        "        kernel_size2=2,\n",
        "        dropout=0.2\n",
        "    )\n",
        "\n",
        "    if mae is None:\n",
        "        continue  # skip invalid fold\n",
        "\n",
        "    print(f\" MAE: {mae:.4f} | RMSE: {rmse:.4f} | MAPE: {mape:.4f}\")\n",
        "    mae_scores.append(mae)\n",
        "    rmse_scores.append(rmse)\n",
        "    mape_scores.append(mape)\n",
        "\n",
        "# --- 7. Final CV Results ---\n",
        "print(\"\\n Final CNN CV Results:\")\n",
        "print(f\"Average MAE:  {np.mean(mae_scores):.4f}\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):.4f}\")\n",
        "print(f\"Average MAPE: {np.mean(mape_scores):.4f}\")"
      ],
      "metadata": {
        "id": "ZEgn9MkM61L_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "620ced5e-76df-45c8-9412-5f4734259187",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Fold 1\n",
            "   MAE: 10.8113 | RMSE: 12.4097 | MAPE: 40.24\n",
            "\n",
            " Fold 2\n",
            "   MAE: 11.1573 | RMSE: 12.5667 | MAPE: 62.22\n",
            "\n",
            " Fold 3\n",
            "   MAE: 7.4514 | RMSE: 9.2816 | MAPE: 50.17\n",
            "\n",
            " Fold 4\n",
            "   MAE: 5.0457 | RMSE: 6.2477 | MAPE: 14.15\n",
            "\n",
            " Fold 5\n",
            "   MAE: 5.4792 | RMSE: 7.5216 | MAPE: 20.43\n",
            "\n",
            " Fold 6\n",
            "   MAE: 6.3125 | RMSE: 8.1016 | MAPE: 24.61\n",
            "\n",
            " Fold 7\n",
            "   MAE: 9.3679 | RMSE: 10.6209 | MAPE: 25.60\n",
            "\n",
            " Fold 8\n",
            "   MAE: 10.9291 | RMSE: 11.8024 | MAPE: 24.61\n",
            "\n",
            " Fold 9\n",
            "   MAE: 10.2448 | RMSE: 11.4666 | MAPE: 22.79\n",
            "\n",
            " Fold 10\n",
            "   MAE: 10.4901 | RMSE: 11.3790 | MAPE: 21.67\n",
            "\n",
            " Fold 11\n",
            "   MAE: 7.8011 | RMSE: 8.4752 | MAPE: 14.54\n",
            "\n",
            " Fold 12\n",
            "   MAE: 6.8545 | RMSE: 8.2418 | MAPE: 12.25\n",
            "\n",
            " Fold 13\n",
            "   MAE: 7.0330 | RMSE: 8.5626 | MAPE: 12.01\n",
            "\n",
            " Fold 14\n",
            "   MAE: 7.2098 | RMSE: 8.6800 | MAPE: 11.99\n",
            "\n",
            " Fold 15\n",
            "   MAE: 14.9396 | RMSE: 16.9888 | MAPE: 28.73\n",
            "\n",
            " Fold 16\n",
            "   MAE: 12.3277 | RMSE: 15.4036 | MAPE: 37.78\n",
            "\n",
            " Fold 17\n",
            "   MAE: 12.3386 | RMSE: 14.8800 | MAPE: 34.21\n",
            "\n",
            " Fold 18\n",
            "   MAE: 9.5719 | RMSE: 12.3540 | MAPE: 21.72\n",
            "\n",
            " Fold 19\n",
            "   MAE: 12.3152 | RMSE: 15.0526 | MAPE: 22.61\n",
            "\n",
            " Fold 20\n",
            "   MAE: 13.4415 | RMSE: 16.8309 | MAPE: 25.23\n",
            "\n",
            " Fold 21\n",
            "   MAE: 13.4031 | RMSE: 16.6120 | MAPE: 30.78\n",
            "\n",
            " Fold 22\n",
            "   MAE: 12.1580 | RMSE: 14.6759 | MAPE: 44.79\n",
            "\n",
            " Fold 23\n",
            "   MAE: 11.9606 | RMSE: 14.7216 | MAPE: 46.98\n",
            "\n",
            " Fold 24\n",
            "   MAE: 9.8552 | RMSE: 13.5555 | MAPE: 31.54\n",
            "\n",
            " Fold 25\n",
            "   MAE: 11.3222 | RMSE: 14.4193 | MAPE: 33.41\n",
            "\n",
            " Fold 26\n",
            "   MAE: 7.1650 | RMSE: 10.7311 | MAPE: 32.84\n",
            "\n",
            " Fold 27\n",
            "   MAE: 7.4554 | RMSE: 11.9334 | MAPE: 56.34\n",
            "\n",
            " Fold 28\n",
            "   MAE: 8.7861 | RMSE: 13.5859 | MAPE: 90.75\n",
            "\n",
            " Fold 29\n",
            "   MAE: 6.1958 | RMSE: 9.2854 | MAPE: 57.63\n",
            "\n",
            " Fold 30\n",
            "   MAE: 5.5606 | RMSE: 7.3114 | MAPE: 18.54\n",
            "\n",
            " Fold 31\n",
            "   MAE: 7.6752 | RMSE: 10.2736 | MAPE: 28.66\n",
            "\n",
            " Fold 32\n",
            "   MAE: 5.7289 | RMSE: 8.8236 | MAPE: 22.10\n",
            "\n",
            " Fold 33\n",
            "   MAE: 6.3734 | RMSE: 9.5221 | MAPE: 28.40\n",
            "\n",
            " Fold 34\n",
            "   MAE: 8.4741 | RMSE: 11.6564 | MAPE: 44.09\n",
            "\n",
            " Fold 35\n",
            "   MAE: 7.0566 | RMSE: 9.7294 | MAPE: 35.57\n",
            "\n",
            " Fold 36\n",
            "   MAE: 5.8816 | RMSE: 7.1524 | MAPE: 20.00\n",
            "\n",
            " Fold 37\n",
            "   MAE: 6.4044 | RMSE: 7.6637 | MAPE: 16.32\n",
            "\n",
            " Fold 38\n",
            "   MAE: 7.4138 | RMSE: 9.4861 | MAPE: 23.38\n",
            "\n",
            " Fold 39\n",
            "   MAE: 5.9985 | RMSE: 8.7699 | MAPE: 24.25\n",
            "\n",
            " Fold 40\n",
            "   MAE: 5.3883 | RMSE: 7.1745 | MAPE: 18.35\n",
            "\n",
            " Fold 41\n",
            "   MAE: 6.1635 | RMSE: 7.4868 | MAPE: 19.96\n",
            "\n",
            " Fold 42\n",
            "   MAE: 5.9689 | RMSE: 7.5156 | MAPE: 19.87\n",
            "\n",
            " Fold 43\n",
            "   MAE: 6.0874 | RMSE: 7.4166 | MAPE: 18.74\n",
            "\n",
            " Fold 44\n",
            "   MAE: 7.2869 | RMSE: 8.6421 | MAPE: 18.94\n",
            "\n",
            " Fold 45\n",
            "   MAE: 8.0658 | RMSE: 9.3892 | MAPE: 18.69\n",
            "\n",
            " Fold 46\n",
            "   MAE: 8.7514 | RMSE: 10.4070 | MAPE: 32.21\n",
            "\n",
            " Fold 47\n",
            "   MAE: 7.7180 | RMSE: 10.8748 | MAPE: 65.85\n",
            "\n",
            " Fold 48\n",
            "   MAE: 6.8182 | RMSE: 10.0105 | MAPE: 60.71\n",
            "\n",
            " Fold 49\n",
            "   MAE: 6.1611 | RMSE: 7.4548 | MAPE: 23.46\n",
            "\n",
            " Fold 50\n",
            "   MAE: 8.2424 | RMSE: 9.3542 | MAPE: 39.78\n",
            "\n",
            " Fold 51\n",
            "   MAE: 10.6159 | RMSE: 12.1376 | MAPE: 87.55\n",
            "\n",
            " Fold 52\n",
            "   MAE: 12.4620 | RMSE: 14.5113 | MAPE: 202.21\n",
            "\n",
            " Fold 53\n",
            "   MAE: 12.1281 | RMSE: 14.3204 | MAPE: 206.94\n",
            "\n",
            " Fold 54\n",
            "   MAE: 10.6772 | RMSE: 12.3008 | MAPE: 117.09\n",
            "\n",
            " Fold 55\n",
            "   MAE: 10.6301 | RMSE: 12.4796 | MAPE: 129.12\n",
            "\n",
            " Fold 56\n",
            "   MAE: 10.6055 | RMSE: 12.8954 | MAPE: 138.50\n",
            "\n",
            " Fold 57\n",
            "   MAE: 8.6015 | RMSE: 10.9735 | MAPE: 95.32\n",
            "\n",
            " Fold 58\n",
            "   MAE: 7.0960 | RMSE: 8.6959 | MAPE: 69.91\n",
            "\n",
            " Fold 59\n",
            "   MAE: 7.6783 | RMSE: 9.7066 | MAPE: 94.19\n",
            "\n",
            " Fold 60\n",
            "   MAE: 7.9991 | RMSE: 9.9384 | MAPE: 71.60\n",
            "\n",
            " Fold 61\n",
            "   MAE: 9.1373 | RMSE: 10.9617 | MAPE: 57.02\n",
            "\n",
            " Fold 62\n",
            "   MAE: 11.1175 | RMSE: 14.0237 | MAPE: 128.67\n",
            "\n",
            " Fold 63\n",
            "   MAE: 10.1279 | RMSE: 13.5833 | MAPE: 149.95\n",
            "\n",
            " Fold 64\n",
            "   MAE: 7.3321 | RMSE: 9.8224 | MAPE: 81.75\n",
            "\n",
            " Fold 65\n",
            "   MAE: 7.9384 | RMSE: 9.8207 | MAPE: 55.71\n",
            "\n",
            " Fold 66\n",
            "   MAE: 7.8472 | RMSE: 9.7762 | MAPE: 24.68\n",
            "\n",
            " Fold 67\n",
            "   MAE: 10.2460 | RMSE: 13.2744 | MAPE: 38.70\n",
            "\n",
            " Fold 68\n",
            "   MAE: 13.9501 | RMSE: 17.2577 | MAPE: 115.65\n",
            "\n",
            " Fold 69\n",
            "   MAE: 13.6160 | RMSE: 16.5044 | MAPE: 125.96\n",
            "\n",
            " Fold 70\n",
            "   MAE: 12.6585 | RMSE: 15.2161 | MAPE: 129.34\n",
            "\n",
            " Fold 71\n",
            "   MAE: 14.5687 | RMSE: 17.0603 | MAPE: 327.97\n",
            "\n",
            " Fold 72\n",
            "   MAE: 17.7718 | RMSE: 20.2828 | MAPE: 496.78\n",
            "\n",
            " Fold 73\n",
            "   MAE: 18.9360 | RMSE: 22.1512 | MAPE: 305.39\n",
            "\n",
            " Fold 74\n",
            "   MAE: 15.5317 | RMSE: 19.4502 | MAPE: 49.55\n",
            "\n",
            " Fold 75\n",
            "   MAE: 15.0210 | RMSE: 18.7859 | MAPE: 48.79\n",
            "\n",
            " Fold 76\n",
            "   MAE: 17.1640 | RMSE: 20.7356 | MAPE: 36.18\n",
            "\n",
            " Fold 77\n",
            "   MAE: 16.5192 | RMSE: 19.2903 | MAPE: 31.22\n",
            "\n",
            " Fold 78\n",
            "   MAE: 12.3856 | RMSE: 15.0458 | MAPE: 25.22\n",
            "\n",
            " Fold 79\n",
            "   MAE: 10.6403 | RMSE: 13.0352 | MAPE: 25.15\n",
            "\n",
            " Fold 80\n",
            "   MAE: 11.9461 | RMSE: 13.8659 | MAPE: 33.21\n",
            "\n",
            " Fold 81\n",
            "   MAE: 13.6966 | RMSE: 16.3062 | MAPE: 48.83\n",
            "\n",
            " Fold 82\n",
            "   MAE: 14.5347 | RMSE: 17.9885 | MAPE: 48.00\n",
            "\n",
            " Fold 83\n",
            "   MAE: 15.7127 | RMSE: 18.6897 | MAPE: 38.44\n",
            "\n",
            " Fold 84\n",
            "   MAE: 17.3997 | RMSE: 19.7465 | MAPE: 42.45\n",
            "\n",
            " Fold 85\n",
            "   MAE: 17.9544 | RMSE: 20.0506 | MAPE: 48.80\n",
            "\n",
            " Fold 86\n",
            "   MAE: 21.0756 | RMSE: 22.8246 | MAPE: 41.61\n",
            "\n",
            " Fold 87\n",
            "   MAE: 24.9744 | RMSE: 26.7437 | MAPE: 34.85\n",
            "\n",
            " Fold 88\n",
            "   MAE: 30.5143 | RMSE: 32.0422 | MAPE: 36.37\n",
            "\n",
            " Fold 89\n",
            "   MAE: 27.6740 | RMSE: 29.9023 | MAPE: 32.96\n",
            "\n",
            " Fold 90\n",
            "   MAE: 19.6438 | RMSE: 22.5137 | MAPE: 31.60\n",
            "\n",
            " Fold 91\n",
            "   MAE: 17.2230 | RMSE: 23.2547 | MAPE: 35.77\n",
            "\n",
            " Fold 92\n",
            "   MAE: 19.5399 | RMSE: 24.5076 | MAPE: 23.30\n",
            "\n",
            " Fold 93\n",
            "   MAE: 46.1182 | RMSE: 51.0583 | MAPE: 36.66\n",
            "\n",
            " Fold 94\n",
            "   MAE: 47.5920 | RMSE: 56.7734 | MAPE: 35.57\n",
            "\n",
            " Fold 95\n",
            "   MAE: 43.3673 | RMSE: 56.9413 | MAPE: 35.77\n",
            "\n",
            " Fold 96\n",
            "   MAE: 47.2669 | RMSE: 57.2870 | MAPE: 59.64\n",
            "\n",
            " Fold 97\n",
            "   MAE: 56.1157 | RMSE: 66.3618 | MAPE: 81.77\n",
            "\n",
            " Fold 98\n",
            "   MAE: 46.1830 | RMSE: 56.1005 | MAPE: 48.09\n",
            "\n",
            " Fold 99\n",
            "   MAE: 55.3471 | RMSE: 69.8186 | MAPE: 31.11\n",
            "\n",
            " Fold 100\n",
            "   MAE: 80.7531 | RMSE: 107.2727 | MAPE: 40.03\n",
            "\n",
            " Fold 101\n",
            "   MAE: 81.6813 | RMSE: 105.6418 | MAPE: 78.28\n",
            "\n",
            " Fold 102\n",
            "   MAE: 70.4841 | RMSE: 83.9284 | MAPE: 110.38\n",
            "\n",
            " Fold 103\n",
            "   MAE: 61.8918 | RMSE: 76.0397 | MAPE: 97.89\n",
            "\n",
            " Fold 104\n",
            "   MAE: 56.2066 | RMSE: 66.0667 | MAPE: 76.24\n",
            "\n",
            " Fold 105\n",
            "   MAE: 87.9393 | RMSE: 130.1216 | MAPE: 45.66\n",
            "\n",
            " Fold 106\n",
            "   MAE: 131.4872 | RMSE: 166.7731 | MAPE: 51.61\n",
            "\n",
            " Fold 107\n",
            "   MAE: 90.4929 | RMSE: 122.2097 | MAPE: 46.49\n",
            "\n",
            " Fold 108\n",
            "   MAE: 51.4252 | RMSE: 63.7905 | MAPE: 48.59\n",
            "\n",
            " Fold 109\n",
            "   MAE: 62.7326 | RMSE: 70.2767 | MAPE: 39.32\n",
            "\n",
            " Fold 110\n",
            "   MAE: 68.4567 | RMSE: 75.4073 | MAPE: 34.04\n",
            "\n",
            " Fold 111\n",
            "   MAE: 55.1639 | RMSE: 63.6570 | MAPE: 43.78\n",
            "\n",
            " Fold 112\n",
            "   MAE: 46.6369 | RMSE: 56.6655 | MAPE: 42.21\n",
            "\n",
            " Fold 113\n",
            "   MAE: 78.0453 | RMSE: 95.9147 | MAPE: 36.80\n",
            "\n",
            " Fold 114\n",
            "   MAE: 96.9743 | RMSE: 112.6839 | MAPE: 37.16\n",
            "\n",
            " Fold 115\n",
            "   MAE: 114.1887 | RMSE: 131.6156 | MAPE: 39.69\n",
            "\n",
            " Fold 116\n",
            "   MAE: 122.1211 | RMSE: 139.4430 | MAPE: 36.48\n",
            "\n",
            " Fold 117\n",
            "   MAE: 133.7706 | RMSE: 166.0966 | MAPE: 29.74\n",
            "\n",
            " Fold 118\n",
            "   MAE: 200.4893 | RMSE: 232.3781 | MAPE: 38.46\n",
            "\n",
            " Fold 119\n",
            "   MAE: 153.8456 | RMSE: 201.5035 | MAPE: 53.34\n",
            "\n",
            " Fold 120\n",
            "   MAE: 169.8690 | RMSE: 211.0864 | MAPE: 169.14\n",
            "\n",
            " Fold 121\n",
            "   MAE: 192.7197 | RMSE: 220.0681 | MAPE: 237.93\n",
            "\n",
            " Fold 122\n",
            "   MAE: 184.9868 | RMSE: 203.4340 | MAPE: 230.69\n",
            "\n",
            " Fold 123\n",
            "   MAE: 97.8396 | RMSE: 106.7020 | MAPE: 349.63\n",
            "\n",
            " Fold 124\n",
            "   MAE: 86.9820 | RMSE: 105.7614 | MAPE: 331.98\n",
            "\n",
            " Fold 125\n",
            "   MAE: 142.4883 | RMSE: 166.9707 | MAPE: 104.48\n",
            "\n",
            " Fold 126\n",
            "   MAE: 159.8370 | RMSE: 187.3275 | MAPE: 118.11\n",
            "\n",
            " Fold 127\n",
            "   MAE: 128.6083 | RMSE: 150.7879 | MAPE: 265.77\n",
            "\n",
            " Fold 128\n",
            "   MAE: 101.8783 | RMSE: 121.3381 | MAPE: 267.64\n",
            "\n",
            " Fold 129\n",
            "   MAE: 51.0992 | RMSE: 69.5551 | MAPE: 88.08\n",
            "\n",
            " Fold 130\n",
            "   MAE: 30.1880 | RMSE: 37.4211 | MAPE: 25.62\n",
            "\n",
            " Fold 131\n",
            "   MAE: 30.2781 | RMSE: 37.9941 | MAPE: 33.92\n",
            "\n",
            " Fold 132\n",
            "   MAE: 33.6025 | RMSE: 42.5054 | MAPE: 49.99\n",
            "\n",
            " Fold 133\n",
            "   MAE: 35.6392 | RMSE: 44.7907 | MAPE: 65.86\n",
            "\n",
            " Fold 134\n",
            "   MAE: 34.0793 | RMSE: 43.0260 | MAPE: 60.57\n",
            "\n",
            " Fold 135\n",
            "   MAE: 30.9508 | RMSE: 36.8299 | MAPE: 42.35\n",
            "\n",
            " Fold 136\n",
            "   MAE: 24.1575 | RMSE: 30.0636 | MAPE: 34.55\n",
            "\n",
            " Fold 137\n",
            "   MAE: 54.7010 | RMSE: 61.0154 | MAPE: 105.08\n",
            "\n",
            " Fold 138\n",
            "   MAE: 42.3137 | RMSE: 49.9099 | MAPE: 90.47\n",
            "\n",
            " Fold 139\n",
            "   MAE: 33.5914 | RMSE: 42.6741 | MAPE: 56.01\n",
            "\n",
            " Fold 140\n",
            "   MAE: 30.9891 | RMSE: 42.6669 | MAPE: 71.59\n",
            "\n",
            " Fold 141\n",
            "   MAE: 31.0394 | RMSE: 43.8410 | MAPE: 149.50\n",
            "\n",
            " Fold 142\n",
            "   MAE: 39.4047 | RMSE: 48.4355 | MAPE: 254.52\n",
            "\n",
            " Fold 143\n",
            "   MAE: 29.4686 | RMSE: 36.8443 | MAPE: 157.21\n",
            "\n",
            " Fold 144\n",
            "   MAE: 29.7888 | RMSE: 35.0907 | MAPE: 76.20\n",
            "\n",
            " Fold 145\n",
            "   MAE: 24.1148 | RMSE: 34.5086 | MAPE: 461.20\n",
            "\n",
            " Fold 146\n",
            "   MAE: 37.6651 | RMSE: 50.2978 | MAPE: 932.79\n",
            "\n",
            " Fold 147\n",
            "   MAE: 45.4086 | RMSE: 54.4028 | MAPE: 2314.32\n",
            "\n",
            " Fold 148\n",
            "   MAE: 40.2230 | RMSE: 49.2766 | MAPE: 2179.59\n",
            "\n",
            " Fold 149\n",
            "   MAE: 26.1276 | RMSE: 31.0665 | MAPE: 497.66\n",
            "\n",
            " Fold 150\n",
            "   MAE: 22.8390 | RMSE: 27.8079 | MAPE: 34.38\n",
            "\n",
            " Fold 151\n",
            "   MAE: 22.2010 | RMSE: 28.9380 | MAPE: 29.59\n",
            "\n",
            " Fold 152\n",
            "   MAE: 39.9258 | RMSE: 48.4898 | MAPE: 120.65\n",
            "\n",
            " Fold 153\n",
            "   MAE: 42.9733 | RMSE: 51.6935 | MAPE: 161.44\n",
            "\n",
            " Fold 154\n",
            "   MAE: 30.9418 | RMSE: 41.9761 | MAPE: 97.45\n",
            "\n",
            " Fold 155\n",
            "   MAE: 20.0473 | RMSE: 23.9953 | MAPE: 40.53\n",
            "\n",
            " Fold 156\n",
            "   MAE: 26.1598 | RMSE: 30.0681 | MAPE: 59.08\n",
            "\n",
            " Fold 157\n",
            "   MAE: 29.1495 | RMSE: 31.8603 | MAPE: 56.44\n",
            "\n",
            " Fold 158\n",
            "   MAE: 26.1142 | RMSE: 29.2729 | MAPE: 49.71\n",
            "\n",
            " Fold 159\n",
            "   MAE: 24.9798 | RMSE: 29.0521 | MAPE: 54.10\n",
            "\n",
            " Fold 160\n",
            "   MAE: 31.4223 | RMSE: 36.0630 | MAPE: 94.97\n",
            "\n",
            " Fold 161\n",
            "   MAE: 30.0680 | RMSE: 34.7962 | MAPE: 93.40\n",
            "\n",
            " Fold 162\n",
            "   MAE: 24.5162 | RMSE: 28.8376 | MAPE: 72.45\n",
            "\n",
            " Fold 163\n",
            "   MAE: 23.4485 | RMSE: 27.7255 | MAPE: 69.77\n",
            "\n",
            " Fold 164\n",
            "   MAE: 21.6194 | RMSE: 26.2238 | MAPE: 73.52\n",
            "\n",
            " Fold 165\n",
            "   MAE: 20.3739 | RMSE: 24.9382 | MAPE: 67.32\n",
            "\n",
            " Fold 166\n",
            "   MAE: 21.7621 | RMSE: 27.1858 | MAPE: 79.95\n",
            "\n",
            " Fold 167\n",
            "   MAE: 19.7531 | RMSE: 25.4707 | MAPE: 68.85\n",
            "\n",
            " Fold 168\n",
            "   MAE: 19.4485 | RMSE: 23.1700 | MAPE: 45.54\n",
            "\n",
            " Fold 169\n",
            "   MAE: 23.5818 | RMSE: 27.9621 | MAPE: 61.85\n",
            "\n",
            " Fold 170\n",
            "   MAE: 31.2231 | RMSE: 34.9784 | MAPE: 69.62\n",
            "\n",
            " Fold 171\n",
            "   MAE: 27.5817 | RMSE: 33.2918 | MAPE: 50.31\n",
            "\n",
            " Fold 172\n",
            "   MAE: 22.8759 | RMSE: 28.8667 | MAPE: 47.54\n",
            "\n",
            " Fold 173\n",
            "   MAE: 22.0530 | RMSE: 27.2266 | MAPE: 57.69\n",
            "\n",
            " Fold 174\n",
            "   MAE: 28.5425 | RMSE: 33.6736 | MAPE: 62.16\n",
            "\n",
            " Fold 175\n",
            "   MAE: 36.5816 | RMSE: 43.2586 | MAPE: 69.04\n",
            "\n",
            " Fold 176\n",
            "   MAE: 43.7508 | RMSE: 54.6440 | MAPE: 157.28\n",
            "\n",
            " Fold 177\n",
            "   MAE: 47.8043 | RMSE: 66.0460 | MAPE: 167.76\n",
            "\n",
            " Fold 178\n",
            "   MAE: 74.6257 | RMSE: 89.4852 | MAPE: 196.98\n",
            "\n",
            " Fold 179\n",
            "   MAE: 65.6505 | RMSE: 81.3594 | MAPE: 217.33\n",
            "\n",
            " Fold 180\n",
            "   MAE: 47.5618 | RMSE: 60.5488 | MAPE: 152.96\n",
            "\n",
            " Fold 181\n",
            "   MAE: 37.0294 | RMSE: 48.7181 | MAPE: 53.81\n",
            "\n",
            " Fold 182\n",
            "   MAE: 37.5983 | RMSE: 47.7611 | MAPE: 33.39\n",
            "\n",
            " Fold 183\n",
            "   MAE: 33.7178 | RMSE: 39.1992 | MAPE: 38.88\n",
            "\n",
            " Fold 184\n",
            "   MAE: 31.7734 | RMSE: 37.8073 | MAPE: 41.97\n",
            "\n",
            " Fold 185\n",
            "   MAE: 24.1116 | RMSE: 32.9519 | MAPE: 67.14\n",
            "\n",
            " Fold 186\n",
            "   MAE: 24.3062 | RMSE: 32.7599 | MAPE: 68.70\n",
            "\n",
            " Fold 187\n",
            "   MAE: 18.8873 | RMSE: 22.2169 | MAPE: 31.90\n",
            "\n",
            " Fold 188\n",
            "   MAE: 16.4600 | RMSE: 20.2951 | MAPE: 26.80\n",
            "\n",
            " Fold 189\n",
            "   MAE: 19.0828 | RMSE: 24.2263 | MAPE: 44.92\n",
            "\n",
            " Fold 190\n",
            "   MAE: 20.3087 | RMSE: 25.6768 | MAPE: 54.43\n",
            "\n",
            " Fold 191\n",
            "   MAE: 20.3059 | RMSE: 25.7312 | MAPE: 51.69\n",
            "\n",
            " Fold 192\n",
            "   MAE: 21.1288 | RMSE: 26.8684 | MAPE: 47.13\n",
            "\n",
            " Fold 193\n",
            "   MAE: 19.0256 | RMSE: 24.1715 | MAPE: 33.57\n",
            "\n",
            " Final CNN CV Results:\n",
            "Average MAE:  33.6563\n",
            "Average RMSE: 40.9315\n",
            "Average MAPE: 103.4405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tunning"
      ],
      "metadata": {
        "id": "G9ogLaJxYxSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Define hyperparameter options\n",
        "num_filters1_list = [64, 128]\n",
        "num_filters2_list = [32, 64]\n",
        "kernel_size1_list = [2, 3]\n",
        "kernel_size2_list = [2, 3]\n",
        "dropouts = [0.0, 0.1, 0.2]\n",
        "learning_rates = [0.001]\n",
        "batch_sizes = [32]\n",
        "\n",
        "# Create full grid of combinations\n",
        "param_grid = list(itertools.product(\n",
        "    num_filters1_list,\n",
        "    num_filters2_list,\n",
        "    kernel_size1_list,\n",
        "    kernel_size2_list,\n",
        "    dropouts,\n",
        "    learning_rates,\n",
        "    batch_sizes\n",
        "))\n",
        "\n",
        "print(f\"Total combinations: {len(param_grid)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juP-gz2tXZ7z",
        "outputId": "48a036fa-c094-4ae5-b666-ddd915853f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Total combinations: 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "sequence_len = 30\n",
        "horizon = 14\n",
        "initial_train_size = 1095\n",
        "step = horizon\n",
        "\n",
        "for i, (nf1, nf2, ks1, ks2, dropout, lr, batch_size) in enumerate(param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(param_grid)}  \"\n",
        "          f\"nf1={nf1}, nf2={nf2}, ks1={ks1}, ks2={ks2}, dropout={dropout}, lr={lr}, batch={batch_size}\")\n",
        "\n",
        "    mae_scores, rmse_scores, mape_scores = [], [], []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y, initial_train_size=initial_train_size,\n",
        "                            horizon=horizon, step=step, sequence_len=sequence_len)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_cnn_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            sequence_len=sequence_len,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            num_filters1=nf1,\n",
        "            num_filters2=nf2,\n",
        "            kernel_size1=ks1,\n",
        "            kernel_size2=ks2,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue\n",
        "\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    # Average fold results\n",
        "    avg_mae = np.mean(mae_scores)\n",
        "    avg_rmse = np.mean(rmse_scores)\n",
        "    avg_mape = np.mean(mape_scores)\n",
        "\n",
        "    print(f\"Avg MAE: {avg_mae:.4f} | RMSE: {avg_rmse:.4f} | MAPE: {avg_mape:.4f}\")\n",
        "\n",
        "    results.append({\n",
        "        \"filters1\": nf1,\n",
        "        \"filters2\": nf2,\n",
        "        \"kernel1\": ks1,\n",
        "        \"kernel2\": ks2,\n",
        "        \"dropout\": dropout,\n",
        "        \"lr\": lr,\n",
        "        \"batch\": batch_size,\n",
        "        \"MAE\": avg_mae,\n",
        "        \"RMSE\": avg_rmse,\n",
        "        \"MAPE\": avg_mape\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDD5h0glXZ-1",
        "outputId": "f85c956a-3a3d-4129-b8f0-84b52fdfcc9d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Config 1/48  nf1=64, nf2=32, ks1=2, ks2=2, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 33.9775 | RMSE: 41.3638 | MAPE: 106.6675\n",
            "\n",
            " Config 2/48  nf1=64, nf2=32, ks1=2, ks2=2, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 33.8449 | RMSE: 41.2838 | MAPE: 107.7709\n",
            "\n",
            " Config 3/48  nf1=64, nf2=32, ks1=2, ks2=2, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 33.6322 | RMSE: 40.9703 | MAPE: 107.1844\n",
            "\n",
            " Config 4/48  nf1=64, nf2=32, ks1=2, ks2=3, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 34.3882 | RMSE: 41.8156 | MAPE: 109.5020\n",
            "\n",
            " Config 5/48  nf1=64, nf2=32, ks1=2, ks2=3, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 33.7267 | RMSE: 41.1032 | MAPE: 107.4888\n",
            "\n",
            " Config 6/48  nf1=64, nf2=32, ks1=2, ks2=3, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 33.7496 | RMSE: 41.0835 | MAPE: 105.9381\n",
            "\n",
            " Config 7/48  nf1=64, nf2=32, ks1=3, ks2=2, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 34.3031 | RMSE: 41.6557 | MAPE: 108.1645\n",
            "\n",
            " Config 8/48  nf1=64, nf2=32, ks1=3, ks2=2, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 33.6298 | RMSE: 41.0275 | MAPE: 105.0264\n",
            "\n",
            " Config 9/48  nf1=64, nf2=32, ks1=3, ks2=2, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 33.7592 | RMSE: 41.0284 | MAPE: 103.6710\n",
            "\n",
            " Config 10/48  nf1=64, nf2=32, ks1=3, ks2=3, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 34.0938 | RMSE: 41.5444 | MAPE: 107.2286\n",
            "\n",
            " Config 11/48  nf1=64, nf2=32, ks1=3, ks2=3, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 34.4584 | RMSE: 41.7389 | MAPE: 104.3593\n",
            "\n",
            " Config 12/48  nf1=64, nf2=32, ks1=3, ks2=3, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 33.4589 | RMSE: 40.7069 | MAPE: 102.3904\n",
            "\n",
            " Config 13/48  nf1=64, nf2=64, ks1=2, ks2=2, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 34.4452 | RMSE: 41.7937 | MAPE: 105.1185\n",
            "\n",
            " Config 14/48  nf1=64, nf2=64, ks1=2, ks2=2, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 34.0485 | RMSE: 41.4226 | MAPE: 108.5052\n",
            "\n",
            " Config 15/48  nf1=64, nf2=64, ks1=2, ks2=2, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 34.1890 | RMSE: 41.5153 | MAPE: 106.0231\n",
            "\n",
            " Config 16/48  nf1=64, nf2=64, ks1=2, ks2=3, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 34.8158 | RMSE: 42.1471 | MAPE: 108.5635\n",
            "\n",
            " Config 17/48  nf1=64, nf2=64, ks1=2, ks2=3, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 33.8205 | RMSE: 41.2033 | MAPE: 105.8440\n",
            "\n",
            " Config 18/48  nf1=64, nf2=64, ks1=2, ks2=3, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 34.4345 | RMSE: 41.8255 | MAPE: 107.6906\n",
            "\n",
            " Config 19/48  nf1=64, nf2=64, ks1=3, ks2=2, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 33.9271 | RMSE: 41.1893 | MAPE: 104.6871\n",
            "\n",
            " Config 20/48  nf1=64, nf2=64, ks1=3, ks2=2, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 34.4972 | RMSE: 41.8039 | MAPE: 105.8108\n",
            "\n",
            " Config 21/48  nf1=64, nf2=64, ks1=3, ks2=2, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 34.2530 | RMSE: 41.5654 | MAPE: 104.7425\n",
            "\n",
            " Config 22/48  nf1=64, nf2=64, ks1=3, ks2=3, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 35.1567 | RMSE: 42.4935 | MAPE: 103.7703\n",
            "\n",
            " Config 23/48  nf1=64, nf2=64, ks1=3, ks2=3, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 34.1343 | RMSE: 41.4392 | MAPE: 105.6583\n",
            "\n",
            " Config 24/48  nf1=64, nf2=64, ks1=3, ks2=3, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 33.9762 | RMSE: 41.2939 | MAPE: 106.2581\n",
            "\n",
            " Config 25/48  nf1=128, nf2=32, ks1=2, ks2=2, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 34.1732 | RMSE: 41.6010 | MAPE: 107.7742\n",
            "\n",
            " Config 26/48  nf1=128, nf2=32, ks1=2, ks2=2, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 34.6550 | RMSE: 41.8840 | MAPE: 103.8658\n",
            "\n",
            " Config 27/48  nf1=128, nf2=32, ks1=2, ks2=2, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 33.9341 | RMSE: 41.1920 | MAPE: 99.8605\n",
            "\n",
            " Config 28/48  nf1=128, nf2=32, ks1=2, ks2=3, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 33.1564 | RMSE: 40.6507 | MAPE: 101.6623\n",
            "\n",
            " Config 29/48  nf1=128, nf2=32, ks1=2, ks2=3, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 35.6599 | RMSE: 42.8621 | MAPE: 99.3852\n",
            "\n",
            " Config 30/48  nf1=128, nf2=32, ks1=2, ks2=3, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 35.3559 | RMSE: 42.4551 | MAPE: 96.4524\n",
            "\n",
            " Config 31/48  nf1=128, nf2=32, ks1=3, ks2=2, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 33.8923 | RMSE: 41.2998 | MAPE: 107.7596\n",
            "\n",
            " Config 32/48  nf1=128, nf2=32, ks1=3, ks2=2, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 34.9698 | RMSE: 42.2118 | MAPE: 101.6583\n",
            "\n",
            " Config 33/48  nf1=128, nf2=32, ks1=3, ks2=2, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 32.8752 | RMSE: 40.1398 | MAPE: 102.1378\n",
            "\n",
            " Config 34/48  nf1=128, nf2=32, ks1=3, ks2=3, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 33.8125 | RMSE: 41.0761 | MAPE: 104.2906\n",
            "\n",
            " Config 35/48  nf1=128, nf2=32, ks1=3, ks2=3, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 35.2663 | RMSE: 42.4617 | MAPE: 103.2376\n",
            "\n",
            " Config 36/48  nf1=128, nf2=32, ks1=3, ks2=3, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 35.5136 | RMSE: 42.7371 | MAPE: 93.7965\n",
            "\n",
            " Config 37/48  nf1=128, nf2=64, ks1=2, ks2=2, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 34.3618 | RMSE: 41.7347 | MAPE: 106.4576\n",
            "\n",
            " Config 38/48  nf1=128, nf2=64, ks1=2, ks2=2, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 33.5696 | RMSE: 40.8645 | MAPE: 104.2685\n",
            "\n",
            " Config 39/48  nf1=128, nf2=64, ks1=2, ks2=2, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 33.9844 | RMSE: 41.3975 | MAPE: 106.2503\n",
            "\n",
            " Config 40/48  nf1=128, nf2=64, ks1=2, ks2=3, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 33.9513 | RMSE: 41.4146 | MAPE: 107.9778\n",
            "\n",
            " Config 41/48  nf1=128, nf2=64, ks1=2, ks2=3, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 34.6598 | RMSE: 42.0534 | MAPE: 106.5303\n",
            "\n",
            " Config 42/48  nf1=128, nf2=64, ks1=2, ks2=3, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 32.4972 | RMSE: 39.8600 | MAPE: 98.3878\n",
            "\n",
            " Config 43/48  nf1=128, nf2=64, ks1=3, ks2=2, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 34.3521 | RMSE: 41.6963 | MAPE: 106.8671\n",
            "\n",
            " Config 44/48  nf1=128, nf2=64, ks1=3, ks2=2, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 34.7390 | RMSE: 42.1395 | MAPE: 105.7301\n",
            "\n",
            " Config 45/48  nf1=128, nf2=64, ks1=3, ks2=2, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 34.1471 | RMSE: 41.4928 | MAPE: 104.8512\n",
            "\n",
            " Config 46/48  nf1=128, nf2=64, ks1=3, ks2=3, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 34.8365 | RMSE: 42.2681 | MAPE: 105.6223\n",
            "\n",
            " Config 47/48  nf1=128, nf2=64, ks1=3, ks2=3, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 34.4907 | RMSE: 41.8732 | MAPE: 104.0905\n",
            "\n",
            " Config 48/48  nf1=128, nf2=64, ks1=3, ks2=3, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 35.9177 | RMSE: 43.2246 | MAPE: 104.8278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results = df_results.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
        "\n",
        "# Display best configs\n",
        "df_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Yj77BZ_TXaBD",
        "outputId": "401e63de-4ba1-4713-d09b-698dbcd74b66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    filters1  filters2  kernel1  kernel2  dropout     lr  batch        MAE  \\\n",
              "0        128        64        2        3      0.2  0.001     32  32.497195   \n",
              "1        128        32        3        2      0.2  0.001     32  32.875210   \n",
              "2        128        32        2        3      0.0  0.001     32  33.156406   \n",
              "3         64        32        3        3      0.2  0.001     32  33.458933   \n",
              "4        128        64        2        2      0.1  0.001     32  33.569576   \n",
              "5         64        32        2        2      0.2  0.001     32  33.632216   \n",
              "6         64        32        3        2      0.1  0.001     32  33.629762   \n",
              "7         64        32        3        2      0.2  0.001     32  33.759234   \n",
              "8        128        32        3        3      0.0  0.001     32  33.812513   \n",
              "9         64        32        2        3      0.2  0.001     32  33.749580   \n",
              "10        64        32        2        3      0.1  0.001     32  33.726679   \n",
              "11        64        64        3        2      0.0  0.001     32  33.927128   \n",
              "12       128        32        2        2      0.2  0.001     32  33.934066   \n",
              "13        64        64        2        3      0.1  0.001     32  33.820487   \n",
              "14        64        32        2        2      0.1  0.001     32  33.844939   \n",
              "15        64        64        3        3      0.2  0.001     32  33.976203   \n",
              "16       128        32        3        2      0.0  0.001     32  33.892261   \n",
              "17        64        32        2        2      0.0  0.001     32  33.977512   \n",
              "18       128        64        2        2      0.2  0.001     32  33.984412   \n",
              "19       128        64        2        3      0.0  0.001     32  33.951323   \n",
              "20        64        64        2        2      0.1  0.001     32  34.048455   \n",
              "21        64        64        3        3      0.1  0.001     32  34.134271   \n",
              "22       128        64        3        2      0.2  0.001     32  34.147120   \n",
              "23        64        64        2        2      0.2  0.001     32  34.188979   \n",
              "24        64        32        3        3      0.0  0.001     32  34.093798   \n",
              "25        64        64        3        2      0.2  0.001     32  34.253008   \n",
              "26       128        32        2        2      0.0  0.001     32  34.173228   \n",
              "27        64        32        3        2      0.0  0.001     32  34.303126   \n",
              "28       128        64        3        2      0.0  0.001     32  34.352080   \n",
              "29       128        64        2        2      0.0  0.001     32  34.361762   \n",
              "30        64        32        3        3      0.1  0.001     32  34.458425   \n",
              "31        64        64        2        2      0.0  0.001     32  34.445220   \n",
              "32        64        64        3        2      0.1  0.001     32  34.497231   \n",
              "33        64        32        2        3      0.0  0.001     32  34.388229   \n",
              "34        64        64        2        3      0.2  0.001     32  34.434535   \n",
              "35       128        64        3        3      0.1  0.001     32  34.490663   \n",
              "36       128        32        2        2      0.1  0.001     32  34.654954   \n",
              "37       128        64        2        3      0.1  0.001     32  34.659840   \n",
              "38       128        64        3        2      0.1  0.001     32  34.739049   \n",
              "39        64        64        2        3      0.0  0.001     32  34.815756   \n",
              "40       128        32        3        2      0.1  0.001     32  34.969848   \n",
              "41       128        64        3        3      0.0  0.001     32  34.836531   \n",
              "42       128        32        2        3      0.2  0.001     32  35.355907   \n",
              "43       128        32        3        3      0.1  0.001     32  35.266327   \n",
              "44        64        64        3        3      0.0  0.001     32  35.156663   \n",
              "45       128        32        3        3      0.2  0.001     32  35.513558   \n",
              "46       128        32        2        3      0.1  0.001     32  35.659901   \n",
              "47       128        64        3        3      0.2  0.001     32  35.917728   \n",
              "\n",
              "         RMSE        MAPE  \n",
              "0   39.859975   98.387844  \n",
              "1   40.139777  102.137753  \n",
              "2   40.650717  101.662257  \n",
              "3   40.706932  102.390350  \n",
              "4   40.864505  104.268514  \n",
              "5   40.970308  107.184378  \n",
              "6   41.027502  105.026401  \n",
              "7   41.028450  103.671009  \n",
              "8   41.076084  104.290589  \n",
              "9   41.083515  105.938059  \n",
              "10  41.103208  107.488845  \n",
              "11  41.189289  104.687094  \n",
              "12  41.192019   99.860520  \n",
              "13  41.203254  105.843964  \n",
              "14  41.283838  107.770889  \n",
              "15  41.293923  106.258103  \n",
              "16  41.299825  107.759628  \n",
              "17  41.363762  106.667501  \n",
              "18  41.397489  106.250311  \n",
              "19  41.414611  107.977757  \n",
              "20  41.422586  108.505151  \n",
              "21  41.439219  105.658280  \n",
              "22  41.492760  104.851179  \n",
              "23  41.515348  106.023082  \n",
              "24  41.544425  107.228582  \n",
              "25  41.565392  104.742507  \n",
              "26  41.600962  107.774184  \n",
              "27  41.655718  108.164521  \n",
              "28  41.696284  106.867124  \n",
              "29  41.734735  106.457556  \n",
              "30  41.738878  104.359297  \n",
              "31  41.793667  105.118473  \n",
              "32  41.803858  105.810795  \n",
              "33  41.815589  109.502014  \n",
              "34  41.825467  107.690594  \n",
              "35  41.873169  104.090461  \n",
              "36  41.884011  103.865777  \n",
              "37  42.053436  106.530283  \n",
              "38  42.139539  105.730061  \n",
              "39  42.147108  108.563535  \n",
              "40  42.211792  101.658270  \n",
              "41  42.268135  105.622286  \n",
              "42  42.455115   96.452412  \n",
              "43  42.461684  103.237590  \n",
              "44  42.493510  103.770316  \n",
              "45  42.737139   93.796523  \n",
              "46  42.862096   99.385164  \n",
              "47  43.224632  104.827770  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d0b062d1-6c56-4f43-ab77-ade77307cec2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filters1</th>\n",
              "      <th>filters2</th>\n",
              "      <th>kernel1</th>\n",
              "      <th>kernel2</th>\n",
              "      <th>dropout</th>\n",
              "      <th>lr</th>\n",
              "      <th>batch</th>\n",
              "      <th>MAE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>32.497195</td>\n",
              "      <td>39.859975</td>\n",
              "      <td>98.387844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>128</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>32.875210</td>\n",
              "      <td>40.139777</td>\n",
              "      <td>102.137753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>128</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>33.156406</td>\n",
              "      <td>40.650717</td>\n",
              "      <td>101.662257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>33.458933</td>\n",
              "      <td>40.706932</td>\n",
              "      <td>102.390350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>33.569576</td>\n",
              "      <td>40.864505</td>\n",
              "      <td>104.268514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>33.632216</td>\n",
              "      <td>40.970308</td>\n",
              "      <td>107.184378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>33.629762</td>\n",
              "      <td>41.027502</td>\n",
              "      <td>105.026401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>33.759234</td>\n",
              "      <td>41.028450</td>\n",
              "      <td>103.671009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>128</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>33.812513</td>\n",
              "      <td>41.076084</td>\n",
              "      <td>104.290589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>33.749580</td>\n",
              "      <td>41.083515</td>\n",
              "      <td>105.938059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>33.726679</td>\n",
              "      <td>41.103208</td>\n",
              "      <td>107.488845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>33.927128</td>\n",
              "      <td>41.189289</td>\n",
              "      <td>104.687094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>128</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>33.934066</td>\n",
              "      <td>41.192019</td>\n",
              "      <td>99.860520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>33.820487</td>\n",
              "      <td>41.203254</td>\n",
              "      <td>105.843964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>33.844939</td>\n",
              "      <td>41.283838</td>\n",
              "      <td>107.770889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>33.976203</td>\n",
              "      <td>41.293923</td>\n",
              "      <td>106.258103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>128</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>33.892261</td>\n",
              "      <td>41.299825</td>\n",
              "      <td>107.759628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>33.977512</td>\n",
              "      <td>41.363762</td>\n",
              "      <td>106.667501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>33.984412</td>\n",
              "      <td>41.397489</td>\n",
              "      <td>106.250311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>33.951323</td>\n",
              "      <td>41.414611</td>\n",
              "      <td>107.977757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.048455</td>\n",
              "      <td>41.422586</td>\n",
              "      <td>108.505151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.134271</td>\n",
              "      <td>41.439219</td>\n",
              "      <td>105.658280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.147120</td>\n",
              "      <td>41.492760</td>\n",
              "      <td>104.851179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.188979</td>\n",
              "      <td>41.515348</td>\n",
              "      <td>106.023082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.093798</td>\n",
              "      <td>41.544425</td>\n",
              "      <td>107.228582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.253008</td>\n",
              "      <td>41.565392</td>\n",
              "      <td>104.742507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>128</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.173228</td>\n",
              "      <td>41.600962</td>\n",
              "      <td>107.774184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.303126</td>\n",
              "      <td>41.655718</td>\n",
              "      <td>108.164521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.352080</td>\n",
              "      <td>41.696284</td>\n",
              "      <td>106.867124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.361762</td>\n",
              "      <td>41.734735</td>\n",
              "      <td>106.457556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.458425</td>\n",
              "      <td>41.738878</td>\n",
              "      <td>104.359297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.445220</td>\n",
              "      <td>41.793667</td>\n",
              "      <td>105.118473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.497231</td>\n",
              "      <td>41.803858</td>\n",
              "      <td>105.810795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.388229</td>\n",
              "      <td>41.815589</td>\n",
              "      <td>109.502014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.434535</td>\n",
              "      <td>41.825467</td>\n",
              "      <td>107.690594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.490663</td>\n",
              "      <td>41.873169</td>\n",
              "      <td>104.090461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>128</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.654954</td>\n",
              "      <td>41.884011</td>\n",
              "      <td>103.865777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.659840</td>\n",
              "      <td>42.053436</td>\n",
              "      <td>106.530283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.739049</td>\n",
              "      <td>42.139539</td>\n",
              "      <td>105.730061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.815756</td>\n",
              "      <td>42.147108</td>\n",
              "      <td>108.563535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>128</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.969848</td>\n",
              "      <td>42.211792</td>\n",
              "      <td>101.658270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>34.836531</td>\n",
              "      <td>42.268135</td>\n",
              "      <td>105.622286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>128</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>35.355907</td>\n",
              "      <td>42.455115</td>\n",
              "      <td>96.452412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>128</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>35.266327</td>\n",
              "      <td>42.461684</td>\n",
              "      <td>103.237590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>35.156663</td>\n",
              "      <td>42.493510</td>\n",
              "      <td>103.770316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>128</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>35.513558</td>\n",
              "      <td>42.737139</td>\n",
              "      <td>93.796523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>128</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>35.659901</td>\n",
              "      <td>42.862096</td>\n",
              "      <td>99.385164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>35.917728</td>\n",
              "      <td>43.224632</td>\n",
              "      <td>104.827770</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d0b062d1-6c56-4f43-ab77-ade77307cec2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d0b062d1-6c56-4f43-ab77-ade77307cec2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d0b062d1-6c56-4f43-ab77-ade77307cec2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-61ae8aa5-b97d-46e7-bdcd-c4b1fae617a5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-61ae8aa5-b97d-46e7-bdcd-c4b1fae617a5')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-61ae8aa5-b97d-46e7-bdcd-c4b1fae617a5 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_e28c34e1-9f58-407d-90f1-412dbdc40f58\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_e28c34e1-9f58-407d-90f1-412dbdc40f58 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 48,\n  \"fields\": [\n    {\n      \"column\": \"filters1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 32,\n        \"min\": 64,\n        \"max\": 128,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          64,\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"filters2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16,\n        \"min\": 32,\n        \"max\": 64,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          32,\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"kernel1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"kernel2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08251369970070349,\n        \"min\": 0.0,\n        \"max\": 0.2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.2,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.574053185158759e-19,\n        \"min\": 0.001,\n        \"max\": 0.001,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6768097222833749,\n        \"min\": 32.497195144325076,\n        \"max\": 35.91772758118424,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          34.303125952165345\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6522022376169356,\n        \"min\": 39.85997514645257,\n        \"max\": 43.22463174753139,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          41.655717579613594\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.179569029019663,\n        \"min\": 93.79652309210262,\n        \"max\": 109.50201409792498,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          108.16452126168872\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TCN"
      ],
      "metadata": {
        "id": "V_OOz67bdxIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Chomp1d(nn.Module):\n",
        "    \"\"\"Removes padding from the end to maintain causality.\"\"\"\n",
        "    def __init__(self, chomp_size):\n",
        "        super().__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :-self.chomp_size]  # Remove last chomp_size elements\n",
        "\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, dilation, padding, dropout):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
        "                               stride=stride, padding=padding, dilation=dilation)\n",
        "        self.chomp1 = Chomp1d(padding)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,\n",
        "                               stride=stride, padding=padding, dilation=dilation)\n",
        "        self.chomp2 = Chomp1d(padding)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.chomp1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.dropout1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.chomp2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.dropout2(out)\n",
        "\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.relu(out + res)\n",
        "\n",
        "\n",
        "class TemporalConvNet(nn.Module):\n",
        "    def __init__(self, num_inputs, num_channels, kernel_size=3, dropout=0.2):\n",
        "        \"\"\"\n",
        "        num_inputs: number of input features\n",
        "        num_channels: list of output channels for each TCN layer (e.g. [64, 64])\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        num_levels = len(num_channels)\n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2 ** i\n",
        "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
        "            out_channels = num_channels[i]\n",
        "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1,\n",
        "                                     dilation=dilation_size, padding=(kernel_size-1)*dilation_size,\n",
        "                                     dropout=dropout)]\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch, seq_len, features]\n",
        "        x = x.permute(0, 2, 1)  # [batch, features, seq_len]\n",
        "        out = self.network(x)\n",
        "        out = out[:, :, -1]  # Use last timestep\n",
        "        return out\n",
        "\n",
        "\n",
        "class TCNForecastNet(nn.Module):\n",
        "    def __init__(self, input_size, num_channels=[64, 64], kernel_size=3, dropout=0.2, output_size=14):\n",
        "        super().__init__()\n",
        "        self.tcn = TemporalConvNet(num_inputs=input_size, num_channels=num_channels,\n",
        "                                   kernel_size=kernel_size, dropout=dropout)\n",
        "        self.fc = nn.Linear(num_channels[-1], output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.tcn(x)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "cPwQSex2Y14X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class TCNForecastDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "def create_tcn_input(X_scaled, y_scaled, sequence_len=30):\n",
        "    X_seq = []\n",
        "    y_seq = []\n",
        "    for i in range(sequence_len, len(X_scaled)):\n",
        "        X_window = X_scaled[i-sequence_len:i]\n",
        "        y_target = y_scaled[i]  # shape: (14,)\n",
        "        X_seq.append(X_window)\n",
        "        y_seq.append(y_target)\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "\n",
        "def train_one_tcn_fold(X_train, y_train, X_val, y_val,\n",
        "                       sequence_len=30, num_epochs=50, patience=5,\n",
        "                       batch_size=32, lr=0.001,\n",
        "                       num_channels=[64, 64], kernel_size=3, dropout=0.2):\n",
        "\n",
        "    # --- 1. Scaling ---\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_val_scaled = scaler_X.transform(X_val)\n",
        "\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "    y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "    # --- 2. Create TCN inputs ---\n",
        "    X_train_seq, y_train_seq = create_tcn_input(X_train_scaled, y_train_scaled, sequence_len)\n",
        "    X_val_seq, y_val_seq = create_tcn_input(X_val_scaled, y_val_scaled, sequence_len)\n",
        "\n",
        "    if len(X_val_seq) == 0:\n",
        "        print(\"Skipping fold: not enough validation sequences.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # --- 3. Dataloaders ---\n",
        "    train_loader = DataLoader(\n",
        "        TCNForecastDataset(X_train_seq, y_train_seq), batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        TCNForecastDataset(X_val_seq, y_val_seq), batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "\n",
        "    # --- 4. Model Setup ---\n",
        "    model = TCNForecastNet(\n",
        "        input_size=X_train_seq.shape[2],\n",
        "        num_channels=num_channels,\n",
        "        kernel_size=kernel_size,\n",
        "        dropout=dropout,\n",
        "        output_size=y_train.shape[1]  # usually 14\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    # --- 5. Early stopping setup ---\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_preds = torch.cat([model(xb.to(device)) for xb, _ in val_loader])\n",
        "            val_targets = torch.cat([yb.to(device) for _, yb in val_loader])\n",
        "            val_loss = criterion(val_preds, val_targets)\n",
        "\n",
        "        if val_loss.item() < best_val_loss - 1e-4:\n",
        "            best_val_loss = val_loss.item()\n",
        "            best_model_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                break\n",
        "\n",
        "    # --- 6. Load best model ---\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    # --- 7. Final evaluation ---\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32).to(device)\n",
        "        preds_scaled = model(X_val_tensor).cpu().numpy()\n",
        "\n",
        "    targets_scaled = y_val_seq\n",
        "    preds = scaler_y.inverse_transform(preds_scaled)\n",
        "    targets = scaler_y.inverse_transform(targets_scaled)\n",
        "\n",
        "    mae = mean_absolute_error(targets.flatten(), preds.flatten())\n",
        "    rmse = np.sqrt(mean_squared_error(targets.flatten(), preds.flatten()))\n",
        "    mape = mean_absolute_percentage_error(targets.flatten(), preds.flatten()) * 100\n",
        "\n",
        "    return mae, rmse, mape"
      ],
      "metadata": {
        "id": "mZnbS2z7Y16U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_len = 30\n",
        "horizon = 14\n",
        "initial_train_size = 1095\n",
        "\n",
        "mae_scores = []\n",
        "rmse_scores = []\n",
        "mape_scores = []\n",
        "\n",
        "for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "    expanding_window_cv(X, y, initial_train_size=initial_train_size, horizon=horizon, step=horizon, sequence_len=sequence_len)\n",
        "):\n",
        "    print(f\"\\nFold {fold+1}\")\n",
        "    mae, rmse, mape = train_one_tcn_fold(\n",
        "        X_tr, y_tr, X_val, y_val,\n",
        "        sequence_len=sequence_len,\n",
        "        num_epochs=50,\n",
        "        patience=5,\n",
        "        batch_size=32,\n",
        "        lr=0.001,\n",
        "        num_channels=[64, 64],\n",
        "        kernel_size=3,\n",
        "        dropout=0.2\n",
        "    )\n",
        "\n",
        "    if mae is None:\n",
        "        continue\n",
        "\n",
        "    print(f\"  MAE: {mae:.4f} | RMSE: {rmse:.4f} | MAPE: {mape:.4f}\")\n",
        "    mae_scores.append(mae)\n",
        "    rmse_scores.append(rmse)\n",
        "    mape_scores.append(mape)\n",
        "\n",
        "# --- Final Results ---\n",
        "print(\"\\n Final TCN CV Results:\")\n",
        "print(f\"Average MAE:  {np.mean(mae_scores):.4f}\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_scores):.4f}\")\n",
        "print(f\"Average MAPE: {np.mean(mape_scores):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBVS9-iCY18N",
        "outputId": "c0351cfa-a70d-4eeb-a61c-2faf5b5a7632",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1\n",
            "  MAE: 8.8175 | RMSE: 10.6820 | MAPE: 35.9513\n",
            "\n",
            "Fold 2\n",
            "  MAE: 8.8393 | RMSE: 10.6083 | MAPE: 52.1615\n",
            "\n",
            "Fold 3\n",
            "  MAE: 8.1011 | RMSE: 9.5065 | MAPE: 44.8374\n",
            "\n",
            "Fold 4\n",
            "  MAE: 4.8996 | RMSE: 6.1100 | MAPE: 14.6355\n",
            "\n",
            "Fold 5\n",
            "  MAE: 5.4180 | RMSE: 7.3444 | MAPE: 20.2983\n",
            "\n",
            "Fold 6\n",
            "  MAE: 6.5057 | RMSE: 8.2876 | MAPE: 24.3780\n",
            "\n",
            "Fold 7\n",
            "  MAE: 9.2561 | RMSE: 10.2730 | MAPE: 24.5859\n",
            "\n",
            "Fold 8\n",
            "  MAE: 8.1772 | RMSE: 9.3998 | MAPE: 19.1793\n",
            "\n",
            "Fold 9\n",
            "  MAE: 7.3479 | RMSE: 9.0078 | MAPE: 17.6021\n",
            "\n",
            "Fold 10\n",
            "  MAE: 7.7834 | RMSE: 8.9665 | MAPE: 17.0664\n",
            "\n",
            "Fold 11\n",
            "  MAE: 6.5112 | RMSE: 7.4065 | MAPE: 12.2486\n",
            "\n",
            "Fold 12\n",
            "  MAE: 6.0139 | RMSE: 7.7098 | MAPE: 10.9147\n",
            "\n",
            "Fold 13\n",
            "  MAE: 6.6347 | RMSE: 8.0358 | MAPE: 11.5139\n",
            "\n",
            "Fold 14\n",
            "  MAE: 7.4766 | RMSE: 8.9077 | MAPE: 12.5203\n",
            "\n",
            "Fold 15\n",
            "  MAE: 11.1171 | RMSE: 14.2510 | MAPE: 28.6449\n",
            "\n",
            "Fold 16\n",
            "  MAE: 12.9627 | RMSE: 15.8488 | MAPE: 37.4568\n",
            "\n",
            "Fold 17\n",
            "  MAE: 13.1779 | RMSE: 15.8476 | MAPE: 33.1182\n",
            "\n",
            "Fold 18\n",
            "  MAE: 10.2801 | RMSE: 13.0285 | MAPE: 22.9988\n",
            "\n",
            "Fold 19\n",
            "  MAE: 10.9501 | RMSE: 13.4344 | MAPE: 20.7568\n",
            "\n",
            "Fold 20\n",
            "  MAE: 13.7729 | RMSE: 16.9759 | MAPE: 26.4259\n",
            "\n",
            "Fold 21\n",
            "  MAE: 11.9050 | RMSE: 15.0896 | MAPE: 29.2872\n",
            "\n",
            "Fold 22\n",
            "  MAE: 11.2611 | RMSE: 15.0724 | MAPE: 47.6052\n",
            "\n",
            "Fold 23\n",
            "  MAE: 10.1629 | RMSE: 15.0163 | MAPE: 50.0026\n",
            "\n",
            "Fold 24\n",
            "  MAE: 12.4793 | RMSE: 16.0424 | MAPE: 37.7967\n",
            "\n",
            "Fold 25\n",
            "  MAE: 11.3556 | RMSE: 14.5305 | MAPE: 33.7913\n",
            "\n",
            "Fold 26\n",
            "  MAE: 7.0172 | RMSE: 10.4173 | MAPE: 32.3880\n",
            "\n",
            "Fold 27\n",
            "  MAE: 6.1057 | RMSE: 10.0638 | MAPE: 46.7221\n",
            "\n",
            "Fold 28\n",
            "  MAE: 7.3399 | RMSE: 11.7440 | MAPE: 78.0406\n",
            "\n",
            "Fold 29\n",
            "  MAE: 5.3085 | RMSE: 7.9033 | MAPE: 48.1960\n",
            "\n",
            "Fold 30\n",
            "  MAE: 5.1442 | RMSE: 8.4805 | MAPE: 19.7045\n",
            "\n",
            "Fold 31\n",
            "  MAE: 7.1584 | RMSE: 10.7475 | MAPE: 28.7681\n",
            "\n",
            "Fold 32\n",
            "  MAE: 6.4239 | RMSE: 9.4290 | MAPE: 23.7595\n",
            "\n",
            "Fold 33\n",
            "  MAE: 6.3759 | RMSE: 9.0630 | MAPE: 27.2803\n",
            "\n",
            "Fold 34\n",
            "  MAE: 6.3229 | RMSE: 9.1657 | MAPE: 33.6112\n",
            "\n",
            "Fold 35\n",
            "  MAE: 6.4434 | RMSE: 8.5917 | MAPE: 31.0041\n",
            "\n",
            "Fold 36\n",
            "  MAE: 6.0990 | RMSE: 7.2279 | MAPE: 20.0111\n",
            "\n",
            "Fold 37\n",
            "  MAE: 5.6542 | RMSE: 6.8235 | MAPE: 15.8368\n",
            "\n",
            "Fold 38\n",
            "  MAE: 7.0460 | RMSE: 9.6251 | MAPE: 24.8902\n",
            "\n",
            "Fold 39\n",
            "  MAE: 7.2644 | RMSE: 9.9730 | MAPE: 28.0679\n",
            "\n",
            "Fold 40\n",
            "  MAE: 5.8423 | RMSE: 7.6111 | MAPE: 19.3441\n",
            "\n",
            "Fold 41\n",
            "  MAE: 6.1740 | RMSE: 7.5836 | MAPE: 19.6543\n",
            "\n",
            "Fold 42\n",
            "  MAE: 5.4315 | RMSE: 6.9109 | MAPE: 17.6406\n",
            "\n",
            "Fold 43\n",
            "  MAE: 5.8807 | RMSE: 7.1664 | MAPE: 17.6306\n",
            "\n",
            "Fold 44\n",
            "  MAE: 6.1932 | RMSE: 7.3308 | MAPE: 16.5991\n",
            "\n",
            "Fold 45\n",
            "  MAE: 5.7855 | RMSE: 6.9317 | MAPE: 13.8128\n",
            "\n",
            "Fold 46\n",
            "  MAE: 6.2261 | RMSE: 7.9533 | MAPE: 26.2469\n",
            "\n",
            "Fold 47\n",
            "  MAE: 10.6814 | RMSE: 14.2584 | MAPE: 82.5817\n",
            "\n",
            "Fold 48\n",
            "  MAE: 6.5451 | RMSE: 9.5225 | MAPE: 58.3661\n",
            "\n",
            "Fold 49\n",
            "  MAE: 6.1860 | RMSE: 7.4768 | MAPE: 22.7086\n",
            "\n",
            "Fold 50\n",
            "  MAE: 6.8630 | RMSE: 8.1794 | MAPE: 33.6051\n",
            "\n",
            "Fold 51\n",
            "  MAE: 10.2584 | RMSE: 11.7128 | MAPE: 81.5430\n",
            "\n",
            "Fold 52\n",
            "  MAE: 12.4517 | RMSE: 14.3924 | MAPE: 188.4025\n",
            "\n",
            "Fold 53\n",
            "  MAE: 10.8924 | RMSE: 12.5753 | MAPE: 171.7695\n",
            "\n",
            "Fold 54\n",
            "  MAE: 11.2520 | RMSE: 12.8201 | MAPE: 115.5748\n",
            "\n",
            "Fold 55\n",
            "  MAE: 10.7767 | RMSE: 12.8514 | MAPE: 127.2512\n",
            "\n",
            "Fold 56\n",
            "  MAE: 9.5318 | RMSE: 11.9113 | MAPE: 124.4303\n",
            "\n",
            "Fold 57\n",
            "  MAE: 7.5072 | RMSE: 9.8286 | MAPE: 81.8452\n",
            "\n",
            "Fold 58\n",
            "  MAE: 6.7815 | RMSE: 8.1910 | MAPE: 64.2163\n",
            "\n",
            "Fold 59\n",
            "  MAE: 6.3803 | RMSE: 7.9225 | MAPE: 68.2577\n",
            "\n",
            "Fold 60\n",
            "  MAE: 7.6294 | RMSE: 9.3409 | MAPE: 66.4009\n",
            "\n",
            "Fold 61\n",
            "  MAE: 7.3254 | RMSE: 9.6495 | MAPE: 54.3086\n",
            "\n",
            "Fold 62\n",
            "  MAE: 9.7256 | RMSE: 13.3990 | MAPE: 134.1957\n",
            "\n",
            "Fold 63\n",
            "  MAE: 10.1852 | RMSE: 14.1376 | MAPE: 168.2097\n",
            "\n",
            "Fold 64\n",
            "  MAE: 8.5053 | RMSE: 11.2328 | MAPE: 94.0254\n",
            "\n",
            "Fold 65\n",
            "  MAE: 7.9705 | RMSE: 10.3539 | MAPE: 63.9792\n",
            "\n",
            "Fold 66\n",
            "  MAE: 7.3688 | RMSE: 9.2295 | MAPE: 23.3201\n",
            "\n",
            "Fold 67\n",
            "  MAE: 9.2891 | RMSE: 12.1200 | MAPE: 35.7194\n",
            "\n",
            "Fold 68\n",
            "  MAE: 12.1025 | RMSE: 15.4178 | MAPE: 96.1952\n",
            "\n",
            "Fold 69\n",
            "  MAE: 11.2562 | RMSE: 13.9906 | MAPE: 96.8371\n",
            "\n",
            "Fold 70\n",
            "  MAE: 10.7371 | RMSE: 13.3378 | MAPE: 110.6058\n",
            "\n",
            "Fold 71\n",
            "  MAE: 14.7157 | RMSE: 16.7191 | MAPE: 285.9675\n",
            "\n",
            "Fold 72\n",
            "  MAE: 18.8734 | RMSE: 21.0037 | MAPE: 433.7908\n",
            "\n",
            "Fold 73\n",
            "  MAE: 18.8439 | RMSE: 22.3733 | MAPE: 325.5312\n",
            "\n",
            "Fold 74\n",
            "  MAE: 15.3330 | RMSE: 19.4781 | MAPE: 54.9076\n",
            "\n",
            "Fold 75\n",
            "  MAE: 17.2316 | RMSE: 21.0939 | MAPE: 56.2910\n",
            "\n",
            "Fold 76\n",
            "  MAE: 16.6815 | RMSE: 20.3081 | MAPE: 35.8311\n",
            "\n",
            "Fold 77\n",
            "  MAE: 13.2247 | RMSE: 15.8928 | MAPE: 26.7344\n",
            "\n",
            "Fold 78\n",
            "  MAE: 11.0677 | RMSE: 13.8536 | MAPE: 23.3214\n",
            "\n",
            "Fold 79\n",
            "  MAE: 10.6384 | RMSE: 12.8749 | MAPE: 25.1615\n",
            "\n",
            "Fold 80\n",
            "  MAE: 12.7024 | RMSE: 14.4547 | MAPE: 32.8412\n",
            "\n",
            "Fold 81\n",
            "  MAE: 12.8021 | RMSE: 15.7503 | MAPE: 45.8617\n",
            "\n",
            "Fold 82\n",
            "  MAE: 15.1372 | RMSE: 18.7011 | MAPE: 48.6663\n",
            "\n",
            "Fold 83\n",
            "  MAE: 15.4356 | RMSE: 18.6389 | MAPE: 38.2881\n",
            "\n",
            "Fold 84\n",
            "  MAE: 15.7672 | RMSE: 18.2110 | MAPE: 38.0782\n",
            "\n",
            "Fold 85\n",
            "  MAE: 17.0232 | RMSE: 19.3370 | MAPE: 44.5573\n",
            "\n",
            "Fold 86\n",
            "  MAE: 18.6901 | RMSE: 20.5287 | MAPE: 37.5930\n",
            "\n",
            "Fold 87\n",
            "  MAE: 22.4113 | RMSE: 23.6910 | MAPE: 31.6828\n",
            "\n",
            "Fold 88\n",
            "  MAE: 22.5913 | RMSE: 23.8172 | MAPE: 27.5562\n",
            "\n",
            "Fold 89\n",
            "  MAE: 19.2054 | RMSE: 21.1186 | MAPE: 24.0381\n",
            "\n",
            "Fold 90\n",
            "  MAE: 14.5808 | RMSE: 18.0482 | MAPE: 24.4855\n",
            "\n",
            "Fold 91\n",
            "  MAE: 13.8250 | RMSE: 17.4868 | MAPE: 24.9927\n",
            "\n",
            "Fold 92\n",
            "  MAE: 17.6386 | RMSE: 22.3189 | MAPE: 19.4970\n",
            "\n",
            "Fold 93\n",
            "  MAE: 30.8657 | RMSE: 34.7034 | MAPE: 25.2299\n",
            "\n",
            "Fold 94\n",
            "  MAE: 36.5088 | RMSE: 46.1136 | MAPE: 30.4162\n",
            "\n",
            "Fold 95\n",
            "  MAE: 42.2539 | RMSE: 57.0840 | MAPE: 42.2099\n",
            "\n",
            "Fold 96\n",
            "  MAE: 52.6865 | RMSE: 65.6294 | MAPE: 66.7757\n",
            "\n",
            "Fold 97\n",
            "  MAE: 61.7676 | RMSE: 72.7331 | MAPE: 77.1607\n",
            "\n",
            "Fold 98\n",
            "  MAE: 49.4128 | RMSE: 58.4054 | MAPE: 41.2223\n",
            "\n",
            "Fold 99\n",
            "  MAE: 50.7067 | RMSE: 64.5510 | MAPE: 31.2980\n",
            "\n",
            "Fold 100\n",
            "  MAE: 74.0031 | RMSE: 96.9780 | MAPE: 40.2102\n",
            "\n",
            "Fold 101\n",
            "  MAE: 89.1773 | RMSE: 113.2742 | MAPE: 87.9473\n",
            "\n",
            "Fold 102\n",
            "  MAE: 67.5254 | RMSE: 86.2114 | MAPE: 92.4853\n",
            "\n",
            "Fold 103\n",
            "  MAE: 41.1864 | RMSE: 51.6037 | MAPE: 61.6654\n",
            "\n",
            "Fold 104\n",
            "  MAE: 37.4114 | RMSE: 45.9253 | MAPE: 47.9886\n",
            "\n",
            "Fold 105\n",
            "  MAE: 91.6328 | RMSE: 139.1636 | MAPE: 41.0591\n",
            "\n",
            "Fold 106\n",
            "  MAE: 114.6435 | RMSE: 153.1673 | MAPE: 54.2758\n",
            "\n",
            "Fold 107\n",
            "  MAE: 72.9053 | RMSE: 101.4418 | MAPE: 49.9741\n",
            "\n",
            "Fold 108\n",
            "  MAE: 51.1215 | RMSE: 61.0827 | MAPE: 42.3958\n",
            "\n",
            "Fold 109\n",
            "  MAE: 58.1913 | RMSE: 66.2938 | MAPE: 40.1906\n",
            "\n",
            "Fold 110\n",
            "  MAE: 60.6070 | RMSE: 71.0204 | MAPE: 29.5918\n",
            "\n",
            "Fold 111\n",
            "  MAE: 40.6393 | RMSE: 51.1412 | MAPE: 40.1619\n",
            "\n",
            "Fold 112\n",
            "  MAE: 51.0010 | RMSE: 60.3824 | MAPE: 43.0314\n",
            "\n",
            "Fold 113\n",
            "  MAE: 73.5871 | RMSE: 89.2110 | MAPE: 35.4201\n",
            "\n",
            "Fold 114\n",
            "  MAE: 84.1883 | RMSE: 100.8071 | MAPE: 33.9277\n",
            "\n",
            "Fold 115\n",
            "  MAE: 91.9977 | RMSE: 112.1645 | MAPE: 32.9145\n",
            "\n",
            "Fold 116\n",
            "  MAE: 108.8325 | RMSE: 126.4859 | MAPE: 33.2191\n",
            "\n",
            "Fold 117\n",
            "  MAE: 120.3259 | RMSE: 152.1255 | MAPE: 27.4020\n",
            "\n",
            "Fold 118\n",
            "  MAE: 166.1132 | RMSE: 197.3508 | MAPE: 33.0998\n",
            "\n",
            "Fold 119\n",
            "  MAE: 160.9783 | RMSE: 188.4584 | MAPE: 63.4676\n",
            "\n",
            "Fold 120\n",
            "  MAE: 112.3328 | RMSE: 145.4785 | MAPE: 105.1274\n",
            "\n",
            "Fold 121\n",
            "  MAE: 114.1997 | RMSE: 143.8160 | MAPE: 157.2670\n",
            "\n",
            "Fold 122\n",
            "  MAE: 92.2785 | RMSE: 121.0080 | MAPE: 133.9769\n",
            "\n",
            "Fold 123\n",
            "  MAE: 54.7402 | RMSE: 63.2064 | MAPE: 229.5948\n",
            "\n",
            "Fold 124\n",
            "  MAE: 88.6639 | RMSE: 113.0944 | MAPE: 239.8002\n",
            "\n",
            "Fold 125\n",
            "  MAE: 147.9063 | RMSE: 168.2339 | MAPE: 81.6233\n",
            "\n",
            "Fold 126\n",
            "  MAE: 147.8314 | RMSE: 163.7872 | MAPE: 151.2544\n",
            "\n",
            "Fold 127\n",
            "  MAE: 107.9327 | RMSE: 130.5653 | MAPE: 243.1313\n",
            "\n",
            "Fold 128\n",
            "  MAE: 68.3916 | RMSE: 88.5034 | MAPE: 174.5600\n",
            "\n",
            "Fold 129\n",
            "  MAE: 39.9430 | RMSE: 47.2609 | MAPE: 45.8967\n",
            "\n",
            "Fold 130\n",
            "  MAE: 34.1034 | RMSE: 41.5415 | MAPE: 33.7251\n",
            "\n",
            "Fold 131\n",
            "  MAE: 25.5290 | RMSE: 31.7427 | MAPE: 25.6992\n",
            "\n",
            "Fold 132\n",
            "  MAE: 35.9489 | RMSE: 44.3742 | MAPE: 49.6086\n",
            "\n",
            "Fold 133\n",
            "  MAE: 33.9944 | RMSE: 43.3296 | MAPE: 61.9767\n",
            "\n",
            "Fold 134\n",
            "  MAE: 31.6600 | RMSE: 40.6157 | MAPE: 55.8882\n",
            "\n",
            "Fold 135\n",
            "  MAE: 24.6946 | RMSE: 30.6440 | MAPE: 33.1861\n",
            "\n",
            "Fold 136\n",
            "  MAE: 21.7787 | RMSE: 27.2073 | MAPE: 30.7083\n",
            "\n",
            "Fold 137\n",
            "  MAE: 47.7200 | RMSE: 53.6411 | MAPE: 89.7902\n",
            "\n",
            "Fold 138\n",
            "  MAE: 30.1218 | RMSE: 35.8979 | MAPE: 59.0497\n",
            "\n",
            "Fold 139\n",
            "  MAE: 23.1726 | RMSE: 31.6014 | MAPE: 35.3784\n",
            "\n",
            "Fold 140\n",
            "  MAE: 43.2590 | RMSE: 59.2003 | MAPE: 93.1478\n",
            "\n",
            "Fold 141\n",
            "  MAE: 35.6555 | RMSE: 48.9940 | MAPE: 138.3512\n",
            "\n",
            "Fold 142\n",
            "  MAE: 30.0767 | RMSE: 39.6013 | MAPE: 209.2444\n",
            "\n",
            "Fold 143\n",
            "  MAE: 30.2436 | RMSE: 38.6002 | MAPE: 167.1297\n",
            "\n",
            "Fold 144\n",
            "  MAE: 28.0816 | RMSE: 35.3288 | MAPE: 78.1436\n",
            "\n",
            "Fold 145\n",
            "  MAE: 29.6048 | RMSE: 41.4964 | MAPE: 543.2464\n",
            "\n",
            "Fold 146\n",
            "  MAE: 36.8146 | RMSE: 50.4113 | MAPE: 981.1073\n",
            "\n",
            "Fold 147\n",
            "  MAE: 44.2395 | RMSE: 54.0468 | MAPE: 2054.3358\n",
            "\n",
            "Fold 148\n",
            "  MAE: 36.6377 | RMSE: 44.8863 | MAPE: 1784.7288\n",
            "\n",
            "Fold 149\n",
            "  MAE: 31.2884 | RMSE: 37.4313 | MAPE: 353.9191\n",
            "\n",
            "Fold 150\n",
            "  MAE: 25.6297 | RMSE: 31.5714 | MAPE: 41.2940\n",
            "\n",
            "Fold 151\n",
            "  MAE: 32.1655 | RMSE: 39.8069 | MAPE: 41.6856\n",
            "\n",
            "Fold 152\n",
            "  MAE: 45.0641 | RMSE: 55.4670 | MAPE: 138.6762\n",
            "\n",
            "Fold 153\n",
            "  MAE: 51.0974 | RMSE: 59.1782 | MAPE: 172.6387\n",
            "\n",
            "Fold 154\n",
            "  MAE: 32.1952 | RMSE: 39.2142 | MAPE: 82.3267\n",
            "\n",
            "Fold 155\n",
            "  MAE: 32.7355 | RMSE: 38.6498 | MAPE: 68.7984\n",
            "\n",
            "Fold 156\n",
            "  MAE: 29.6483 | RMSE: 36.4965 | MAPE: 68.3011\n",
            "\n",
            "Fold 157\n",
            "  MAE: 19.7358 | RMSE: 24.5827 | MAPE: 38.7561\n",
            "\n",
            "Fold 158\n",
            "  MAE: 15.7708 | RMSE: 19.7452 | MAPE: 30.1281\n",
            "\n",
            "Fold 159\n",
            "  MAE: 21.4605 | RMSE: 25.7328 | MAPE: 46.2453\n",
            "\n",
            "Fold 160\n",
            "  MAE: 21.3326 | RMSE: 25.4094 | MAPE: 62.6110\n",
            "\n",
            "Fold 161\n",
            "  MAE: 22.4326 | RMSE: 27.2417 | MAPE: 59.4807\n",
            "\n",
            "Fold 162\n",
            "  MAE: 26.1568 | RMSE: 32.4967 | MAPE: 76.8123\n",
            "\n",
            "Fold 163\n",
            "  MAE: 29.2604 | RMSE: 35.4910 | MAPE: 85.2984\n",
            "\n",
            "Fold 164\n",
            "  MAE: 26.2002 | RMSE: 32.5406 | MAPE: 85.8803\n",
            "\n",
            "Fold 165\n",
            "  MAE: 20.8160 | RMSE: 26.2519 | MAPE: 64.9555\n",
            "\n",
            "Fold 166\n",
            "  MAE: 23.9576 | RMSE: 30.4966 | MAPE: 89.2942\n",
            "\n",
            "Fold 167\n",
            "  MAE: 20.8640 | RMSE: 26.9701 | MAPE: 70.4665\n",
            "\n",
            "Fold 168\n",
            "  MAE: 16.2276 | RMSE: 22.0900 | MAPE: 44.4837\n",
            "\n",
            "Fold 169\n",
            "  MAE: 21.5399 | RMSE: 28.8956 | MAPE: 66.7988\n",
            "\n",
            "Fold 170\n",
            "  MAE: 29.3457 | RMSE: 36.8666 | MAPE: 84.8654\n",
            "\n",
            "Fold 171\n",
            "  MAE: 27.4856 | RMSE: 34.3273 | MAPE: 57.4267\n",
            "\n",
            "Fold 172\n",
            "  MAE: 24.9120 | RMSE: 31.8281 | MAPE: 58.7817\n",
            "\n",
            "Fold 173\n",
            "  MAE: 24.4199 | RMSE: 30.6484 | MAPE: 63.4629\n",
            "\n",
            "Fold 174\n",
            "  MAE: 28.5494 | RMSE: 33.3466 | MAPE: 60.9471\n",
            "\n",
            "Fold 175\n",
            "  MAE: 33.8967 | RMSE: 39.9364 | MAPE: 67.6302\n",
            "\n",
            "Fold 176\n",
            "  MAE: 40.2613 | RMSE: 46.3482 | MAPE: 102.4624\n",
            "\n",
            "Fold 177\n",
            "  MAE: 38.1507 | RMSE: 61.0979 | MAPE: 111.4604\n",
            "\n",
            "Fold 178\n",
            "  MAE: 65.0603 | RMSE: 88.2596 | MAPE: 156.7623\n",
            "\n",
            "Fold 179\n",
            "  MAE: 66.8459 | RMSE: 84.9542 | MAPE: 197.7665\n",
            "\n",
            "Fold 180\n",
            "  MAE: 47.2288 | RMSE: 60.1693 | MAPE: 120.3428\n",
            "\n",
            "Fold 181\n",
            "  MAE: 39.0298 | RMSE: 51.7128 | MAPE: 45.0752\n",
            "\n",
            "Fold 182\n",
            "  MAE: 33.9232 | RMSE: 42.5538 | MAPE: 34.8525\n",
            "\n",
            "Fold 183\n",
            "  MAE: 32.7773 | RMSE: 40.1043 | MAPE: 42.5077\n",
            "\n",
            "Fold 184\n",
            "  MAE: 29.2706 | RMSE: 35.2025 | MAPE: 45.9109\n",
            "\n",
            "Fold 185\n",
            "  MAE: 24.0685 | RMSE: 30.1356 | MAPE: 51.7010\n",
            "\n",
            "Fold 186\n",
            "  MAE: 20.1910 | RMSE: 26.4519 | MAPE: 51.1381\n",
            "\n",
            "Fold 187\n",
            "  MAE: 18.0895 | RMSE: 21.9333 | MAPE: 30.5241\n",
            "\n",
            "Fold 188\n",
            "  MAE: 15.6811 | RMSE: 19.4732 | MAPE: 23.5760\n",
            "\n",
            "Fold 189\n",
            "  MAE: 16.4163 | RMSE: 21.4133 | MAPE: 36.5101\n",
            "\n",
            "Fold 190\n",
            "  MAE: 18.8188 | RMSE: 24.4014 | MAPE: 48.8631\n",
            "\n",
            "Fold 191\n",
            "  MAE: 16.2481 | RMSE: 20.4479 | MAPE: 38.3491\n",
            "\n",
            "Fold 192\n",
            "  MAE: 20.2557 | RMSE: 25.8281 | MAPE: 40.7422\n",
            "\n",
            "Fold 193\n",
            "  MAE: 19.8581 | RMSE: 25.3661 | MAPE: 34.4101\n",
            "\n",
            " Final TCN CV Results:\n",
            "Average MAE:  30.0998\n",
            "Average RMSE: 37.4501\n",
            "Average MAPE: 92.5977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tunning (lr = 0.001)"
      ],
      "metadata": {
        "id": "5Wqo1q1eeO8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "# Grid options\n",
        "num_channels_list = [\n",
        "    [64, 64],\n",
        "    [128, 64],\n",
        "    [128, 128]\n",
        "]\n",
        "\n",
        "kernel_sizes = [2, 3, 5]\n",
        "dropouts = [0.0, 0.1, 0.2]\n",
        "learning_rates = [0.001]\n",
        "batch_sizes = [32]\n",
        "\n",
        "# Generate combinations\n",
        "tcn_param_grid = list(product(num_channels_list, kernel_sizes, dropouts, learning_rates, batch_sizes))\n",
        "print(f\"Total combinations: {len(tcn_param_grid)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrXTo2t9eKq_",
        "outputId": "893cc929-8562-4dd3-e434-cd7222ba4228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Total combinations: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tcn_results = []\n",
        "\n",
        "for i, (channels, kernel_size, dropout, lr, batch_size) in enumerate(tcn_param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(tcn_param_grid)}  channels={channels}, kernel={kernel_size}, dropout={dropout}, lr={lr}, batch={batch_size}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14, sequence_len=30)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_tcn_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            sequence_len=30,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            num_channels=channels,\n",
        "            kernel_size=kernel_size,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue\n",
        "\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    # Save results\n",
        "    tcn_results.append({\n",
        "        \"channels\": channels,\n",
        "        \"kernel_size\": kernel_size,\n",
        "        \"dropout\": dropout,\n",
        "        \"lr\": lr,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"MAE\": np.mean(mae_scores),\n",
        "        \"RMSE\": np.mean(rmse_scores),\n",
        "        \"MAPE\": np.mean(mape_scores)\n",
        "    })\n",
        "\n",
        "    print(f\"Avg MAE: {np.mean(mae_scores):.4f} | RMSE: {np.mean(rmse_scores):.4f} | MAPE: {np.mean(mape_scores):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7yS11DweKtP",
        "outputId": "c5e60269-ef91-49de-ddd9-788e5dc2f15d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Config 1/27  channels=[64, 64], kernel=2, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 30.2883 | RMSE: 37.7419 | MAPE: 95.4350\n",
            "\n",
            " Config 2/27  channels=[64, 64], kernel=2, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 30.1341 | RMSE: 37.5239 | MAPE: 94.3021\n",
            "\n",
            " Config 3/27  channels=[64, 64], kernel=2, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 30.0400 | RMSE: 37.4928 | MAPE: 93.3407\n",
            "\n",
            " Config 4/27  channels=[64, 64], kernel=3, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 29.9895 | RMSE: 37.4984 | MAPE: 92.6713\n",
            "\n",
            " Config 5/27  channels=[64, 64], kernel=3, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 29.8698 | RMSE: 37.2964 | MAPE: 93.2529\n",
            "\n",
            " Config 6/27  channels=[64, 64], kernel=3, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 29.6635 | RMSE: 37.0594 | MAPE: 92.3209\n",
            "\n",
            " Config 7/27  channels=[64, 64], kernel=5, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 29.7391 | RMSE: 37.1541 | MAPE: 91.8287\n",
            "\n",
            " Config 8/27  channels=[64, 64], kernel=5, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 30.1389 | RMSE: 37.4193 | MAPE: 92.2227\n",
            "\n",
            " Config 9/27  channels=[64, 64], kernel=5, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 29.5645 | RMSE: 36.9352 | MAPE: 91.4074\n",
            "\n",
            " Config 10/27  channels=[128, 64], kernel=2, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 31.3152 | RMSE: 38.8464 | MAPE: 96.7475\n",
            "\n",
            " Config 11/27  channels=[128, 64], kernel=2, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 30.3094 | RMSE: 37.7382 | MAPE: 95.9560\n",
            "\n",
            " Config 12/27  channels=[128, 64], kernel=2, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 30.6742 | RMSE: 38.2376 | MAPE: 93.8075\n",
            "\n",
            " Config 13/27  channels=[128, 64], kernel=3, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 31.2834 | RMSE: 38.6859 | MAPE: 94.9087\n",
            "\n",
            " Config 14/27  channels=[128, 64], kernel=3, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 31.1275 | RMSE: 38.6209 | MAPE: 97.1138\n",
            "\n",
            " Config 15/27  channels=[128, 64], kernel=3, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 30.1970 | RMSE: 37.7020 | MAPE: 94.7197\n",
            "\n",
            " Config 16/27  channels=[128, 64], kernel=5, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 30.9841 | RMSE: 38.4970 | MAPE: 96.2397\n",
            "\n",
            " Config 17/27  channels=[128, 64], kernel=5, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 30.4503 | RMSE: 37.7932 | MAPE: 96.2202\n",
            "\n",
            " Config 18/27  channels=[128, 64], kernel=5, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 31.1581 | RMSE: 38.4677 | MAPE: 96.1685\n",
            "\n",
            " Config 19/27  channels=[128, 128], kernel=2, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 29.7771 | RMSE: 37.2072 | MAPE: 93.3808\n",
            "\n",
            " Config 20/27  channels=[128, 128], kernel=2, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 30.0441 | RMSE: 37.5390 | MAPE: 94.6344\n",
            "\n",
            " Config 21/27  channels=[128, 128], kernel=2, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 30.1994 | RMSE: 37.5937 | MAPE: 92.6517\n",
            "\n",
            " Config 22/27  channels=[128, 128], kernel=3, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 29.8907 | RMSE: 37.3339 | MAPE: 93.5822\n",
            "\n",
            " Config 23/27  channels=[128, 128], kernel=3, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 30.4379 | RMSE: 37.8009 | MAPE: 92.2864\n",
            "\n",
            " Config 24/27  channels=[128, 128], kernel=3, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 29.5813 | RMSE: 36.9260 | MAPE: 92.2619\n",
            "\n",
            " Config 25/27  channels=[128, 128], kernel=5, dropout=0.0, lr=0.001, batch=32\n",
            " Avg MAE: 30.7297 | RMSE: 38.3053 | MAPE: 94.2064\n",
            "\n",
            " Config 26/27  channels=[128, 128], kernel=5, dropout=0.1, lr=0.001, batch=32\n",
            " Avg MAE: 30.2773 | RMSE: 37.6252 | MAPE: 92.4496\n",
            "\n",
            " Config 27/27  channels=[128, 128], kernel=5, dropout=0.2, lr=0.001, batch=32\n",
            " Avg MAE: 29.0104 | RMSE: 36.2616 | MAPE: 90.5036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_tcn = pd.DataFrame(tcn_results)\n",
        "df_tcn = df_tcn.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
        "df_tcn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "LRJD2ZeleKvl",
        "outputId": "003caf0f-c7f4-46f0-958d-ebf058232a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      channels  kernel_size  dropout     lr  batch_size        MAE       RMSE  \\\n",
              "0   [128, 128]            5      0.2  0.001          32  29.010372  36.261627   \n",
              "1   [128, 128]            3      0.2  0.001          32  29.581293  36.925982   \n",
              "2     [64, 64]            5      0.2  0.001          32  29.564508  36.935240   \n",
              "3     [64, 64]            3      0.2  0.001          32  29.663507  37.059407   \n",
              "4     [64, 64]            5      0.0  0.001          32  29.739125  37.154115   \n",
              "5   [128, 128]            2      0.0  0.001          32  29.777122  37.207161   \n",
              "6     [64, 64]            3      0.1  0.001          32  29.869814  37.296391   \n",
              "7   [128, 128]            3      0.0  0.001          32  29.890747  37.333907   \n",
              "8     [64, 64]            5      0.1  0.001          32  30.138905  37.419350   \n",
              "9     [64, 64]            2      0.2  0.001          32  30.039987  37.492814   \n",
              "10    [64, 64]            3      0.0  0.001          32  29.989485  37.498447   \n",
              "11    [64, 64]            2      0.1  0.001          32  30.134129  37.523921   \n",
              "12  [128, 128]            2      0.1  0.001          32  30.044052  37.538994   \n",
              "13  [128, 128]            2      0.2  0.001          32  30.199415  37.593711   \n",
              "14  [128, 128]            5      0.1  0.001          32  30.277315  37.625177   \n",
              "15   [128, 64]            3      0.2  0.001          32  30.197014  37.701958   \n",
              "16   [128, 64]            2      0.1  0.001          32  30.309386  37.738191   \n",
              "17    [64, 64]            2      0.0  0.001          32  30.288304  37.741902   \n",
              "18   [128, 64]            5      0.1  0.001          32  30.450321  37.793169   \n",
              "19  [128, 128]            3      0.1  0.001          32  30.437875  37.800911   \n",
              "20   [128, 64]            2      0.2  0.001          32  30.674199  38.237573   \n",
              "21  [128, 128]            5      0.0  0.001          32  30.729669  38.305338   \n",
              "22   [128, 64]            5      0.2  0.001          32  31.158120  38.467714   \n",
              "23   [128, 64]            5      0.0  0.001          32  30.984123  38.497000   \n",
              "24   [128, 64]            3      0.1  0.001          32  31.127531  38.620925   \n",
              "25   [128, 64]            3      0.0  0.001          32  31.283375  38.685916   \n",
              "26   [128, 64]            2      0.0  0.001          32  31.315233  38.846385   \n",
              "\n",
              "         MAPE  \n",
              "0   90.503628  \n",
              "1   92.261882  \n",
              "2   91.407405  \n",
              "3   92.320876  \n",
              "4   91.828736  \n",
              "5   93.380802  \n",
              "6   93.252879  \n",
              "7   93.582211  \n",
              "8   92.222743  \n",
              "9   93.340683  \n",
              "10  92.671305  \n",
              "11  94.302074  \n",
              "12  94.634404  \n",
              "13  92.651727  \n",
              "14  92.449602  \n",
              "15  94.719749  \n",
              "16  95.956006  \n",
              "17  95.435034  \n",
              "18  96.220201  \n",
              "19  92.286436  \n",
              "20  93.807531  \n",
              "21  94.206378  \n",
              "22  96.168545  \n",
              "23  96.239650  \n",
              "24  97.113818  \n",
              "25  94.908692  \n",
              "26  96.747484  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7c719089-d0f3-4508-ad63-a09fab2460e0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>channels</th>\n",
              "      <th>kernel_size</th>\n",
              "      <th>dropout</th>\n",
              "      <th>lr</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>MAE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[128, 128]</td>\n",
              "      <td>5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>29.010372</td>\n",
              "      <td>36.261627</td>\n",
              "      <td>90.503628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[128, 128]</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>29.581293</td>\n",
              "      <td>36.925982</td>\n",
              "      <td>92.261882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[64, 64]</td>\n",
              "      <td>5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>29.564508</td>\n",
              "      <td>36.935240</td>\n",
              "      <td>91.407405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[64, 64]</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>29.663507</td>\n",
              "      <td>37.059407</td>\n",
              "      <td>92.320876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[64, 64]</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>29.739125</td>\n",
              "      <td>37.154115</td>\n",
              "      <td>91.828736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[128, 128]</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>29.777122</td>\n",
              "      <td>37.207161</td>\n",
              "      <td>93.380802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[64, 64]</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>29.869814</td>\n",
              "      <td>37.296391</td>\n",
              "      <td>93.252879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[128, 128]</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>29.890747</td>\n",
              "      <td>37.333907</td>\n",
              "      <td>93.582211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[64, 64]</td>\n",
              "      <td>5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30.138905</td>\n",
              "      <td>37.419350</td>\n",
              "      <td>92.222743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[64, 64]</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30.039987</td>\n",
              "      <td>37.492814</td>\n",
              "      <td>93.340683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[64, 64]</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>29.989485</td>\n",
              "      <td>37.498447</td>\n",
              "      <td>92.671305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[64, 64]</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30.134129</td>\n",
              "      <td>37.523921</td>\n",
              "      <td>94.302074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[128, 128]</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30.044052</td>\n",
              "      <td>37.538994</td>\n",
              "      <td>94.634404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[128, 128]</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30.199415</td>\n",
              "      <td>37.593711</td>\n",
              "      <td>92.651727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[128, 128]</td>\n",
              "      <td>5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30.277315</td>\n",
              "      <td>37.625177</td>\n",
              "      <td>92.449602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>[128, 64]</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30.197014</td>\n",
              "      <td>37.701958</td>\n",
              "      <td>94.719749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>[128, 64]</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30.309386</td>\n",
              "      <td>37.738191</td>\n",
              "      <td>95.956006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>[64, 64]</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30.288304</td>\n",
              "      <td>37.741902</td>\n",
              "      <td>95.435034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>[128, 64]</td>\n",
              "      <td>5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30.450321</td>\n",
              "      <td>37.793169</td>\n",
              "      <td>96.220201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>[128, 128]</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30.437875</td>\n",
              "      <td>37.800911</td>\n",
              "      <td>92.286436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>[128, 64]</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30.674199</td>\n",
              "      <td>38.237573</td>\n",
              "      <td>93.807531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>[128, 128]</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30.729669</td>\n",
              "      <td>38.305338</td>\n",
              "      <td>94.206378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>[128, 64]</td>\n",
              "      <td>5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>31.158120</td>\n",
              "      <td>38.467714</td>\n",
              "      <td>96.168545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>[128, 64]</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>30.984123</td>\n",
              "      <td>38.497000</td>\n",
              "      <td>96.239650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>[128, 64]</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>31.127531</td>\n",
              "      <td>38.620925</td>\n",
              "      <td>97.113818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>[128, 64]</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>31.283375</td>\n",
              "      <td>38.685916</td>\n",
              "      <td>94.908692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>[128, 64]</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>31.315233</td>\n",
              "      <td>38.846385</td>\n",
              "      <td>96.747484</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c719089-d0f3-4508-ad63-a09fab2460e0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7c719089-d0f3-4508-ad63-a09fab2460e0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7c719089-d0f3-4508-ad63-a09fab2460e0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c0f39953-e649-422f-8d08-eef054497fa1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c0f39953-e649-422f-8d08-eef054497fa1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c0f39953-e649-422f-8d08-eef054497fa1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_e2413cb9-c486-460c-88db-e22822c2f17f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_tcn')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_e2413cb9-c486-460c-88db-e22822c2f17f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_tcn');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_tcn",
              "summary": "{\n  \"name\": \"df_tcn\",\n  \"rows\": 27,\n  \"fields\": [\n    {\n      \"column\": \"channels\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"kernel_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 2,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          5,\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08320502943378438,\n        \"min\": 0.0,\n        \"max\": 0.2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.2,\n          0.0,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.629132989485078e-19,\n        \"min\": 0.001,\n        \"max\": 0.001,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5740655732302499,\n        \"min\": 29.010372169414897,\n        \"max\": 31.31523267705186,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          30.13890461992337\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6126994519110502,\n        \"min\": 36.26162664916548,\n        \"max\": 38.84638539461157,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          37.41934972816823\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.780294720627416,\n        \"min\": 90.50362807327915,\n        \"max\": 97.11381767868293,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          92.22274338868391\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tunning (lr = 0.0005)"
      ],
      "metadata": {
        "id": "6Bzp87aB0Enu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "# Grid options\n",
        "num_channels_list = [\n",
        "    [64, 64],\n",
        "    [128, 64],\n",
        "    [128, 128]\n",
        "]\n",
        "\n",
        "kernel_sizes = [2, 3, 5]\n",
        "dropouts = [0.0, 0.1, 0.2]\n",
        "learning_rates = [0.0005]\n",
        "batch_sizes = [32]\n",
        "\n",
        "# Generate combinations\n",
        "tcn_param_grid = list(product(num_channels_list, kernel_sizes, dropouts, learning_rates, batch_sizes))\n",
        "print(f\"Total combinations: {len(tcn_param_grid)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oALYS-Lm0Dnp",
        "outputId": "9d5cffd4-2af9-45c9-fa91-9b2462e45604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Total combinations: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tcn_results = []\n",
        "\n",
        "for i, (channels, kernel_size, dropout, lr, batch_size) in enumerate(tcn_param_grid):\n",
        "    print(f\"\\n Config {i+1}/{len(tcn_param_grid)}  channels={channels}, kernel={kernel_size}, dropout={dropout}, lr={lr}, batch={batch_size}\")\n",
        "\n",
        "    mae_scores = []\n",
        "    rmse_scores = []\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (X_tr, y_tr, X_val, y_val) in enumerate(\n",
        "        expanding_window_cv(X, y, initial_train_size=1095, horizon=14, step=14, sequence_len=30)\n",
        "    ):\n",
        "        mae, rmse, mape = train_one_tcn_fold(\n",
        "            X_tr, y_tr, X_val, y_val,\n",
        "            sequence_len=30,\n",
        "            num_epochs=50,\n",
        "            patience=5,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            num_channels=channels,\n",
        "            kernel_size=kernel_size,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        if mae is None:\n",
        "            continue\n",
        "\n",
        "        mae_scores.append(mae)\n",
        "        rmse_scores.append(rmse)\n",
        "        mape_scores.append(mape)\n",
        "\n",
        "    # Save results\n",
        "    tcn_results.append({\n",
        "        \"channels\": channels,\n",
        "        \"kernel_size\": kernel_size,\n",
        "        \"dropout\": dropout,\n",
        "        \"lr\": lr,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"MAE\": np.mean(mae_scores),\n",
        "        \"RMSE\": np.mean(rmse_scores),\n",
        "        \"MAPE\": np.mean(mape_scores)\n",
        "    })\n",
        "\n",
        "    print(f\" Avg MAE: {np.mean(mae_scores):.4f} | RMSE: {np.mean(rmse_scores):.4f} | MAPE: {np.mean(mape_scores):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8myg9M40Dwe",
        "outputId": "a2ee5fe3-cb3f-4dd8-cee9-4e75bfe476ad",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Config 1/27  channels=[64, 64], kernel=2, dropout=0.0, lr=0.0005, batch=32\n",
            " Avg MAE: 30.6086 | RMSE: 38.1115 | MAPE: 94.7117\n",
            "\n",
            " Config 2/27  channels=[64, 64], kernel=2, dropout=0.1, lr=0.0005, batch=32\n",
            " Avg MAE: 29.9620 | RMSE: 37.4246 | MAPE: 93.7656\n",
            "\n",
            " Config 3/27  channels=[64, 64], kernel=2, dropout=0.2, lr=0.0005, batch=32\n",
            " Avg MAE: 30.0178 | RMSE: 37.3518 | MAPE: 94.3049\n",
            "\n",
            " Config 4/27  channels=[64, 64], kernel=3, dropout=0.0, lr=0.0005, batch=32\n",
            " Avg MAE: 30.2330 | RMSE: 37.6867 | MAPE: 93.2914\n",
            "\n",
            " Config 5/27  channels=[64, 64], kernel=3, dropout=0.1, lr=0.0005, batch=32\n",
            " Avg MAE: 29.8627 | RMSE: 37.3141 | MAPE: 94.0388\n",
            "\n",
            " Config 6/27  channels=[64, 64], kernel=3, dropout=0.2, lr=0.0005, batch=32\n",
            " Avg MAE: 29.5401 | RMSE: 36.8300 | MAPE: 93.0457\n",
            "\n",
            " Config 7/27  channels=[64, 64], kernel=5, dropout=0.0, lr=0.0005, batch=32\n",
            " Avg MAE: 30.4781 | RMSE: 37.9067 | MAPE: 94.5364\n",
            "\n",
            " Config 8/27  channels=[64, 64], kernel=5, dropout=0.1, lr=0.0005, batch=32\n",
            " Avg MAE: 29.9715 | RMSE: 37.3238 | MAPE: 92.5855\n",
            "\n",
            " Config 9/27  channels=[64, 64], kernel=5, dropout=0.2, lr=0.0005, batch=32\n",
            " Avg MAE: 29.5219 | RMSE: 36.8012 | MAPE: 92.6615\n",
            "\n",
            " Config 10/27  channels=[128, 64], kernel=2, dropout=0.0, lr=0.0005, batch=32\n",
            " Avg MAE: 30.3769 | RMSE: 37.8010 | MAPE: 93.7272\n",
            "\n",
            " Config 11/27  channels=[128, 64], kernel=2, dropout=0.1, lr=0.0005, batch=32\n",
            " Avg MAE: 30.4464 | RMSE: 37.8964 | MAPE: 95.1792\n",
            "\n",
            " Config 12/27  channels=[128, 64], kernel=2, dropout=0.2, lr=0.0005, batch=32\n",
            " Avg MAE: 30.2727 | RMSE: 37.7155 | MAPE: 92.7541\n",
            "\n",
            " Config 13/27  channels=[128, 64], kernel=3, dropout=0.0, lr=0.0005, batch=32\n",
            " Avg MAE: 30.0104 | RMSE: 37.4269 | MAPE: 92.3270\n",
            "\n",
            " Config 14/27  channels=[128, 64], kernel=3, dropout=0.1, lr=0.0005, batch=32\n",
            " Avg MAE: 30.0162 | RMSE: 37.3573 | MAPE: 94.4059\n",
            "\n",
            " Config 15/27  channels=[128, 64], kernel=3, dropout=0.2, lr=0.0005, batch=32\n",
            " Avg MAE: 30.0786 | RMSE: 37.4617 | MAPE: 91.3180\n",
            "\n",
            " Config 16/27  channels=[128, 64], kernel=5, dropout=0.0, lr=0.0005, batch=32\n",
            " Avg MAE: 30.7182 | RMSE: 38.1040 | MAPE: 95.3747\n",
            "\n",
            " Config 17/27  channels=[128, 64], kernel=5, dropout=0.1, lr=0.0005, batch=32\n",
            " Avg MAE: 30.1348 | RMSE: 37.4268 | MAPE: 94.4352\n",
            "\n",
            " Config 18/27  channels=[128, 64], kernel=5, dropout=0.2, lr=0.0005, batch=32\n",
            " Avg MAE: 30.1748 | RMSE: 37.5478 | MAPE: 94.9085\n",
            "\n",
            " Config 19/27  channels=[128, 128], kernel=2, dropout=0.0, lr=0.0005, batch=32\n",
            " Avg MAE: 30.0866 | RMSE: 37.4570 | MAPE: 94.1981\n",
            "\n",
            " Config 20/27  channels=[128, 128], kernel=2, dropout=0.1, lr=0.0005, batch=32\n",
            " Avg MAE: 29.7685 | RMSE: 37.1399 | MAPE: 93.3495\n",
            "\n",
            " Config 21/27  channels=[128, 128], kernel=2, dropout=0.2, lr=0.0005, batch=32\n",
            " Avg MAE: 29.8198 | RMSE: 37.2495 | MAPE: 93.6050\n",
            "\n",
            " Config 22/27  channels=[128, 128], kernel=3, dropout=0.0, lr=0.0005, batch=32\n",
            " Avg MAE: 30.1518 | RMSE: 37.5585 | MAPE: 91.5598\n",
            "\n",
            " Config 23/27  channels=[128, 128], kernel=3, dropout=0.1, lr=0.0005, batch=32\n",
            " Avg MAE: 29.5674 | RMSE: 36.9134 | MAPE: 92.9875\n",
            "\n",
            " Config 24/27  channels=[128, 128], kernel=3, dropout=0.2, lr=0.0005, batch=32\n",
            " Avg MAE: 29.9343 | RMSE: 37.3179 | MAPE: 92.6639\n",
            "\n",
            " Config 25/27  channels=[128, 128], kernel=5, dropout=0.0, lr=0.0005, batch=32\n",
            " Avg MAE: 30.3237 | RMSE: 37.7656 | MAPE: 94.1576\n",
            "\n",
            " Config 26/27  channels=[128, 128], kernel=5, dropout=0.1, lr=0.0005, batch=32\n",
            " Avg MAE: 29.6632 | RMSE: 37.0033 | MAPE: 93.2728\n",
            "\n",
            " Config 27/27  channels=[128, 128], kernel=5, dropout=0.2, lr=0.0005, batch=32\n",
            " Avg MAE: 29.6455 | RMSE: 37.0151 | MAPE: 93.6812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_tcn = pd.DataFrame(tcn_results)\n",
        "df_tcn = df_tcn.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
        "df_tcn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "Ao3b56-B0D6G",
        "outputId": "18c4144a-e091-4990-d9dc-e8d7fde61fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      channels  kernel_size  dropout      lr  batch_size        MAE  \\\n",
              "0     [64, 64]            5      0.2  0.0005          32  29.521908   \n",
              "1     [64, 64]            3      0.2  0.0005          32  29.540073   \n",
              "2   [128, 128]            3      0.1  0.0005          32  29.567380   \n",
              "3   [128, 128]            5      0.1  0.0005          32  29.663175   \n",
              "4   [128, 128]            5      0.2  0.0005          32  29.645477   \n",
              "5   [128, 128]            2      0.1  0.0005          32  29.768491   \n",
              "6   [128, 128]            2      0.2  0.0005          32  29.819751   \n",
              "7     [64, 64]            3      0.1  0.0005          32  29.862674   \n",
              "8   [128, 128]            3      0.2  0.0005          32  29.934321   \n",
              "9     [64, 64]            5      0.1  0.0005          32  29.971523   \n",
              "10    [64, 64]            2      0.2  0.0005          32  30.017767   \n",
              "11   [128, 64]            3      0.1  0.0005          32  30.016245   \n",
              "12    [64, 64]            2      0.1  0.0005          32  29.962032   \n",
              "13   [128, 64]            5      0.1  0.0005          32  30.134813   \n",
              "14   [128, 64]            3      0.0  0.0005          32  30.010370   \n",
              "15  [128, 128]            2      0.0  0.0005          32  30.086627   \n",
              "16   [128, 64]            3      0.2  0.0005          32  30.078565   \n",
              "17   [128, 64]            5      0.2  0.0005          32  30.174787   \n",
              "18  [128, 128]            3      0.0  0.0005          32  30.151776   \n",
              "19    [64, 64]            3      0.0  0.0005          32  30.232984   \n",
              "20   [128, 64]            2      0.2  0.0005          32  30.272656   \n",
              "21  [128, 128]            5      0.0  0.0005          32  30.323732   \n",
              "22   [128, 64]            2      0.0  0.0005          32  30.376911   \n",
              "23   [128, 64]            2      0.1  0.0005          32  30.446352   \n",
              "24    [64, 64]            5      0.0  0.0005          32  30.478123   \n",
              "25   [128, 64]            5      0.0  0.0005          32  30.718241   \n",
              "26    [64, 64]            2      0.0  0.0005          32  30.608581   \n",
              "\n",
              "         RMSE       MAPE  \n",
              "0   36.801219  92.661470  \n",
              "1   36.830018  93.045734  \n",
              "2   36.913412  92.987458  \n",
              "3   37.003283  93.272773  \n",
              "4   37.015112  93.681235  \n",
              "5   37.139857  93.349502  \n",
              "6   37.249475  93.604952  \n",
              "7   37.314095  94.038797  \n",
              "8   37.317935  92.663935  \n",
              "9   37.323805  92.585483  \n",
              "10  37.351785  94.304893  \n",
              "11  37.357273  94.405868  \n",
              "12  37.424568  93.765619  \n",
              "13  37.426835  94.435195  \n",
              "14  37.426860  92.327041  \n",
              "15  37.457012  94.198130  \n",
              "16  37.461673  91.317972  \n",
              "17  37.547800  94.908461  \n",
              "18  37.558534  91.559837  \n",
              "19  37.686730  93.291419  \n",
              "20  37.715524  92.754077  \n",
              "21  37.765570  94.157637  \n",
              "22  37.800979  93.727201  \n",
              "23  37.896352  95.179232  \n",
              "24  37.906653  94.536392  \n",
              "25  38.104000  95.374689  \n",
              "26  38.111470  94.711674  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7d5cbd41-5a72-49c5-b774-4c8a7a38e8e8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>channels</th>\n",
              "      <th>kernel_size</th>\n",
              "      <th>dropout</th>\n",
              "      <th>lr</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>MAE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MAPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[64, 64]</td>\n",
              "      <td>5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>29.521908</td>\n",
              "      <td>36.801219</td>\n",
              "      <td>92.661470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[64, 64]</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>29.540073</td>\n",
              "      <td>36.830018</td>\n",
              "      <td>93.045734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[128, 128]</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>29.567380</td>\n",
              "      <td>36.913412</td>\n",
              "      <td>92.987458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[128, 128]</td>\n",
              "      <td>5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>29.663175</td>\n",
              "      <td>37.003283</td>\n",
              "      <td>93.272773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[128, 128]</td>\n",
              "      <td>5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>29.645477</td>\n",
              "      <td>37.015112</td>\n",
              "      <td>93.681235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[128, 128]</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>29.768491</td>\n",
              "      <td>37.139857</td>\n",
              "      <td>93.349502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[128, 128]</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>29.819751</td>\n",
              "      <td>37.249475</td>\n",
              "      <td>93.604952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[64, 64]</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>29.862674</td>\n",
              "      <td>37.314095</td>\n",
              "      <td>94.038797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[128, 128]</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>29.934321</td>\n",
              "      <td>37.317935</td>\n",
              "      <td>92.663935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[64, 64]</td>\n",
              "      <td>5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>29.971523</td>\n",
              "      <td>37.323805</td>\n",
              "      <td>92.585483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[64, 64]</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.017767</td>\n",
              "      <td>37.351785</td>\n",
              "      <td>94.304893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[128, 64]</td>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.016245</td>\n",
              "      <td>37.357273</td>\n",
              "      <td>94.405868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[64, 64]</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>29.962032</td>\n",
              "      <td>37.424568</td>\n",
              "      <td>93.765619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[128, 64]</td>\n",
              "      <td>5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.134813</td>\n",
              "      <td>37.426835</td>\n",
              "      <td>94.435195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[128, 64]</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.010370</td>\n",
              "      <td>37.426860</td>\n",
              "      <td>92.327041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>[128, 128]</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.086627</td>\n",
              "      <td>37.457012</td>\n",
              "      <td>94.198130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>[128, 64]</td>\n",
              "      <td>3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.078565</td>\n",
              "      <td>37.461673</td>\n",
              "      <td>91.317972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>[128, 64]</td>\n",
              "      <td>5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.174787</td>\n",
              "      <td>37.547800</td>\n",
              "      <td>94.908461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>[128, 128]</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.151776</td>\n",
              "      <td>37.558534</td>\n",
              "      <td>91.559837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>[64, 64]</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.232984</td>\n",
              "      <td>37.686730</td>\n",
              "      <td>93.291419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>[128, 64]</td>\n",
              "      <td>2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.272656</td>\n",
              "      <td>37.715524</td>\n",
              "      <td>92.754077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>[128, 128]</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.323732</td>\n",
              "      <td>37.765570</td>\n",
              "      <td>94.157637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>[128, 64]</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.376911</td>\n",
              "      <td>37.800979</td>\n",
              "      <td>93.727201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>[128, 64]</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.446352</td>\n",
              "      <td>37.896352</td>\n",
              "      <td>95.179232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>[64, 64]</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.478123</td>\n",
              "      <td>37.906653</td>\n",
              "      <td>94.536392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>[128, 64]</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.718241</td>\n",
              "      <td>38.104000</td>\n",
              "      <td>95.374689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>[64, 64]</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>32</td>\n",
              "      <td>30.608581</td>\n",
              "      <td>38.111470</td>\n",
              "      <td>94.711674</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7d5cbd41-5a72-49c5-b774-4c8a7a38e8e8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7d5cbd41-5a72-49c5-b774-4c8a7a38e8e8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7d5cbd41-5a72-49c5-b774-4c8a7a38e8e8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-50ab8c08-0176-4f7f-81fc-2c47d8e41309\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-50ab8c08-0176-4f7f-81fc-2c47d8e41309')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-50ab8c08-0176-4f7f-81fc-2c47d8e41309 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_139bfda1-ce8b-4d22-b770-7d57ac47d74e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_tcn')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_139bfda1-ce8b-4d22-b770-7d57ac47d74e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_tcn');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_tcn",
              "summary": "{\n  \"name\": \"df_tcn\",\n  \"rows\": 27,\n  \"fields\": [\n    {\n      \"column\": \"channels\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"kernel_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 2,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          5,\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08320502943378438,\n        \"min\": 0.0,\n        \"max\": 0.2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.2,\n          0.1,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.314566494742539e-19,\n        \"min\": 0.0005,\n        \"max\": 0.0005,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0005\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3212067093221057,\n        \"min\": 29.5219080147926,\n        \"max\": 30.718241094192738,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          29.93432134193107\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3579748671464038,\n        \"min\": 36.80121872050173,\n        \"max\": 38.111469524712646,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          37.31793456526621\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MAPE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0314035320172619,\n        \"min\": 91.31797179713386,\n        \"max\": 95.37468880752594,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          92.6639346363705\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kw3HJdUhwaOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pTb4SmE0wbS0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}